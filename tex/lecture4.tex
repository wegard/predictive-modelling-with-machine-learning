% !TEX TS-program = XeLaTeX
% !TEX spellcheck = en-US
\documentclass[aspectratio=169]{beamer}

% Choose your theme
\usetheme{example}

<<<<<<< HEAD
%------------------------------------------------------------------------------
% Title and author info
%------------------------------------------------------------------------------
\title{Lecture 4:\\ Classification Analysis}
\institute{GRA4160: Predictive Modelling with Machine Learning}
\date{February 29th 2025}
=======
\title{Lecture 4:\\ Classification Analysis}
\institute{GRA4160: Predictive Modelling with Machine Learning}
\date{February 30, 2025} 
>>>>>>> a758e4a4267ad4122c0a3bb845bd2bd86f0c3ff5
\author{Vegard H\o ghaug Larsen}

%------------------------------------------------------------------------------
\begin{document}
%------------------------------------------------------------------------------
\maketitle

<<<<<<< HEAD
%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Plan for today}
    \begin{itemize}
        \item \textbf{Logistic Regression}
        \item \textbf{Decision Trees}
        \item Model Evaluation Metrics
        \item Practical Considerations \& Regularization
        %\item Exercise: Recognizing Handwritten Digits
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Logistic Regression}
    \begin{itemize}
        \item Logistic regression is a model for \textbf{binary classification} (e.g.\ yes/no, success/failure).
        \pause
        \item It assumes a linear relationship between the predictors and the \emph{log-odds} of the outcome.
        \[
            \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1x_1 +\beta_2x_2 + \cdots + \beta_kx_k
        \]
		\pause
        \item The logarithmic (log-odds) transformation ensures that predicted probabilities remain in the range $(0,1)$.
        \pause
        \item Logistic regression can be extended to multiclass classification:
        \begin{itemize}
            \item One-vs-Rest (OvR) strategy
            \item Softmax/Multinomial logistic regression
        \end{itemize}
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Maximum Likelihood Estimation}
    \begin{itemize}
        \item Logistic regression parameters are typically estimated by maximizing the \textbf{likelihood function}:
        \[
            L(\boldsymbol{\beta}) = \prod_{i=1}^n p_i^{y_i} \, (1 - p_i)^{1-y_i}
        \]
        or equivalently by \textbf{minimizing} the negative log-likelihood (also known as \emph{cross-entropy loss}).
        \item This is usually done through iterative optimization algorithms like \emph{gradient descent} or \emph{Newton's method}.
        \item Interpretation of coefficients: $\beta_j$ describes the change in the \emph{log-odds} for a unit change in $x_j$.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{The Logistic (Sigmoid) Function}
    \begin{itemize}
        \item The logistic function (sigmoid) maps real numbers to $(0,1)$:
        \[
            f(z) = \frac{1}{1 + e^{-z}}
        \]
        \item For $z \rightarrow \infty$, $f(z) \rightarrow 1$, and for $z \rightarrow -\infty$, $f(z) \rightarrow 0$.
        \item Thus, the model links a linear function in the inputs ($z$) to a probability.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Why Not Linear Regression?}
    \begin{itemize}
        \item Linear regression can predict probabilities less than 0 or greater than 1, which is inconsistent with probability theory.
        \item The assumptions (e.g.\ normally distributed errors) are not valid for classification.
        \item Logistic regression addresses these issues with the log-odds transformation.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Regularization in Logistic Regression}
    \begin{itemize}
        \item Prevents overfitting by penalizing large coefficient values.
        \item Common penalties:
        \begin{itemize}
            \item L2 (Ridge): \(\lambda \sum_j \beta_j^2\)
            \item L1 (Lasso): \(\lambda \sum_j |\beta_j|\)
        \end{itemize}
        \item Balances model complexity (bias) with model flexibility (variance).
        \item Important hyperparameter: the regularization strength \(\lambda\) (or \(C = 1/\lambda\) in some software).
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Decision Trees}
    \begin{itemize}
        \item A non-parametric, tree-based method.
        \item Begins from a root node and partitions the data into subsets (branches) based on feature-based splitting rules.
        \item Each leaf node (terminal node) represents a prediction (class label).
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Training a Decision Tree}
    \begin{itemize}
        \item Constructed recursively by splitting the data into subsets.
        \item At each node, choose the feature and threshold that produce the \emph{best} split:
        \begin{itemize}
            \item Best according to a purity metric such as \textbf{Gini} or \textbf{Entropy}.
        \end{itemize}
        \item Stopping criteria:
        \begin{itemize}
            \item Maximum depth of the tree
            \item Minimum number of samples required to split
            \item Minimum impurity decrease, etc.
        \end{itemize}
        \item A fully grown tree can overfit. Pruning or limiting tree depth often helps generalize better.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Gini Impurity}
    \begin{itemize}
        \item Gini index (impurity) at node:
        \[
            G = \sum_{i=1}^{C} p_i(1 - p_i) = 1 - \sum_{i=1}^C p_i^2
        \]
        where \(p_i\) is the proportion of class \(i\).
        \item Measures the chance of misclassifying a random observation from the node.
        \item Smaller Gini means higher purity.
        \item Often used by the CART (Classification and Regression Tree) algorithm.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Entropy}
    \begin{itemize}
        \item Entropy at node:
        \[
            H = -\sum_{i=1}^C p_i \log_2(p_i)
        \]
        \item Also measures uncertainty/disorder.
        \item The goal is to minimize entropy to create "purer" splits.
        \item In practice, Gini and Entropy often yield similar results.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Overfitting in Decision Trees}
    \begin{itemize}
        \item Deep trees can memorize noise in the training data.
        \item Pruning:
        \begin{itemize}
            \item Post-pruning or pre-pruning to reduce depth.
            \item Set constraints like \textit{max depth}, \textit{min samples per leaf}, or \textit{min impurity decrease}.
        \end{itemize}
        \item Ensemble methods (e.g. Random Forest, Gradient Boosting) further reduce overfitting by combining multiple trees.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Model Evaluation Metrics}
    \begin{itemize}
        \item \textbf{Accuracy}:
        \[
            \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}
        \]
        \item \textbf{Precision} and \textbf{Recall}:
        \[
            \text{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}, \quad
            \text{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}
        \]
        \item \textbf{F1-score}: harmonic mean of precision and recall.
        \item \textbf{Confusion Matrix}: shows the counts of true positives, false positives, false negatives, and true negatives.
        \item \textbf{ROC/AUC}: ability to rank predictions correctly across various thresholds.
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Practical Considerations}
    \begin{itemize}
        \item \textbf{Feature Engineering}:
        \begin{itemize}
            \item Scaling, encoding categorical variables, etc.
            \item Especially relevant for logistic regression (e.g.\ standardization).
        \end{itemize}
        \item \textbf{Cross-Validation}:
        \begin{itemize}
            \item Helps tune hyperparameters (e.g.\ regularization strength, tree depth).
            \item Reduces risk of overfitting to a single train/test split.
        \end{itemize}
        \item \textbf{Class Imbalance}:
        \begin{itemize}
            \item Techniques like oversampling minority class, undersampling majority class, or using class-weighted loss.
        \end{itemize}
        \item \textbf{Interpretability}:
        \begin{itemize}
            \item Decision trees are quite interpretable (splits are easy to visualize).
            \item Logistic regression coefficients are straightforward to interpret in terms of log-odds.
        \end{itemize}
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
% \begin{frame}
%     \frametitle{Exercise: Recognizing Handwritten Digits}
%     \begin{itemize}
%         \item Use any available dataset (e.g., \texttt{sklearn.datasets.load\_digits}) which contains 8x8 pixel images of handwritten digits.
%         \item Tasks:
%         \begin{enumerate}
%             \item Load and inspect the dataset (distribution of classes, feature shapes).
%             \item Train a \textbf{Logistic Regression} model and a \textbf{Decision Tree} model.
%             \item Compare performance using metrics:
%             \begin{itemize}
%                 \item Confusion matrix
%                 \item Accuracy
%                 \item F1-score
%                 \item (Optional) ROC/AUC for each digit vs. rest
%             \end{itemize}
%             \item Investigate overfitting (e.g., adjust tree depth or regularization).
%             \item Report results and discuss.
%         \end{enumerate}
%     \end{itemize}
% \end{frame}

%-------------------------------------------------------------------------------
=======
\frame{
    \frametitle{Plan for Today}
    \begin{itemize}
        \item \textbf{Recap} from last week: LDA and Regularized Regression
        \item \textbf{Logistic Regression}
        \item \textbf{Decision Trees}
        \item \textbf{Exercise: Recognizing handwritten digits}
    \end{itemize}
}

% ------------------------------------------------------------
%   LDA Recap
% ------------------------------------------------------------
\begin{frame}
    \frametitle{Recap: Linear Discriminant Analysis (LDA)}
    \begin{itemize}
        \item \textbf{Purpose:}
            \begin{itemize}
                \item Supervised classification \& dimensionality reduction.
                \item Projects data onto a lower-dimensional space to maximize class separation.
            \end{itemize}
        \item \textbf{When to Use:}
            \begin{itemize}
                \item Two or more well-separated classes with similar covariance matrices.
                \item Need for interpretable linear boundaries and reduced dimensionality.
            \end{itemize}
        \item \textbf{Mathematical Core:}
            \begin{itemize}
                \item Defines scatter matrices: \(\mathbf{S}_B\) (between-class) and \(\mathbf{S}_W\) (within-class).
                \item Objective: 
                \[
                    \max_{\mathbf{w}} \; \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}.
                \]
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Recap: LDA Assumptions \& Practical Tips}
    \begin{itemize}
        \item \textbf{Key Assumptions:}
            \begin{enumerate}
                \item Each class follows (approximately) a normal (Gaussian) distribution.
                \item All classes share the same covariance matrix.
                \item Features are ideally uncorrelated, but mild correlations are often acceptable.
            \end{enumerate}
        \item \textbf{Implications:}
            \begin{itemize}
                \item If covariances differ significantly, consider Quadratic Discriminant Analysis (QDA).
                \item LDA can be robust to mild violations of normality.
            \end{itemize}
        \item \textbf{Practical Tips:}
            \begin{itemize}
                \item Scale/standardize data to stabilize covariance estimation.
                \item Use cross-validation to evaluate classification performance (e.g., confusion matrices).
                \item Works well with moderate-to-large datasets that have reasonable class separation.
            \end{itemize}
        \item \textbf{Takeaway:} 
            A simple, interpretable linear boundary with dimensionality reduction under Gaussian assumptions.
    \end{itemize}
\end{frame}

% ------------------------------------------------------------
%   Regularized Regression Recap
% ------------------------------------------------------------
\begin{frame}
    \frametitle{Recap: Regularized Regression (Ridge, Lasso, Elastic Net)}
    \begin{itemize}
        \item \textbf{Motivation:} Mitigate overfitting by penalizing large coefficients.
        \item \textbf{Methods Overview:}
            \begin{itemize}
                \item \textbf{Ridge (L2):}
                    \begin{itemize}
                        \item Shrinks coefficients (rarely to exactly zero).
                        \item Useful for handling multicollinearity.
                    \end{itemize}
                \item \textbf{Lasso (L1):}
                    \begin{itemize}
                        \item Can set some coefficients to zero (\emph{feature selection}).
                        \item Often helpful in high-dimensional settings.
                    \end{itemize}
                \item \textbf{Elastic Net (L1 + L2):}
                    \begin{itemize}
                        \item Balances the strengths of Ridge and Lasso.
                        \item Two hyperparameters: overall strength (\(\lambda\)) + mixing ratio (\(\alpha\)).
                    \end{itemize}
            \end{itemize}
        \item \textbf{Choosing \(\lambda\):}
            \begin{itemize}
                \item Use cross-validation to find the best bias-variance trade-off.
                \item Standardize features before applying the penalty terms.
            \end{itemize}
        \item \textbf{Key Takeaway:}
            Regularization improves generalization, reduces overfitting, and can do feature selection (Lasso/Elastic Net).
    \end{itemize}
\end{frame}

% ------------------------------------------------------------
%   Logistic Regression
% ------------------------------------------------------------
\frame{
    \frametitle{Logistic Regression}
    \begin{itemize}
        \item A \textbf{binary classification} model (e.g., yes/no, success/failure).
        \pause
        \item Assumes a linear relationship between predictors \(\mathbf{x}\) and the \emph{log-odds} of the outcome:
        \[
            \log\!\Bigl(\tfrac{p}{1-p}\Bigr) = \beta_0 + \beta_1x_1 + \cdots + \beta_k x_k
        \]
        \pause
        \item The log-odds transformation (logit) ensures predicted probabilities are always in \([0,1]\).
        \pause
        \item Often fit by \emph{maximum likelihood} (minimizing negative log-likelihood).
        \pause
        \item Can be extended to multi-class problems (via One-vs-All or Multinomial/Softmax).
    \end{itemize}
}

% ------------------------------------------------------------
%   Logistic Function
% ------------------------------------------------------------
\frame{
    \frametitle{The Logistic (Sigmoid) Function}
    \begin{itemize}
        \item Maps \(\mathbb{R}\) to \([0,1]\), producing valid probabilities.
        \[
            f(z) = \frac{1}{1 + e^{-z}}
        \]
        \item As \(z \rightarrow \infty\), \(f(z) \rightarrow 1\); as \(z \rightarrow -\infty\), \(f(z) \rightarrow 0\).
        \item Relates directly to the log-odds:
        \[
            p = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
        \]
    \end{itemize}
}

\frame{
    \frametitle{Relation: Log-odds, Probability, and Sigmoid}
    \[
        \log\!\Bigl(\tfrac{p}{1-p}\Bigr) = z = \beta_0 + \beta_1x_1 + \cdots + \beta_kx_k
    \]
    \pause
    \[
        \tfrac{p}{1-p} = e^z \quad \Rightarrow \quad p = \frac{e^z}{1 + e^z}
    \]
    \pause
    \[
        p = \frac{1}{1 + e^{-z}} \quad \text{(the sigmoid)}
    \]
    \begin{itemize}
        \item Changing \(z\) by 1 unit multiplies the odds \(\frac{p}{1-p}\) by \(e^1 \approx 2.718.\)
    \end{itemize}
}

% ------------------------------------------------------------
%   Decision Trees
% ------------------------------------------------------------
\frame{
    \frametitle{Decision Trees}
    \begin{itemize}
        \item A \textbf{non-parametric} method for classification and regression.
        \pause
        \item The data is repeatedly \emph{split} into subsets based on feature thresholds, growing a tree structure.
        \pause
        \item Each \textbf{split} is chosen to maximize the purity of the resulting subsets (minimize impurity).
        \pause
        \item \textbf{Advantages}: Interpretability, handles mixed feature types, no need for feature scaling.
        \item \textbf{Drawbacks}: Can overfit if not pruned, can be unstable (high variance) for small data.
    \end{itemize}
}

\frame{
    \frametitle{Training a Decision Tree}
    \begin{itemize}
        \item Constructed \emph{recursively}:
        \begin{itemize}
            \item Choose a feature + threshold to best split the data (based on impurity).
            \item Split the data into two (or more) subsets.
            \item Repeat the process on each subset until a stopping criterion is met.
        \end{itemize}
        \pause
        \item Stopping criteria:
        \begin{itemize}
            \item Maximum depth reached.
            \item Fewer samples in a node than the minimum required for a split.
            \item Impurity improvement is below a certain threshold.
        \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Impurity Measures: Gini vs. Entropy}
    \begin{itemize}
        \item \textbf{Gini Index} (\(G\)):
            \[
                G = \sum_{i=1}^{C} p_i (1 - p_i) = 1 - \sum_{i=1}^{C} p_i^2
            \]
            \item Measures the chance of misclassification if you pick a label at random.
            \item \(G = 0\) means perfectly pure (all samples in one class).
        \pause
        \item \textbf{Entropy} (\(\mathcal{H}\)):
            \[
                \mathcal{H} = -\sum_{i=1}^{C} p_i \log_2(p_i)
            \]
            \item Based on the concept of information theory (uncertainty).
            \item \(\mathcal{H} = 0\) means a perfectly pure subset.
        \pause
        \item \textbf{Information Gain}: The reduction in impurity from a split.
    \end{itemize}
}

% ------------------------------------------------------------
%   Exercise
% ------------------------------------------------------------
% \frame{
%     \frametitle{Exercise: Recognizing Handwritten Digits}
%     \begin{itemize}
%         \item Use the \texttt{digits} dataset from scikit-learn.
%         \item Apply different classification methods (e.g., Logistic Regression, Decision Tree, KNN, etc.).
%         \item Compare accuracy and see how hyperparameter tuning affects performance.
%     \end{itemize}
%     \vspace{2em}
%     \textbf{Note:} This exercise helps you practice using multiple classification algorithms and gain intuition for model performance on image data.
% }

>>>>>>> a758e4a4267ad4122c0a3bb845bd2bd86f0c3ff5
\end{document}
