
# Dimensionality Reduction with PCA and K-Means Clustering
*Improving insights by reducing dimensions and discovering clusters in data*

% Figure: High-dimensional data being projected onto a lower-dimensional subspace (illustration of dimensionality reduction concept).

## Outline
- **Introduction to Dimensionality Reduction** – Motivation and key concepts (curse of dimensionality, importance of reducing features).
- **Principal Component Analysis (PCA)** – Concept and goals, mathematical foundations, step-by-step algorithm, example and choosing components, practical use cases.
- **K-Means Clustering** – Concept and objectives, algorithm breakdown, practical considerations (choosing *k*, initialization), example use cases.
- **Combining PCA and K-Means** – How PCA preprocessing can improve clustering, example workflows.
- **Extensions and Limitations** – Variants of PCA and K-means (kernel PCA, k-medoids, etc.), assumptions and limitations of each method.
- **Conclusion** – Summary of key points and takeaways.

## Introduction to Dimensionality Reduction
- **Definition:** Dimensionality reduction is the process of reducing the number of features (variables) in a dataset while preserving as much important information as possible. It can be achieved by selecting a subset of original features or by transforming data to a new lower-dimensional space.
- **Motivation:** High-dimensional data can be challenging for both computation and analysis. Many machine learning algorithms suffer from the *curse of dimensionality* – increased features can lead to overfitting, higher computational cost, and difficulty in visualizing data ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=Introduction)) ([The Curse of Dimensionality in Machine Learning: Challenges, Impacts, and Solutions | DataCamp](https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning#:~:text=High,reduce%20dimensions%20for%20visualization%20purposes)). Reducing dimensionality can alleviate these issues by removing redundant features and noise.
- **Benefits:** Fewer dimensions mean simpler models and faster computations, often improving learning performance and generalization. It also enables **visualization** of data in 2D or 3D, which is otherwise impossible for datasets with hundreds of features ([The Curse of Dimensionality in Machine Learning: Challenges, Impacts, and Solutions | DataCamp](https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning#:~:text=High,reduce%20dimensions%20for%20visualization%20purposes)). For example, using PCA or t-SNE to project data down for visualization can reveal cluster structure in the data ([The Curse of Dimensionality in Machine Learning: Challenges, Impacts, and Solutions | DataCamp](https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning#:~:text=High,reduce%20dimensions%20for%20visualization%20purposes)).
- **Approaches:** Dimensionality reduction techniques fall into two categories: (1) *Feature selection* – selecting a subset of existing features based on importance, and (2) *Feature extraction* – creating new features as combinations of original ones (PCA is of this type). In addition, methods can be **linear** (like PCA, which finds linear combinations) or **non-linear** (like t-SNE, UMAP for capturing complex manifolds).
- In this presentation, we focus on **Principal Component Analysis (PCA)**, a widely used linear feature extraction method, and then discuss **K-Means clustering**, an algorithm for grouping data, which often benefits from dimensionality reduction preprocessing.

## PCA: Concept and Goal
- **Principal Component Analysis (PCA)** is a linear dimensionality reduction technique that finds a new set of orthogonal axes (principal components) that capture the maximum variance in the data ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=algorithms,retaining%20most%20of%20the%20information)). In essence, PCA seeks the directions in the feature space along which the data varies the most, and uses those directions as new axes while discarding others with minimal loss of information.
- **How it works (intuitively):** PCA identifies patterns in data by detecting correlations between original features. If variables are strongly correlated, PCA can replace them with a single principal component that represents their common information. It projects the data onto a lower-dimensional subspace (spanned by the leading principal components) that retains most of the variability (signal) in the dataset ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=algorithms,retaining%20most%20of%20the%20information)).
- The principal components are **orthogonal** (perpendicular) directions. The first principal component is the direction that maximizes the variance of the projected data (it captures the largest variation in the dataset). The second principal component is the orthogonal direction that captures the largest remaining variance, and so on ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=and%20it%20is%20desirable%20to,variance%20of%20the%20projected%20data)). This means each subsequent component explains variance that was not captured by earlier components, with all components being uncorrelated with each other.
- **Key idea:** PCA does not “throw away” data arbitrarily; it finds an optimal lower-dimensional representation in the sense of maximum variance preservation. It is proven that PCA yields the best *k*-dimensional approximation of the data in terms of least-squares reconstruction error, under the assumption of linearity. In other words, among all possible ways to project data onto *k* dimensions, PCA’s choice loses the least information (maximizes variance kept) ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Dimensionality%20reduction%20results%20in%20a,certain%20signal%20and%20noise%20models)).
- PCA is an **unsupervised** technique (it ignores class labels) and is often used as a preprocessing step or for exploratory data analysis. For example, many studies use the first two principal components to plot high-dimensional data in two dimensions and visually identify clusters or patterns ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=i,1)), which can be especially useful in understanding the structure of the data.

% Figure: Illustration of PCA on a 2D dataset. A scatter plot of data with an arrow showing the direction of the first principal component (max variance direction) and projection of points onto this axis.

## PCA: Mathematical Foundation
- **Data preparation:** To perform PCA, we usually start by mean-centering the data (subtracting the mean of each feature from the dataset) so that the data has zero mean. Often, we also **standardize** features (scale to unit variance) especially if they are on different scales, since PCA is sensitive to feature scaling ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=dimensionality%20reduction%20techniques%20tend%20to,more%20computationally%20demanding%20than%20PCA)) (variables with larger scales can dominate the variance).
- **Covariance matrix:** Compute the covariance matrix of the data, \( \Sigma = \frac{1}{n}\mathbf{X}^T \mathbf{X} \) (for mean-centered data, \(\Sigma_{ij}\) is the covariance between feature *i* and *j*). The covariance matrix captures how variables co-vary. If features are correlated, \(\Sigma\) will have large off-diagonal values indicating redundancy that PCA can exploit.
- **Eigen-decomposition:** We solve the eigenvalue problem \( \Sigma \mathbf{v} = \lambda \mathbf{v} \). The solutions give a set of eigenvectors \(\mathbf{v}_1, \mathbf{v}_2, \dots\) (principal component directions) and their corresponding eigenvalues \(\lambda_1, \lambda_2, \dots\) (which equal the variance of the data along those directions). The eigenvectors (unit vectors) form an orthonormal basis, and the eigenvalues tell us how much variance is along each direction ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=and%20it%20is%20desirable%20to,variance%20of%20the%20projected%20data)) ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=For%20either%20objective%2C%20it%20can,CCA%20defines%20coordinate%20systems%20that)). By convention, we sort eigenvalues in descending order: \(\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d\).
- **Principal components:** The eigenvector with the largest eigenvalue is the first principal component (direction of maximum variance). The next eigenvector (orthogonal to the first) with the second-largest eigenvalue is the second principal component, and so on ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=and%20it%20is%20desirable%20to,variance%20of%20the%20projected%20data)). If we choose the top *k* eigenvectors as principal axes, we are effectively projecting the data onto a *k*-dimensional subspace that captures the most variance. Each original data point can be transformed (projected) into coordinates in this new subspace.
- **Eigendecomposition vs. SVD:** In practice, PCA can be computed via eigendecomposition of \(\Sigma\) or directly via singular value decomposition (SVD) of the data matrix \(\mathbf{X}\) ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=For%20either%20objective%2C%20it%20can,CCA%20defines%20coordinate%20systems%20that)). SVD is often used for numerical stability and efficiency, and it yields singular values related to the eigenvalues of \(\Sigma\). Regardless of method, the result is the set of principal components (eigenvectors).
- **Remark:** PCA was originally formulated by Karl Pearson in 1901 and later developed by Harold Hotelling in 1933 ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=PCA%20was%20invented%20in%201901,for%20a%20discussion%20of%20the)). It is also known as the *Karhunen–Loève transform* in signal processing and is one of the foundational techniques in multivariate analysis.

## PCA: Algorithm Steps
1. **Standardize the data** – Ensure each feature has mean 0 (and optionally unit variance). This step is crucial if features have different units or scales ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=dimensionality%20reduction%20techniques%20tend%20to,more%20computationally%20demanding%20than%20PCA)).
2. **Compute covariance matrix** – Calculate the \(d \times d\) covariance matrix \(\Sigma\) of the normalized data (or alternatively, compute the correlation matrix if standardization was applied).
3. **Eigen-decomposition** – Compute the eigenvalues and eigenvectors of the covariance matrix \(\Sigma\). This can be done via eigen-decomposition or via SVD on the data matrix ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=For%20either%20objective%2C%20it%20can,CCA%20defines%20coordinate%20systems%20that)). We obtain eigenpairs \((\lambda_i, \mathbf{v}_i)\) for \(i=1,\dots,d\).
4. **Sort eigenpairs** – Sort the eigenvalues in descending order and order the eigenvectors correspondingly. Decide how many principal components *k* to keep. This choice can be based on the cumulative explained variance (see next slide). Typically, choose the top *k* eigenvectors such that they account for a sufficiently large proportion of total variance.
5. **Project the data** – Form a projection matrix \(\mathbf{W}\) using the selected top *k* eigenvectors as columns. Multiply the original data matrix \(\mathbf{X}\) by \(\mathbf{W}\) to obtain the reduced dataset \(\mathbf{Y} = \mathbf{X}\mathbf{W}\), which has *k* dimensions ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=,W%7D%5C%29%20to%20obtain%20a)). Each row of \(\mathbf{Y}\) is the representation of the corresponding data point in the *k*-dimensional principal component space.
6. **Use the results** – The transformed data \(\mathbf{Y}\) can now be used for visualization, as input to other algorithms (classification, clustering), or for storage/compression. The principal components (columns of \(\mathbf{W}\)) themselves can be analyzed to understand which combinations of original features form the new axes.

*These steps summarize the standard PCA procedure ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=,W%7D%5C%29%20to%20obtain%20a)). Modern software (e.g., scikit-learn’s PCA implementation) performs these steps internally given a dataset and the number of components *k*.*

% Figure: Flowchart of PCA process – data standardization, computing covariance matrix, eigen-decomposition, selecting top components, and projecting data.

## PCA: Example and Choosing Number of Components
- **Explained variance:** A crucial question in PCA is “How many principal components should we keep?” We answer this by looking at the **explained variance** of each component ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=After%20sorting%20the%20eigenpairs%2C%20the,new%20feature%20subspace%3F%E2%80%9D%20A%20useful)). The *explained variance* of the $i$-th principal component is the proportion of the dataset’s total variance captured by that component. It is computed from the eigenvalues. If $\lambda_1, \dots, \lambda_d$ are the eigenvalues of the covariance matrix, then the percentage of variance explained by the $i$-th component is $\lambda_i / \sum_{j=1}^d \lambda_j$ (or as a percentage, multiply by 100%).
- We often examine the **cumulative explained variance** as we include more components. By plotting the cumulative variance vs. number of components (a scree plot or elbow plot for PCA), we can see how many components are needed to capture, say, 90% or 95% of the variance. We choose the smallest $k$ such that the cumulative explained variance is above our desired threshold.
- **Example (Iris dataset):** The Iris flower dataset has 4 features. If we perform PCA on this dataset, we get four eigenvalues. It turns out that the first principal component alone explains about 72.77% of the variance, and the second explains about 23.03%. Together, the first two components account for **95.8%** of the total variance ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=The%20plot%20above%20clearly%20shows,of%20the%20information)). This means we can reduce the 4-dimensional data to 2 dimensions and still retain almost all the information. In practice, one would likely choose $k=2$ components for analysis or visualization of the Iris dataset.
- **Choosing $k$:** In general, one might choose $k$ such that (a) the cumulative explained variance exceeds a threshold (e.g., 95%), or (b) the eigenvalues beyond the $k$-th are very small (approaching noise level). The scree plot often shows an “elbow” point where additional components have diminishing returns. That elbow is a good heuristic for $k$. For example, if the first 10 components explain 90% variance and adding an 11th gives only marginal increase, one might select 10 components. (It’s worth noting the “elbow” location can be subjective ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,29)).)
- **Trade-off:** Using more components retains more information but yields less dimensionality reduction. Using fewer components yields more compression but risks losing important information. PCA provides a principled way to balance this by quantifying information loss via explained variance.

% Figure: Scree plot showing eigenvalues or explained variance vs. component number. An “elbow” point indicates an optimal number of components where additional components have diminishing returns.

% Figure: 2D scatter plot of the Iris dataset in the space of the first two principal components, with points labeled by species. (Demonstrates that PCA can reveal clustering structure corresponding to species.)

## PCA: Applications and Use Cases
- **Data Visualization:** PCA is widely used to visualize high-dimensional datasets by projecting them into 2D or 3D. Plotting the first two or three principal components often reveals clustering or patterns in the data ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=i,1)). For instance, in genomics, PCA plots can show how individuals cluster by genetic similarity; in marketing, PCA can help visualize customer segments.
- **Preprocessing and Feature Reduction:** In machine learning pipelines, PCA is often applied to reduce the feature dimension before feeding data into a model. This can remove multicollinearity (since principal components are uncorrelated) and reduce overfitting. A smaller feature set also means faster training and improved performance in some cases. For example, in regression or classification tasks, using PCA to compress features can improve stability of the model if many features are noisy or redundant.
- **Noise Filtering:** PCA can act as a noise filter by discarding components with very low variance, which often represent noise. By reconstructing data using only the top *k* components, we eliminate minor variance components (assumed to be noise) and thus obtain a smoother, denoised dataset.
- **Compression and Reconstruction:** PCA is effectively a compression method. A classic example is image compression: treating a set of images as data vectors, PCA can find a basis of “eigenimages.” The **Eigenfaces** method for face recognition is a famous application, where PCA is applied to a large set of face images. The principal components (eigenfaces) capture the main variations in faces ([](http://faculty.washington.edu/sbrunton/me565/pdf/L29secure.pdf#:~:text=One%20of%20the%20most%20striking,with%20each%20of%20the%20principal)). Any face image can be approximately reconstructed from a small number of these eigenfaces, achieving compression. Moreover, images of the same person tend to cluster in the PCA space defined by eigenfaces ([](http://faculty.washington.edu/sbrunton/me565/pdf/L29secure.pdf#:~:text=this%20decomposition%20is%20a%20set,first%20studied%20by%20Sirovich%20and)), aiding face recognition tasks.
- **Domains of use:** PCA has broad applications across science and engineering. It has been used in **population genetics** to infer population structure, in **finance** to reduce risk factors, in **computer vision** (e.g., eigenfaces for facial recognition), in **neuroscience** for analyzing neural signal patterns, and in **chemometrics** for spectral data analysis. In microbiome studies and atmospheric science, PCA helps to identify principal modes of variation in complex environmental datasets ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Principal%20component%20analysis%20has%20applications,2)). Its versatility and relative simplicity make PCA a go-to tool for exploratory data analysis in many fields.

% Figure: Examples of PCA applications. (Left) A set of eigenfaces (principal components) derived from a face image dataset. (Right) Projection of complex data onto first two principal components, enabling 2D visualization of clusters.

## K-Means Clustering: Concept and Goal
- **Clustering** is the task of grouping a set of objects such that those in the same group (cluster) are more similar to each other than to those in other groups. **K-Means** is one of the simplest and most popular clustering algorithms in unsupervised machine learning ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=K,points%20into%20groups%20or%20clusters)).
- **K-means definition:** Given a dataset of $n$ observations (data points) in a $d$-dimensional space, K-means aims to partition the data into $k$ distinct clusters (where $k$ is a predefined number). Each cluster is defined by its **centroid** (the mean of points in that cluster), and each data point belongs to the cluster with the nearest centroid ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=Given%20a%20set%20of%20observations,of%20points%20in)). In other words, K-means finds $k$ cluster centers and assigns every point to the nearest center, effectively forming Voronoi partitions of the space.
- **“Hard” clustering:** K-means produces an *exclusive* or “hard” clustering ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=While%20various%20types%20of%20clustering,is%20efficient%2C%20effective%20and%20simple)) – each data point is assigned to exactly one cluster (no overlaps). This contrasts with fuzzy clustering (where points have degrees of membership in multiple clusters) or hierarchical clustering (which creates nested clusters). In K-means, the clusters are flat (unrelated to each other except by their distinct members) and non-overlapping ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=While%20various%20types%20of%20clustering,is%20efficient%2C%20effective%20and%20simple)).
- **Unsupervised learning:** Like PCA, K-means is unsupervised. It discovers patterns (clusters) without any labels. The hope is that clusters correspond to some meaningful grouping (e.g., customer segments, document topics, etc.). However, K-means will always create *k* clusters even if the data doesn’t naturally have *k* groups; it’s up to the user to interpret the clusters.
- **Popularity:** K-means is popular because it is conceptually simple and computationally efficient on large datasets. It often yields useful results in practice, especially when clusters are roughly spherical in shape. It’s used in a variety of domains – from image compression to market segmentation – due to its efficiency and effectiveness ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=analysis%20is%20commonly%20used%20in,is%20efficient%2C%20effective%20and%20simple)). The algorithm typically runs in time linear in $n$ (number of points) for a given $k$ and converges quickly.
- **Note on $k$:** The number of clusters $k$ must be chosen in advance (or determined through trial and evaluation, see later slide on choosing *k*). The choice of $k$ can significantly influence the outcome; a poor choice can lead to meaningless clusters.

% Figure: Points in a 2D space partitioned into 3 clusters by K-means. Each cluster is indicated by a different color, and the centroid of each cluster is marked (showing how points are assigned to the nearest centroid).

## K-Means: Objective and Properties
- **Clustering objective:** K-means tries to find the clustering that best represents the data in terms of cluster “compactness.” Formally, it attempts to minimize the **within-cluster sum of squares (WCSS)**, also known as cluster inertia or sum of squared errors. The WCSS is defined as the sum of the squared Euclidean distances of each point to its cluster’s centroid ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=Given%20a%20set%20of%20observations,of%20points%20in)). If $S_1, S_2, \dots, S_k$ denote the $k$ clusters with centroids $\mu_1, \dots, \mu_k$, the objective is: 
  $$\min_{S_1,\dots,S_k} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_i} \|\mathbf{x} - \mu_i\|^2,$$ 
  which is the same as minimizing the variance within each cluster ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=Given%20a%20set%20of%20observations,of%20points%20in)). Intuitively, we want clusters where points are very close to their centroid (and hence to each other).
- **Cluster centroids:** The centroid $\mu_i$ of cluster $S_i$ is simply the arithmetic mean of all points in $S_i$. K-means implicitly assumes clusters can be represented by their mean. This works well for “isotropic” or blob-shaped clusters in Euclidean space. It also means K-means is restricted to numeric data where a mean is meaningful (it wouldn’t apply directly to categorical data without some numeric encoding).
- **Algorithm (Lloyd’s algorithm):** The standard K-means algorithm is an iterative refinement procedure (described in detail on the next slide). It alternates between assigning points to the nearest clusters and recomputing centroids, and it will converge to a solution where no point would switch clusters upon further iterations ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20objective%20function%20in%20k,necessarily%20to%20the%20global%20optimum)). Each iteration guarantees to decrease the WCSS (or leave it unchanged once converged) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20objective%20function%20in%20k,necessarily%20to%20the%20global%20optimum)).
- **Convergence and optimum:** K-means will always converge in a finite number of steps (because there are only finitely many ways to assign $n$ points to $k$ clusters). However, the solution it reaches is generally a **local optimum** of the WCSS objective, not necessarily the global optimum ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20objective%20function%20in%20k,necessarily%20to%20the%20global%20optimum)). The outcome can depend on the initial placement of centroids. Because the WCSS objective is non-convex and the problem of finding the global minimum is NP-hard in general ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20problem%20is%20computationally%20difficult,clusters%20to%20have%20different%20shapes)), the algorithm’s result may not be the absolute best clustering possible, but typically it is a good approximation.
- **Complexity:** The clustering problem itself (for arbitrary $k$) is NP-hard, but the K-means algorithm is a heuristic that is efficient in practice ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20problem%20is%20computationally%20difficult,clusters%20to%20have%20different%20shapes)). Each iteration involves computing distances for all points to centroids (O($nkd$) operations for $d$-dimensional data) and updating centroids. In practice, K-means is quite fast and scales to large $n$. The number of iterations until convergence is usually small. However, one must be cautious that K-means could converge to a suboptimal configuration; hence, it’s common to run the algorithm multiple times with different initializations and keep the best result (lowest WCSS).
- **Cluster shape assumption:** K-means implicitly assumes clusters are roughly spherical (or at least convex) in the feature space, due to using the mean and Euclidean distance. It tends to partition the space into Voronoi cells around centroids. If true clusters are elongated or non-convex (e.g., a crescent shape), K-means may perform poorly because it can only produce convex (Voronoi) regions.

## K-Means: Algorithm Steps (Lloyd’s Algorithm)
1. **Initialization:** Choose $k$ initial cluster centroids. This can be done randomly (pick $k$ random points from the dataset as initial means) or via smarter methods like *k-means++* (which spreads out initial centers). Initialization matters because a bad starting point can lead to a poor local optimum. (With k-means++ initialization, the algorithm generally finds better clusters and converges faster by reducing the chance of poor initial seeds.)
2. **Assignment step:** Assign each data point to the nearest centroid, using a distance metric (typically Euclidean distance). After this step, we have $k$ clusters formed by the points assigned to each centroid ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=1,centroids%29%20for%20observations%20assigned)). Mathematically, for each point we find $\arg\min_{i} \|\mathbf{x} - \mu_i\|$, and assign the point to that cluster $i$. This creates a partition of the data based on proximity to the current centroids (geometrically, this is equivalent to constructing the Voronoi diagram for the centroids) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=1,centroids%29%20for%20observations%20assigned)).
3. **Update step:** Recalculate each centroid $\mu_i$ as the mean of all points assigned to cluster $i$ in the assignment step ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=1,centroids%29%20for%20observations%20assigned)). That is, $\mu_i = \frac{1}{|S_i|} \sum_{\mathbf{x} \in S_i} \mathbf{x}$. This gives a new set of centroids.
4. **Iterate:** Repeat the Assignment and Update steps. After each full iteration, the centroids move to better fit the clusters, and points may get reassigned. The algorithm iterates until convergence – e.g., until no points change clusters (assignment no longer changes) or until the changes in centroids or WCSS fall below a small threshold ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20objective%20function%20in%20k,necessarily%20to%20the%20global%20optimum)). In practice, a maximum number of iterations can be set as a stopping criterion as well.
5. **Result:** The output is the final set of $k$ clusters, defined by their members and centroids. Often, one also reports the final WCSS (sum of squared distances within clusters) as a measure of cluster compactness achieved.
- **Convergence:** Because WCSS decreases with each iteration and is bounded below (by 0), the algorithm is guaranteed to converge ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20objective%20function%20in%20k,necessarily%20to%20the%20global%20optimum)). However, as noted, the convergence point can depend on initialization. It’s common to run K-means multiple times and choose the clustering with the lowest final WCSS to mitigate the issue of local minima.

% Figure: Illustration of K-means clustering process. (1) Initial random centroids on a dataset. (2) Points assigned to nearest centroids (clusters formed). (3) Centroids recomputed. (4) Points reassign around new centroids, and so on, until convergence. Arrows can show centroid movements between iterations.

## K-Means: Practical Considerations
- **Initialization sensitivity:** K-means results can vary with different initial centroid positions. The algorithm is **non-deterministic** when using random initialization – running it twice on the same data can yield different clusters ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=results)). A poor initialization might lead to a suboptimal clustering (higher final WCSS). To improve consistency, use strategies like **k-means++** (which initializes centroids in a way that they are distant from each other) or run the algorithm multiple times and pick the best outcome. Many implementations allow specifying the number of restarts (random initial seeds) and automatically return the best clustering found.
- **Choosing $k$:** The number of clusters $k$ is a critical parameter that is usually not known a priori. Choosing $k$ too low can force dissimilar points into the same cluster; too high can split natural groups unnecessarily or even assign isolated outliers as separate clusters. Techniques for determining a good $k$ include the elbow method and silhouette analysis (covered on the next slide). In practice, one might try several values of $k$ and use domain knowledge and validation metrics to decide ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,29)) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,poorly%20matched%20to%20neighboring%20clusters)).
- **Feature scaling:** It’s important to scale features appropriately before clustering. Since K-means uses Euclidean distance, features with larger scales or units can dominate the distance calculation. Standardizing features to zero mean and unit variance (or normalizing ranges) is recommended if features are on different scales (e.g., age in [0,100] vs. income in [0,100000]) so that no single feature unduly influences the clusters.
- **Outliers:** K-means is sensitive to outliers and noise ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=K,the%20algorithm%20by%20using%20evaluation)). An outlier far from all centroids can significantly increase WCSS and might end up as its own cluster or skew a centroid’s position. It’s often useful to detect and remove extreme outliers before clustering, or use a more robust clustering method (like k-medoids) if outliers are a concern. Alternatively, one can set a rule to discard small clusters that may consist of anomalies.
- **Cluster shape:** Remember that K-means assumes clusters are convex and roughly equal in size. If your data has clusters with very different sizes or non-convex shapes, K-means may merge or split them inappropriately. Always visually inspect clusters if possible or evaluate with metrics. For non-spherical clusters, consider alternative algorithms (e.g., DBSCAN for arbitrary shapes).
- **Empty clusters:** In some iterations, it’s possible (though uncommon with large $n$) that a cluster ends up with no points (especially if $k$ is large or after an outlier is removed). Most K-means implementations handle this by re-initializing that centroid to a different position (e.g., another random point or the furthest point from a large centroid).
- **Performance:** K-means is efficient, but it can still be computationally heavy for very large datasets or very large $k$. There are optimized versions (mini-batch K-means, which uses random subsets of data in each iteration, and yields approximate results much faster) and methods to accelerate distance computations. Also, as $d$ (number of features) grows, distance calculations become more expensive and also less meaningful (curse of dimensionality affects distance-based methods). Applying PCA to reduce $d$ before K-means can help in such cases by speeding up computation and potentially improving cluster quality ([clustering before or after PCA? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/52016/clustering-before-or-after-pca#:~:text=2,are%20informative%20then%20dimensionality%20reduction)).

## K-Means: Applications and Use Cases
- **Market Segmentation:** In marketing and business analytics, K-means is widely used to segment customers into distinct groups based on purchasing behavior, demographics, or interests ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=analysis%20is%20commonly%20used%20in,is%20efficient%2C%20effective%20and%20simple)). For example, clustering customers by their purchase history might reveal a group of bargain shoppers versus luxury shoppers, enabling targeted marketing strategies.
- **Document Clustering:** In natural language processing or information retrieval, K-means can cluster documents (or news articles, etc.) by topic. By embedding documents in a vector space (e.g., using TF-IDF or word embeddings), K-means finds groups of similar documents. This can be used for organizing information or building recommender systems (grouping similar content together).
- **Image Segmentation and Compression:** K-means is frequently applied in image processing to partition an image into $k$ regions (image segmentation). By clustering the color intensities or pixel features, one can separate an image into different objects or backgrounds. Similarly, K-means can compress images by clustering pixels in color space and replacing each cluster with its centroid color (reducing the number of distinct colors) ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=analysis%20is%20commonly%20used%20in,is%20efficient%2C%20effective%20and%20simple)). For instance, using K-means on the RGB values of pixels, an image can be quantized to $k$ colors (this is effectively vector quantization).
- **Biology and Medicine:** Clustering is used to find patterns in biological data. K-means can cluster gene expression profiles to find groups of genes with similar expression, or cluster patients based on various clinical measurements to identify subtypes of a disease. While K-means might not capture complex patterns, it serves as a baseline for quick grouping in high-throughput data analysis.
- **Anomaly Detection:** Sometimes K-means is used as a rudimentary anomaly detector: after clustering, points that do not strongly belong to any cluster (e.g., far from all centroids) might be considered outliers. However, specialized algorithms are usually preferred for anomaly detection.
- **General exploratory analysis:** Any time we want to discover natural groupings in unlabelled data, K-means is an option. It has been applied in areas from astronomy (clustering stars/galaxies by properties) to sociology (grouping survey respondents) due to its simplicity. Its efficiency allows it to handle large datasets that might be infeasible for more complex clustering methods.
- **Efficiency note:** The reason for K-means’ ubiquity is partly because of how fast and scalable it is. It is often the first clustering method tried on a new problem ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=analysis%20is%20commonly%20used%20in,is%20efficient%2C%20effective%20and%20simple)), to get a quick sense of structure, before refining with more advanced or domain-specific clustering methods if needed.

% Figure: Example applications of K-means. (Left) Image compression: original image vs. K-means compressed image using 5 colors. (Right) Customer segments in a 2D feature space, colored by cluster.

## Choosing the Number of Clusters (Determining *k*)
- Deciding on the right number of clusters *k* is a fundamental challenge in K-means clustering, since the algorithm requires *k* as an input. There is often no “correct” answer; *k* should be guided by the data and the context of the problem. However, several techniques can help inform the choice of *k*:
- **Elbow Method:** This heuristic involves running K-means for a range of values of *k* (say 1 through 10 or 1 through 20) and plotting the resulting total WCSS (or equivalently, inertia or SSE) as a function of *k*. As *k* increases, WCSS will always decrease (more clusters, smaller within-cluster variance). Initially, adding clusters greatly reduces WCSS, but after a certain point the marginal gain decreases. We look for an “elbow” in the curve – a point after which the rate of decrease sharply levels off ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,29)). The elbow (if discernible) is taken as the appropriate number of clusters. **Caveat:** The elbow method doesn’t always yield a clear answer; sometimes the curve is smooth without a sharp elbow, making it subjective.
- **Silhouette Analysis:** The silhouette score is a measure of how well-separated the clusters are. For each point, the silhouette score is defined as $(b - a) / \max(a,b)$, where *a* is the average distance to other points in the same cluster, and *b* is the average distance to points in the nearest different cluster. The score ranges from -1 to 1; a higher silhouette means the point is closer to its own cluster than to others (well-clustered) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,poorly%20matched%20to%20neighboring%20clusters)). We can compute the average silhouette score for the entire dataset for different *k*. The *k* that gives the highest average silhouette is often considered the best. Silhouette analysis not only suggests a good *k* but also indicates if clusters are well-separated (scores near 1) or not (scores near 0 or negative).
- **Gap Statistic and Other Methods:** More advanced statistical methods compare the within-cluster dispersion to that expected under a null reference distribution of the data. The gap statistic essentially standardizes WCSS by comparing it to an expected WCSS if points were uniformly random, and chooses *k* where the gap is largest. There are also information-theoretic criteria (AIC, BIC when using mixture models) to select *k*. These methods can be more robust but are beyond the scope of this presentation.
- **Domain Knowledge:** Often, practical considerations or domain knowledge dictate *k*. For example, if clustering customers, the marketing team might want exactly 4 segments for a campaign. Or if clustering image regions, one might know there are, say, 3 tissue types in a medical image. Using domain insight can guide or validate the choice of *k*.
- **Validation:** Regardless of the method, it’s good practice to validate clustering results. One can hold out some data and see if cluster assignments remain stable (though clustering doesn’t have a training/test split in the usual sense). Another approach is to see if the clusters correlate with some external ground truth or meaningful pattern (if available).
- Ultimately, choosing *k* is part of the exploratory analysis. Many analysts will try multiple *k* values and examine the results for interpretability and stability, in addition to using the above quantitative methods as guides ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,a%20feature%20may%20have%20different)).

% Figure: Plot of K-means inertia (SSE) vs. number of clusters k, illustrating the elbow method. The curve shows a sharp drop until k=4, then flattens out, suggesting 4 clusters may be optimal.

% Figure: Illustration of silhouette scores for points in a clustering. Perhaps a bar chart of average silhouette for different k values, highlighting the best k with highest silhouette.

## Combining PCA and K-Means
- PCA and K-means are often used **together** in data analysis workflows – PCA as a preprocessing step before clustering. This leverages the strengths of both: PCA reduces dimensionality and noise, and K-means then clusters the data in this simplified space. There are several reasons and scenarios for combining them:
- **Clustering high-dimensional data:** When the data has many features, distance computations in K-means become less reliable (due to curse of dimensionality) and more computationally expensive. Applying PCA to reduce the number of dimensions can focus on the most informative features and filter out noise, leading to improved clustering results ([clustering before or after PCA? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/52016/clustering-before-or-after-pca#:~:text=2,are%20informative%20then%20dimensionality%20reduction)). By reducing redundancy, we often get more well-separated clusters. Empirically, clustering in the space of principal components can yield more pronounced clusters than clustering in the original space, especially if many original features were irrelevant or noisy ([clustering before or after PCA? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/52016/clustering-before-or-after-pca#:~:text=2,are%20informative%20then%20dimensionality%20reduction)).
- **Visualization and interpretation:** PCA can compress data to 2D or 3D, and then K-means can be applied in that space. One can visualize the PCA-projected data with points colored by their cluster assignments. This helps interpret clusters. If clusters are nicely separated in the PCA plot, it validates the clustering. If not, perhaps the data doesn’t naturally cluster well or more components are needed.
- **Speed:** Running K-means on a dataset with, say, 1000 dimensions can be slow. If PCA can reduce it to 10 dimensions while retaining most variance, the K-means assignment steps become *100x* faster. This can be important for big data. Many practical clustering pipelines thus include a PCA (or similar) dimensionality reduction beforehand for efficiency.
- **Denoising:** Imagine we have a dataset where the true clusters differ only subtly, but there is a lot of background noise in many features. PCA will tend to put that noise into lower-variance components. By truncating those, we effectively denoise the data. K-means on the denoised data can then detect the subtle clustering signal more easily. This is common in fields like genetics, where thousands of gene expression values might have only a few principal components explaining biologically meaningful variation.
- **Example – image clustering:** Suppose we want to cluster a collection of images into groups (perhaps by object type). Each image could be represented by thousands of pixel values or features. PCA can reduce each image to a vector of principal component coefficients (sometimes these are called “eigenfeatures”). If we keep, say, 50 principal components, we dramatically compress the data. K-means can then cluster these 50-dimensional vectors to group similar images. The combination might succeed in grouping images by content (e.g., all images of dogs in one cluster, landscapes in another) if the principal components capture those high-level variations.
- **Real-world observation:** In the earlier mentioned **Eigenfaces** example, after applying PCA to faces, it was noted that images of the same person cluster together in the PCA feature space ([](http://faculty.washington.edu/sbrunton/me565/pdf/L29secure.pdf#:~:text=this%20decomposition%20is%20a%20set,first%20studied%20by%20Sirovich%20and)). While that particular study used the PCA space for a nearest-neighbor classifier, one could also directly apply a clustering algorithm in that space to group images by person. This highlights that PCA can create a feature space where simple algorithms like K-means can perform well in separating categories.
- **Caution:** When combining PCA and K-means, one should ensure not to reduce dimensions *too* much such that important cluster-distinguishing information is lost. If you compress to very few components, some finer cluster structure might vanish. It’s about finding a sweet spot: enough components to preserve cluster structure, but few enough to simplify the task. Using explained variance as a guide (retain, e.g., 90% variance) is a good heuristic.
- In summary, PCA and K-means together form a powerful duo: PCA sets the stage by revealing the underlying structure in the data, and K-means concretely identifies cluster groupings. Indeed, many modern techniques for complex data rely on a similar two-step approach (representation learning followed by clustering), and historically numerous nonlinear dimensionality reduction methods trace their roots back to PCA or K-means ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Most%20of%20the%20modern%20methods,88)).

% Figure: A pipeline diagram showing data -> PCA -> reduced data -> K-means -> clusters. Alternatively, a scatter plot of data in 2D PCA space with clusters circled to illustrate the combination.

## Extensions and Variants of PCA and K-Means
- **PCA Variants:**
  - *Kernel PCA:* PCA is linear, but **Kernel PCA** extends it to capture nonlinear structure by first implicitly mapping data into a higher-dimensional feature space using a kernel function, then performing PCA there ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=approximation%20%20followed%20by%20,with%20a%20positive%20definite%20kernel)). This allows extraction of non-linear principal components (e.g., separating data that lies on a non-linear manifold).
  - *Sparse PCA:* Introduces a penalty so that principal components have only a few non-zero loadings (for easier interpretability).
  - *Incremental PCA:* Allows PCA computation on streaming data or in batches, useful for very large datasets that don’t fit in memory at once.
  - *Robust PCA:* Uses techniques like L1-norm minimization to make PCA less sensitive to outliers ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=some%20contexts%2C%20outliers%20can%20be,based%20on%20their%20estimated%20relevancy)).
  - Other linear techniques related to PCA include **Factor Analysis** (which has a probabilistic model and separates common vs. unique variance) and **Independent Component Analysis (ICA)** (which finds components that are statistically independent rather than just uncorrelated).
  - There are also many other dimensionality reduction methods: **Linear Discriminant Analysis (LDA)** for supervised dimensionality reduction, and **t-SNE** and **UMAP** for nonlinear embedding primarily used in visualization. Many of these methods, in concept, build upon ideas of variance maximization or clustering akin to PCA and K-means at their core ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Most%20of%20the%20modern%20methods,88)).
- **K-Means Variants and Alternatives:**
  - *K-Means++:* An improved initialization procedure for K-means that chooses initial centroids in a probabilistic way to spread them out. This often leads to better solutions (lower WCSS) and faster convergence.
  - *K-Medoids (PAM):* A related algorithm where cluster centers are restricted to be actual data points (medoids) instead of the mean. This is more robust to outliers (because a medoid won’t be dragged by outliers as a mean would) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20algorithm%20is%20often%20presented,allow%20using%20other%20distance%20measures)). K-medoids is suitable when mean might not be meaningful (e.g., clustering categorical data by using a representative member).
  - *K-Medians:* Uses median instead of mean (useful for certain distance metrics or to reduce outlier effect).
  - *Fuzzy C-Means:* A soft clustering version of K-means where each point has a degree of membership in each cluster (useful when cluster boundaries are not crisp).
  - *Hierarchical Clustering:* An alternative approach that builds a tree of clusters either by successive merging (agglomerative) or splitting (divisive). It doesn’t require choosing *k* upfront, though you eventually cut the tree at some level to obtain clusters. Hierarchical methods can capture nested clusters and don’t assume equal-size clusters.
  - *Gaussian Mixture Models (GMM):* A probabilistic clustering that assumes clusters are Gaussian distributed. It can be seen as a soft version of K-means (points belong to clusters with certain probabilities) and can handle clusters with different shapes and sizes (through covariance matrices) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=mixtures%20%20of%20%20237,clusters%20to%20have%20different%20shapes)). K-means is actually a special case of GMM (with spherical equal-variance Gaussians and hard assignments).
  - *Density-Based Clustering:* Methods like DBSCAN and OPTICS form clusters based on dense regions in the data, and can find arbitrarily shaped clusters, unlike K-means which fails on non-convex shapes.
  - *High-dimensional clustering:* For very high-dimensional data, methods like subspace clustering or spectral clustering (which involves eigen-decomposition of a similarity matrix) might perform better. Interestingly, spectral clustering’s math involves eigenvectors of a matrix (similar spirit to PCA) and often one can interpret it as PCA on an affinity matrix followed by K-means in that reduced space.
- Many modern unsupervised learning techniques build on or incorporate PCA/K-means. For instance, the widely-used t-SNE for visualization often benefits from an initial PCA step. Likewise, deep learning methods (autoencoders) reduce dimensionality, and one can cluster in the low-dimensional latent space. The theoretical foundations of numerous non-linear methods can be traced back to principles of PCA (variance, projection) or K-means (centroid-based partitioning) ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Most%20of%20the%20modern%20methods,88)).

## Limitations of PCA
- **Linearity:** PCA can only capture linear relationships in the data. If the true structure of the data lies on a nonlinear manifold, PCA may require many components to approximate it or may fail to capture it entirely. For example, PCA would struggle with data arranged in a curved “S” shape in 3D – the first PCs will still be linear axes that cannot fully unwrap the nonlinear shape. In such cases, techniques like kernel PCA or other nonlinear dimensionality reduction are needed.
- **Feature scaling and units:** PCA is sensitive to how variables are scaled ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=dimensionality%20reduction%20techniques%20tend%20to,more%20computationally%20demanding%20than%20PCA)). If one feature has a variance 100 times larger than another (perhaps due to units or outliers), it will dominate the first principal component. It is essential to standardize features (zero mean, unit variance) before PCA, otherwise the principal components might largely reflect the scale of features rather than true patterns. PCA on unscaled data can yield misleading results if units are not comparable.
- **Interpretability:** The principal components are linear combinations of original features, which can sometimes be hard to interpret. For instance, if the first principal component is $0.7 \times (\text{height}) + 0.7 \times (\text{width})$ (normalized units), it might be interpretable as “size”. But often with many features, principal components mix dozens of original features, which makes it challenging to assign a simple meaning to each component. This is a trade-off: we get simplicity in terms of fewer dimensions, but those dimensions are abstract.
- **Information loss:** Unless we keep all $d$ components, PCA is a lossy compression. By choosing a reduced number of components $k < d$, some variance (information) is inevitably lost ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Dimensionality%20reduction%20results%20in%20a,certain%20signal%20and%20noise%20models)). We hope it’s mostly noise or redundant information, but there is always a possibility that discarded components carry something important for a specific task (especially if that task wasn’t considered in the PCA criterion). One should check the explained variance to gauge how much information might be lost.
- **Outlier sensitivity:** PCA uses variance as a criterion, which is greatly affected by outliers. An outlier (an unusual data point) can have a large influence on the covariance matrix and hence the eigenvectors ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=While%20PCA%20finds%20the%20mathematically,based%20on%20their%20estimated%20relevancy)). For example, a single extreme point can pull a principal component toward itself, reducing PCA’s effectiveness for the majority of the data. It’s common to detect and remove or down-weight outliers prior to PCA, or use robust PCA techniques that are less sensitive to outliers ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=some%20contexts%2C%20outliers%20can%20be,based%20on%20their%20estimated%20relevancy)).
- **Assumption of mean and variance sufficiency:** PCA compresses data based on second-order statistics (means and covariances). If the data distribution is very non-Gaussian or complex, two datasets with identical covariance structure but different higher-order structure would appear the same to PCA. For example, PCA might not distinguish between bimodal distributions vs. unimodal, if the variance is similar.
- **Dynamic data:** PCA is a static linear combination. If new data points arrive, one might need to recalculate PCA (though incremental PCA can handle that). Similarly, if the data distribution changes over time, PCA components might need updating.
- **Summary:** Despite these limitations, PCA remains useful as an initial analysis tool. Awareness of its assumptions (linearity, importance of scaling, etc.) is crucial. In practice, combining PCA with domain knowledge (e.g., understanding which features drive each component) is important to ensure the reduced representation makes sense for the problem at hand.

% Figure: Effect of an outlier on PCA. (Illustration: A 2D dataset where most points form a clear direction, but one outlier far off causes the first principal component to skew towards it.)

## Limitations of K-Means
- **Choosing *k*:** K-means requires the number of clusters *k* to be specified in advance. An incorrect choice of *k* can yield poor results (over-segmentation if *k* too large, or mixing distinct groups if *k* too small). In real datasets, the “true” number of clusters is often ambiguous. Methods like the elbow or silhouette help but are not foolproof ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,29)) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,poorly%20matched%20to%20neighboring%20clusters)). This is a fundamental limitation: more advanced clustering methods attempt to determine *k* automatically (e.g., by model selection or based on density).
- **Cluster shape and size:** K-means assumes clusters are roughly spherical (isotropic) and of similar sizes, because it uses Euclidean distance to centroids. It partitions space with Voronoi cells around centroids. If actual clusters are elongated, uneven in size, or non-convex, K-means can perform poorly. For example, K-means cannot properly cluster a dataset of concentric circles or moons: it will cut them into pie slices or other shapes. Hierarchical or density-based methods would handle such shapes better. Also, if one cluster has much more variance or points than another, K-means tends to bias towards splitting the larger cluster.
- **Sensitivity to initialization:** While not a theoretical limitation of the clustering definition, the practical K-means algorithm (Lloyd’s) can converge to different solutions depending on initial centroids ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=results)). This means K-means can yield inconsistent results across runs. The user must use good initialization or multiple runs to mitigate this. There’s no guarantee the algorithm finds the global minimum of the objective (which is NP-hard to find) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20problem%20is%20computationally%20difficult,clusters%20to%20have%20different%20shapes)).
- **Outliers and noise:** K-means is sensitive to outliers. A single outlier point will pull a centroid towards it, possibly leaving the rest of that cluster poorly represented, or it might even form its own cluster if *k* is large enough. Because K-means uses the mean, it does not handle outliers robustly (contrast with k-medoids which would choose an actual point as center, mitigating outlier effect). No mechanism in standard K-means explicitly deals with noise points – every point is assigned to some cluster even if it doesn’t truly belong anywhere.
- **Distance metric:** K-means typically uses (squared) Euclidean distance. If a different notion of similarity is more appropriate (Manhattan distance, cosine similarity, etc.), the standard algorithm doesn’t directly apply without modification. Using other distance metrics might prevent convergence or complicate the centroid update (e.g., for cosine similarity one might need to normalize vectors). Variants like spherical K-means or kernel K-means exist, but standard K-means is limited in this regard ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20algorithm%20is%20often%20presented,allow%20using%20other%20distance%20measures)).
- **Empty clusters:** As mentioned, if a cluster loses all its points during the assignment step (which can happen if no points are closest to a particular centroid), the algorithm has to handle it (usually by reassigning the centroid). While this is manageable, it indicates that *k* might have been too large or some clusters are not stable.
- **Scalability to very large *n*:** K-means is relatively scalable, but for extremely large datasets (millions of points), even K-means can become slow, especially if *k* is also large. There are approximate methods (mini-batch K-means) that trade some accuracy for speed. Also, computing distances in very high-dimensional spaces can be computationally expensive and less meaningful (many distances become similar due to concentration phenomenon).
- **Cluster comparison across runs:** If one needs reproducibility or tracking clusters over time, the variability of K-means can be a problem. Clusters also lack an inherent label or order – one run’s “Cluster 1” might correspond to “Cluster 3” in another run, which requires post-hoc matching if you need to compare them.
- **Interpretability:** The clusters are defined just by their members and centroids. Sometimes it might be hard to attach a semantic meaning to each cluster without additional domain knowledge. For instance, K-means might cluster customers, but understanding what defines each cluster (e.g., “young urban professionals” vs “rural families”) requires interpreting the centroid in terms of original features.
- **In summary**, K-means is powerful for well-behaved data but one must be cautious about its assumptions. Always examine the cluster output to ensure it makes sense, and consider alternative methods if the data characteristics violate K-means assumptions. Combining K-means with preprocessing (like PCA or outlier removal) can alleviate some limitations but not all.

% Figure: A case where K-means fails. (Example: points in the shape of two moons. K-means with k=2 will produce incorrect clusters that cut through the moons, since it cannot handle non-convex shapes, whereas a density-based method would find the two moon-shaped clusters.)

## Conclusion
- **PCA and K-Means Recap:** We have discussed two fundamental techniques in unsupervised learning. **PCA** reduces dimensionality by finding new axes that capture the most variance in the data, effectively compressing data with minimal information loss. **K-Means** clustering partitions data into *k* groups based on similarity, aiming for compact, well-separated clusters.
- **Mathematical Foundations:** PCA is rooted in linear algebra – it leverages the eigen-decomposition of the covariance matrix to identify principal components (eigenvectors) and their importance (eigenvalues). K-means is grounded in an optimization problem – minimizing within-cluster variance – and is solved by an iterative algorithm (Lloyd’s algorithm) that alternates assignments and updates.
- **When to use PCA:** PCA is useful when you have many features and suspect that most of them are correlated or redundant. It helps in visualizing high-dimensional data, speeding up algorithms, and reducing noise. However, remember it assumes linear relationships and requires careful scaling and interpretation. Use PCA as an exploratory tool to uncover structure, but combine with understanding of features.
- **When to use K-Means:** K-means is a go-to method for clustering when you need a quick, simple partitioning of data and you have an idea of how many clusters might be appropriate. It works well when clusters are roughly spherical and similar in size. It can handle large datasets efficiently. Be mindful of initializing it well and checking multiple *k* values. If clusters are oddly shaped or *k* is unknown, consider other clustering methods or validate K-means results with domain expertise.
- **Synergy:** PCA and K-means can be combined to great effect. PCA sets up a lower-dimensional stage that can make clusters more distinct, and K-means then finds those clusters. This combination is especially powerful in high dimensions or when data is noisy ([clustering before or after PCA? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/52016/clustering-before-or-after-pca#:~:text=2,are%20informative%20then%20dimensionality%20reduction)). For example, first applying PCA (or another feature learning method) and then clustering is a common pipeline in data science.
- **Limitations and Next Steps:** Both methods have limitations – PCA with linearity and interpretability, K-means with cluster shape and *k* selection. It’s important to use diagnostics (explained variance for PCA, silhouette scores for K-means, etc.) and consider advanced methods when needed (kernel PCA for nonlinear structures, GMM or DBSCAN for complex clusters, etc.). Many modern algorithms build upon these basics: e.g., principal components can be viewed as a linear autoencoder in neural networks, and K-means relates to learning vector quantization and mixture models. Understanding PCA and K-means provides a foundation for grasping more complex techniques.
- **Key takeaway:** Dimensionality reduction and clustering are key tools for making sense of data without supervision. PCA helps to **summarize** and **simplify** data, and K-means helps to **group** and **categorize** data. Used together, they allow us to reveal hidden structure in datasets, reduce complexity, and prepare data for further analysis or modeling. As noted in the literature, many sophisticated methods in data analysis trace their roots to these two techniques ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Most%20of%20the%20modern%20methods,88)), underlining their importance in the data scientist’s toolkit.

## References
- Wikipedia: *Principal Component Analysis* – overview of PCA’s concepts, history, and mathematical details ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=,displaystyle)) ([Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=For%20either%20objective%2C%20it%20can,CCA%20defines%20coordinate%20systems%20that)).
- Sebastian Raschka (2015): *PCA in 3 Steps* – tutorial article illustrating PCA with the Iris dataset and step-by-step computations ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=,W%7D%5C%29%20to%20obtain%20a)) ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=The%20plot%20above%20clearly%20shows,of%20the%20information)).
- Wikipedia: *K-means Clustering* – detailed description of the algorithm, objective function, and variations ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=1,centroids%29%20for%20observations%20assigned)) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=The%20objective%20function%20in%20k,necessarily%20to%20the%20global%20optimum)).
- IBM Cloud Learn Hub: *What is K-Means Clustering?* – introduction and explanation of K-means with use cases and considerations ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=While%20various%20types%20of%20clustering,is%20efficient%2C%20effective%20and%20simple)) ([What is k-means clustering? | IBM](https://www.ibm.com/think/topics/k-means-clustering#:~:text=K,the%20algorithm%20by%20using%20evaluation)).
- DataCamp Blog (2023): *The Curse of Dimensionality in Machine Learning* – discussion on challenges of high-dimensional data and mention of PCA as a remedy ([Principal Component Analysis](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=Introduction)) ([The Curse of Dimensionality in Machine Learning: Challenges, Impacts, and Solutions | DataCamp](https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning#:~:text=High,reduce%20dimensions%20for%20visualization%20purposes)).
- Stack Exchange (Cross Validated): discussions on using PCA before clustering and determining number of clusters ([clustering before or after PCA? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/52016/clustering-before-or-after-pca#:~:text=1,the%20same%20cluster%20are%20close)) ([k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=,poorly%20matched%20to%20neighboring%20clusters)).
- Brunton et al. (2015): *Eigenfaces example* – demonstration of PCA on face images and noting that faces of the same person cluster in PCA space ([](http://faculty.washington.edu/sbrunton/me565/pdf/L29secure.pdf#:~:text=One%20of%20the%20most%20striking,with%20each%20of%20the%20principal)) ([](http://faculty.washington.edu/sbrunton/me565/pdf/L29secure.pdf#:~:text=this%20decomposition%20is%20a%20set,first%20studied%20by%20Sirovich%20and)).

