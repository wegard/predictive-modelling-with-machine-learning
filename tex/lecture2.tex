% !TEX TS-program = XeLaTeX
% !TEX spellcheck = en-US
\documentclass[aspectratio=169]{beamer}
%\documentclass[handout, aspectratio=169]{beamer} % for handouts

\usetheme{bi}
\usepackage{tikz}
\title{Lecture 2:\\ Machine learning basics and supervised learning}
\institute{GRA4160: Predictive Modelling with Machine Learning}
\date{January 18th 2023}
\author{Vegard H\o ghaug Larsen}

\begin{document}

\maketitle

\begin{frame}
    \frametitle{Plan for today:}
    \begin{columns}
    
        \begin{column}{0.5\textwidth}
            \begin{enumerate}
                \item Machine learning basics
                \item Linear regression
                \item Supervised learning with $k$-nearest neighbors
                \item Exercise: Spam filtering with naive Bayes
            \end{enumerate}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{tikzpicture}
                \node[draw,minimum width=\textwidth] {
                    \includegraphics[width=\textwidth]{../figures/Machine learning basics and supervised learning.png} % Replace with the path to your image
                };
            \end{tikzpicture}
        \end{column}

    \end{columns}
\end{frame}

\frame{
	\frametitle{Exploring the Fundamentals of Machine Learning}
	\begin{itemize}
		\item Machine learning teaches computers to learn from data, much like how a child learns from experience.
		\pause
		\item Three main types: Supervised, Unsupervised, and Reinforcement Learning.
		\pause
		\item \textbf{Supervised learning}: Trained on labeled data for predictions. Example: Handwritten digit recognition.
		\pause
		\item \textbf{Unsupervised learning}: Finds patterns in unlabeled data. Example: Customer segmentation.
		\pause
		\item \textbf{Reinforcement learning}: Training agents to make decisions towards a goal. Example: Robot navigation.
	\end{itemize}
}

\frame{
	\frametitle{Understanding Supervised Learning}
	\begin{itemize}
		\item Supervised learning: Like a student learning from a solved exercise, it predicts new outcomes based on learned patterns.
		\pause
		\item It involves input-output pair training, like predicting house prices from their size and location.
		\pause
		\item Key algorithms include:
		    \begin{itemize}
		        \item \textbf{Linear regression}: For continuous output prediction.
		        \item \textbf{$k$-nearest neighbors}: Classifies based on similarity.
		        \item \textbf{Decision trees}: Maps decisions and their possible consequences.
		        \item \textbf{Neural networks}: For complex pattern recognition.
		    \end{itemize}
	\end{itemize}
}


\frame{
	\frametitle{Linear Regression: Prediction \& Inference}
	\begin{itemize}
		\item Predicts continuous values, like estimating house prices based on size.
		\pause
		\item Useful for inference: understanding how factors influence outcomes.
		\pause
		\item A parametric model with fixed parameters representing feature weights and a bias term (intercept).
		\pause
		\item Aims to minimize the mean squared error for the best prediction accuracy.
		\pause
		\item Ordinary least squares (OLS): A method to find the best-fit line through data.
		\pause
		\item OLS is BLUE (best linear unbiased estimator) under conditions like no multicollinearity.
	\end{itemize}
}


\frame{
	\frametitle{Diving into $k$-Nearest Neighbors in Supervised Learning}
	\begin{itemize}
		\item kNN: A versatile tool for classification and regression, akin to categorizing based on nearest examples.
		\pause
		\item Works by identifying the $k$ nearest data points to a new point and predicting its class or value accordingly.
		\pause
        \item Choice of $k$ (like 5 or 10) influences model accuracy and complexity.
		\pause
		\item A non-parametric method, adaptable to different data distributions. 
		\pause
		\item Simple and effective, serving as a benchmark for more complex models, but slower on large datasets.
	\end{itemize}
}


\frame{
	\frametitle{Naive Bayes: A Fundamental Classifier}
	\textbf{Bayes' Theorem Formula}:
   \[ P(A|B) = \frac{P(B|A) \times P(A)}{P(B)} \]
	\begin{itemize}
		\item A classifier used for tasks like identifying spam emails.
		\pause
		\item Based on Bayes' theorem, which relates the probability of an event with prior knowledge.
		\pause
		\item Assumes features in a class are independent, known as the \textbf{naive} assumption.
		\pause
		\item This independence is often not true (like in spam emails), but Naive Bayes still performs well.
		\pause
		\item We'll explore its application in spam filtering, demonstrating its effectiveness despite simplistic assumptions.
		\pause
		\item Valued for its simplicity and speed, often used as a benchmark for more complex models.
	\end{itemize}
}

\frame{
	\frametitle{Notes}
	Figures without a source are created by me or with help from DALL-E
}


\end{document}