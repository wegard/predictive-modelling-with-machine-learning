{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Introduction and Setup\n",
    "We’ll create a minimal automatic differentiation system, similar to [micrograd](https://github.com/karpathy/micrograd), to demonstrate how backpropagation works under the hood. \n",
    "The idea is to build a Value class that records a computation graph node’s value and gradient, and supports basic arithmetic operations. Then we will implement a method to perform a backward pass (using the chain rule) to compute gradients through the graph, and use those gradients to perform gradient descent updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = ((w1 - 1)^2 + (w2 - 5)^2) * 0.5 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by defining a Value class to represent nodes in our computational graph.\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        \"\"\"\n",
    "        Initialize a Value object.\n",
    "        data: the numeric value (scalar) this node holds.\n",
    "        _children: the nodes that produced this value (for building the graph).\n",
    "        _op: the operation that produced this value (for debug/tracing purposes).\n",
    "        \"\"\"\n",
    "        self.data = data                  # the actual scalar value\n",
    "        self.grad = 0.0                   # gradient of the loss w.rt this value (to be computed in backprop)\n",
    "        self._prev = set(_children)       # set of parent nodes (inputs to the operation that produced this node)\n",
    "        self._op = _op                    # op name (optional, useful for debug)\n",
    "        self._backward = lambda: None     # function to backpropagate gradient from this node to its _prev\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # For convenience, when we print a Value it will show its data\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Support addition: Value + Value or Value + scalar\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), _op='+')\n",
    "        # Define the backward function for addition\n",
    "        def _backward():\n",
    "            # Gradient of the output w.rt each input is 1 (∂(a+b)/∂a = 1, ∂(a+b)/∂b = 1)\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # Ensure commutativity: allows scalar + Value to use __add__\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Support multiplication: Value * Value or Value * scalar\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), _op='*')\n",
    "        def _backward():\n",
    "            # ∂(a*b)/∂a = b, ∂(a*b)/∂b = a\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # Ensure commutativity for scalar * Value\n",
    "        return self * other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # Define subtraction in terms of addition: a - b = a + (-b)\n",
    "        return self + (-1 * other)\n",
    "\n",
    "    def __pow__(self, exponent):\n",
    "        # Only support exponent as int or float (scalar exponent)\n",
    "        assert isinstance(exponent, (int, float)), \"Only supporting int/float exponents for simplicity.\"\n",
    "        out = Value(self.data ** exponent, (self,), _op=f'**{exponent}')\n",
    "        def _backward():\n",
    "            # ∂(a^k)/∂a = k * a^(k-1)\n",
    "            self.grad += exponent * (self.data ** (exponent - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        # Compute gradients of all values in the graph w.rt this Value (self).\n",
    "        # 1. Topologically sort the graph of dependencies\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        # 2. Initialize the output node's gradient\n",
    "        self.grad = 1.0\n",
    "        # 3. Traverse nodes in reverse topological order and propagate gradients\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick forward computation test:\n",
    "w1 = Value(2.0)\n",
    "w2 = Value(3.0)\n",
    "\n",
    "# Construct an expression: ((w1 - 1)**2 + (w2 - 5)**2) * 0.5\n",
    "a = w1 - 1\n",
    "b = a**2\n",
    "c = w2 - 5\n",
    "d = c**2\n",
    "s = b + d\n",
    "loss = s * 0.5\n",
    "print(loss)  # This is the loss given the current w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "1. **Inputs:**\n",
    "   - $ w_1 = 2.0 $\n",
    "   - $ w_2 = 3.0 $\n",
    "\n",
    "2. **Intermediate Computations:**\n",
    "   - **For $ w_1 $:**\n",
    "     - $ a = w_1 - 1 = 2.0 - 1 = 1.0 $\n",
    "     - $ b = a^2 = 1.0^2 = 1.0 $\n",
    "   - **For $ w_2 $:**\n",
    "     - $ c = w_2 - 5 = 3.0 - 5 = -2.0 $\n",
    "     - $ d = c^2 = (-2.0)^2 = 4.0 $\n",
    "   - **Combine and Scale:**\n",
    "     - $ s = b + d = 1.0 + 4.0 = 5.0 $\n",
    "     - $ \\text{loss} = 0.5 \\times s = 0.5 \\times 5.0 = 2.5 $\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "Start by setting $\\frac{d\\,\\text{loss}}{d\\,\\text{loss}} = 1$.\n",
    "\n",
    "1. **Loss to Sum $ s $:**\n",
    "   - $\\frac{d\\,\\text{loss}}{d\\,s} = 0.5$ since $\\text{loss} = 0.5 \\times s$.\n",
    "\n",
    "2. **Gradients Through the Sum:**\n",
    "   - Since $ s = b + d $:\n",
    "     - $\\frac{d\\,s}{d\\,b} = 1$\n",
    "     - $\\frac{d\\,s}{d\\,d} = 1$\n",
    "   - Thus:\n",
    "     - $\\frac{d\\,\\text{loss}}{d\\,b} = 0.5$\n",
    "     - $\\frac{d\\,\\text{loss}}{d\\,d} = 0.5$\n",
    "\n",
    "3. **Backpropagation to $ w_1 $:**\n",
    "   - For $ b = a^2 $ where $ a = w_1 - 1 $:\n",
    "     - $\\frac{d\\,b}{d\\,a} = 2a$\n",
    "     - At $ a = 1.0 $, this is $ 2 \\times 1.0 = 2 $\n",
    "     - So, $\\frac{d\\,\\text{loss}}{d\\,a} = \\frac{d\\,\\text{loss}}{d\\,b} \\times \\frac{d\\,b}{d\\,a} = 0.5 \\times 2 = 1.0$\n",
    "   - For $ a = w_1 - 1 $:\n",
    "     - $\\frac{d\\,a}{d\\,w_1} = 1$\n",
    "     - Hence, $\\frac{d\\,\\text{loss}}{d\\,w_1} = 1.0 \\times 1 = 1.0$\n",
    "\n",
    "4. **Backpropagation to $ w_2 $:**\n",
    "   - For $ d = c^2 $ where $ c = w_2 - 5 $:\n",
    "     - $\\frac{d\\,d}{d\\,c} = 2c$\n",
    "     - At $ c = -2.0 $, this is $ 2 \\times (-2.0) = -4 $\n",
    "     - So, $\\frac{d\\,\\text{loss}}{d\\,c} = \\frac{d\\,\\text{loss}}{d\\,d} \\times \\frac{d\\,d}{d\\,c} = 0.5 \\times (-4) = -2.0$\n",
    "   - For $ c = w_2 - 5 $:\n",
    "     - $\\frac{d\\,c}{d\\,w_2} = 1$\n",
    "     - Thus, $\\frac{d\\,\\text{loss}}{d\\,w_2} = -2.0 \\times 1 = -2.0$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Summary\n",
    "\n",
    "- **Forward Computation:**\n",
    "  - $ w_1 = 2.0 $, $ w_2 = 3.0 $\n",
    "  - $ a = 1.0 $, $ b = 1.0 $\n",
    "  - $ c = -2.0 $, $ d = 4.0 $\n",
    "  - $ s = 5.0 $\n",
    "  - $ \\text{loss} = 2.5 $\n",
    "\n",
    "- **Backward Computation:**\n",
    "  - $\\frac{d\\,\\text{loss}}{d\\,w_1} = 1.0$\n",
    "  - $\\frac{d\\,\\text{loss}}{d\\,w_2} = -2.0$\n",
    "\n",
    "Thus, the gradient with respect to $ w_1 $ is **1.0**, and the gradient with respect to $ w_2 $ is **-2.0**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backpropagation on the computational graph\n",
    "loss.backward()\n",
    "# After running backward, w1.grad and w2.grad should be populated with ∂loss/∂w1 and ∂loss/∂w2\n",
    "\n",
    "\n",
    "print(f\"∂loss/∂w1 = {w1.grad}\")  # expected 1.0 (since w1-1 = 1)\n",
    "print(f\"∂loss/∂w2 = {w2.grad}\")  # expected -2.0 (since w2-5 = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w1 = Value(2.0)\n",
    "w2 = Value(3.0)\n",
    "params = [w1, w2]\n",
    "\n",
    "learning_rate = 0.1  # choose a learning rate\n",
    "for i in range(100):\n",
    "    # 1. Forward pass: compute the loss for current w1, w2\n",
    "    loss = 0.5 * ((w1 - 1)**2 + (w2 - 5)**2)\n",
    "    print(f\"Iteration {i}: loss = {loss.data:.4f}\")\n",
    "    # 2. Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    # 3. Gradient descent update: w <- w - α * grad\n",
    "    for p in params:\n",
    "        p.data -= learning_rate * p.grad\n",
    "        #  Reset gradient to 0 for next iteration (since .backward() accumulates gradients)\n",
    "        p.grad = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gra4160",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
