{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data in Jupyter notebooks\n",
    "\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "### What we will cover in this notebook:\n",
    "1. Setting up the environment\n",
    "2. Reading and Writing Data with Pandas\n",
    "3. Exploring and Visualizing Data\n",
    "4. Handling Missing Values and Outliers\n",
    "5. Encoding Categorical Variables\n",
    "6. Feature Scaling and Normalization\n",
    "7. Train-Test Split and Basic Data Pipelines\n",
    "8. Introduction to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the environment\n",
    "\n",
    "Using a virtual environment or a conda environment is crucial for ensuring reproducibility, consistency, and maintainability in your projects. Without an isolated environment, library installations and updates can affect your system-wide settings or other projects, leading to version conflicts and unpredictable behavior. By creating a dedicated environment for each project, you can precisely control which versions of Python and its packages are used, making it easier to replicate your results, share your work with others, and quickly recover a working setup if something goes wrong. This practice streamlines collaboration, simplifies troubleshooting, and ultimately helps maintain the integrity and reliability of your codebase.\n",
    "\n",
    "- The core libraries we will be using in this course are:\n",
    "    - pandas for data manipulation and exploration.\n",
    "        - [Docs](https://pandas.pydata.org/docs/)\n",
    "    - scikit-learn for preprocessing and modeling\n",
    "        - [Docs](https://scikit-learn.org/stable/)\n",
    "    - matplotlib for basic plotting and data visualization\n",
    "        - [Docs](https://matplotlib.org/stable/contents.html)\n",
    "    - seaborn for statistical data visualization\n",
    "        - [Docs](https://seaborn.pydata.org/)\n",
    "- Additional libraries that will be used/discussed:\n",
    "    - PyTorch for deep learning workflows (optional)\n",
    "        - PyTorch is not a core library, but it is widely used for deep learning tasks. We will only touch on it briefly in this course, but you may want to explore it further if you are interested in deep learning. \n",
    "        - [Docs](https://pytorch.org/docs/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our the versions of the libraries\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Matplotlib version: {plt.matplotlib.__version__}')\n",
    "print(f'Seaborn version: {sns.__version__}')\n",
    "print(f'Scikit-learn version: {pd.__version__}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "# Check if GPU is available\n",
    "print(f'Acess to GPU: {torch.cuda.is_available()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading and Writing Data with Pandas\n",
    "- **Read data** from common file formats such as CSV.\n",
    "- **Write processed data** back to disk in CSV format.\n",
    "- Use **basic inspection methods** (such as `head()`, `info()`, and `describe()`) to quickly understand the structure and statistical properties of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "df = pd.read_csv('../data/house-prices/test.csv')  \n",
    "\n",
    "# Displaying the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .info() method to get a summary of the dataframe \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() method to get a statistical summary of the dataframe\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to a csv file\n",
    "\n",
    "df.to_csv('../data/tmp/processed_housing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring and Visualizing Data\n",
    "- Basic summary statistics.\n",
    "- Identifying distributions of features.\n",
    "- Simple visualizations (histograms, box plots, scatter plots) to understand data distribution, outliers, and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "# Quick summary statistics\n",
    "print(tips.describe())\n",
    "\n",
    "# Pairplot to visualize relationships\n",
    "print(\"Note: pairplot can be slow for larger datasets.\\n\")\n",
    "\n",
    "sns.pairplot(tips, hue='time')\n",
    "plt.show()\n",
    "\n",
    "# Let's also do a boxplot for 'total_bill' grouped by 'day'\\n\",\n",
    "sns.boxplot(x='day', y='total_bill', data=tips)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Values and Outliers\n",
    "- Techniques for detecting missing values (`isnull().sum()`) and outliers (using IQR or $z$-score).\n",
    "- Strategies for handling missing data (drop vs. impute).\n",
    "- Using `sklearn.impute.SimpleImputer` for numerical and categorical data.\n",
    "- Discussion of domain knowledge in deciding how to handle anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabetes dataset as a DataFrame\n",
    "data = load_diabetes(as_frame=True)\n",
    "df_missing = data.frame\n",
    "\n",
    "# Artificially introduce some missing values\\n\",\n",
    "df_missing.iloc[:10, 2] = np.nan  # Suppose the 3rd column has missing for first 10 rows\n",
    "  \n",
    "# Detect missing values\n",
    "print(\"Missing values per column:\\n\", df_missing.isnull().sum())\n",
    "\n",
    "# Simple strategy: fill numerical missing values with the mean\n",
    "df_missing.fillna(df_missing.mean(), inplace=True)\n",
    "\n",
    "# Detect outliers in 'bmi' column using IQR\n",
    "Q1 = df_missing['bmi'].quantile(0.25)\n",
    "Q3 = df_missing['bmi'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df_missing[(df_missing['bmi'] < Q1 - 1.5 * IQR) | (df_missing['bmi'] > Q3 + 1.5 * IQR)]\n",
    "print(f\"Number of outliers in 'bmi': {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encoding Categorical Variables\n",
    "- Importance of converting string labels into numeric form for modeling.\n",
    "- One-hot encoding with `pd.get_dummies()` or `sklearn.preprocessing.OneHotEncoder`.\n",
    "- Label encoding vs. one-hot encoding and when to use each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Load the 'tips' dataset, which has some categorical features\n",
    "tips = sns.load_dataset('tips')\n",
    "print(\"Data types before encoding:\\n\", tips.dtypes)\n",
    "print(\"\\nData before encoding:\\n\", tips.head())\n",
    "\n",
    "# One-hot encoding on categorical columns\n",
    "encoded_tips = pd.get_dummies(tips, columns=['day','sex','smoker','time'], drop_first=True)\n",
    "\n",
    "print(\"\\nData after one-hot encoding:\\n\", encoded_tips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Normalization\n",
    "- Show `StandardScaler` and `MinMaxScaler` from `scikit-learn`.\n",
    "- Discuss when scaling is necessary (e.g., for neural networks or distance-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Load the Iris dataset as a DataFrame\n",
    "iris = load_iris(as_frame=True)\n",
    "df_iris = iris.data.copy()\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "df_standard_scaled = scaler.fit_transform(df_iris)\n",
    " \n",
    "# Min-Max scaling\n",
    "minmax = MinMaxScaler()\n",
    "df_minmax_scaled = minmax.fit_transform(df_iris)\n",
    "\n",
    "print(\"Original (first 5 rows):\\n\", df_iris.head(), \"\\n\")\n",
    "print(\"Standard Scaled (first 5 rows):\\n\", df_standard_scaled[:5], \"\\n\")\n",
    "print(\"Min-Max Scaled (first 5 rows):\\n\", df_minmax_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `.fit_transform()`, `.fit()`, and `.transform()`:\n",
    "1.\t`.fit()`:\n",
    "    - Calculates the parameters required for transformation based on the input data.\n",
    "    - Does not return transformed data.\n",
    "    - Example: In `StandardScaler`, `.fit()` calculates the mean and standard deviation.\n",
    "2.\t`.transform()`:\n",
    "    - Applies the transformation to the data using parameters calculated during `.fit()`.\n",
    "    - Requires that `.fit()` has been called earlier (either directly or implicitly).\n",
    "    - Example: In `StandardScaler`, `.transform()` scales the data using the precomputed mean and standard deviation.\n",
    "3.\t`.fit_transform()`:\n",
    "    - Combines `.fit()` and `.transform()` in one step.\n",
    "    - Useful when you need to fit and transform the same dataset in a single line.\n",
    "    - Example: In `StandardScaler`, `.fit_transform()` computes the mean and standard deviation (fit) and then scales the data (transform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split and Basic Data Pipelines\n",
    "- Introduce the concept of splitting data into training and test sets.\n",
    "- Show `train_test_split` usage from `scikit-learn`.\n",
    "- Introduce basic pipeline concepts (`sklearn.pipeline.Pipeline`) to ensure consistent preprocessing and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset for demonstration\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Pipeline test accuracy: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Introduction to PyTorch Tensors\n",
    "- Briefly show how PyTorch tensors differ from NumPy arrays and how to convert between them.\n",
    "- This will be relevant for deep learning sessions later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple NumPy array\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "print(\"NumPy Array:\")\n",
    "print(np_array)\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "torch_tensor = torch.tensor(np_array)\n",
    "print(\"\\nPyTorch Tensor:\")\n",
    "print(torch_tensor)\n",
    "\n",
    "# Perform operations\n",
    "# Element-wise addition\n",
    "np_result = np_array + 2\n",
    "torch_result = torch_tensor + 2\n",
    "\n",
    "print(\"\\nNumPy Array after adding 2:\")\n",
    "print(np_result)\n",
    "\n",
    "print(\"\\nPyTorch Tensor after adding 2:\")\n",
    "print(torch_result)\n",
    "\n",
    "# Key differences:\n",
    "# 1. NumPy arrays are part of the NumPy library and are used for general-purpose numerical computations.\n",
    "# 2. PyTorch tensors are similar to NumPy arrays but support GPU acceleration and are optimized for deep learning.\n",
    "\n",
    "# Example: Moving a PyTorch tensor to a GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch_tensor_gpu = torch_tensor.to('cuda')\n",
    "    print(\"\\nPyTorch Tensor moved to GPU:\")\n",
    "    print(torch_tensor_gpu)\n",
    "else:\n",
    "    print(\"\\nGPU is not available, tensor remains on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
