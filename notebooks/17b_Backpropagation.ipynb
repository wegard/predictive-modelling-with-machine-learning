{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "## Lecture 9\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is an optimization algorithm used in training feedforward artificial neural networks.\n",
    "It is a form of supervised learning that involves minimizing a loss function with respect to the weights and biases of the network.\n",
    "The algorithm is based on the chain rule of calculus and calculates the gradient of the loss function with respect to each weight by propagating the error signal backward through the network.\n",
    "The primary goal of backpropagation is to adjust the weights and biases of the network so that the predicted output converges to the actual output, reducing the error between them.\n",
    "\n",
    "In the first step of backpropagation, the input is passed through the network in a forward pass to generate the predicted output.\n",
    "This output is then compared to the actual target output to calculate the error using a predefined loss function.\n",
    "The error serves as an indicator of how well the network has performed on that particular input.\n",
    "The gradient of the loss function with respect to the output layer's activation is calculated, which represents the direction in which the loss function's value increases the most.\n",
    "\n",
    "In the second step, the error signal is propagated backward through the network from the output layer to the input layer.\n",
    "The chain rule of calculus is applied to compute the gradients of the loss function with respect to the weights and biases in each layer.\n",
    "These gradients are used to update the weights and biases in the network using an optimization algorithm, such as gradient descent or a variant thereof.\n",
    "This process is repeated for multiple epochs or iterations, with the weights and biases being adjusted incrementally to minimize the loss function and improve the network's performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python example of backpropagation\n",
    "\n",
    "This example uses a small neural network with one hidden layer to perform binary classification on a synthetic dataset.\n",
    "\n",
    "We create a synthetic dataset with 500 data points for binary classification. The neural network has one input layer with two neurons (corresponding to the two dimensions of the data), one hidden layer with four neurons, and one output layer with a single neuron.\n",
    "\n",
    "The forward pass calculates the output of the network using the current weights and biases. Then, the loss is calculated using mean squared error (MSE) as the cost function. The backpropagation algorithm computes the gradients of the loss with respect to the weights and biases of the network. The weights and biases are then updated using gradient descent with a learning rate of 0.1.\n",
    "\n",
    "The training loop runs for 1,000 epochs, and the loss is printed every 100 epochs. Finally, the trained model is used to make predictions on a test dataset.\n",
    "\n",
    "## Model architecture\n",
    "Mathematically, the model can be represented as follows:\n",
    "\n",
    "Hidden layer calculation:\n",
    "\n",
    "$$h = \\sigma(W1 \\cdot x + b1)$$\n",
    "\n",
    "Output layer calculation:\n",
    "\n",
    "$$\\hat{y} = \\sigma(W2 \\cdot h + b2)$$\n",
    "\n",
    "Where $x$ is the input vector, $h$ is the hidden layer output, \\hat{y} is the predicted output, and $\\sigma$ is the sigmoid activation function given by:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The loss function is given by:\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\text{mean}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $y$ is the actual output.\n",
    "\n",
    "The derivative of the loss function with respect to the output is given by:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)$$\n",
    "\n",
    "Evaluated at a particular data point, the derivative of the loss function with respect to the weights and biases of the output layer is given by:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y_i}} = (\\hat{y}_i - y_i)$$\n",
    "\n",
    "The derivative of the sigmoid activation function is given by:\n",
    "\n",
    "$$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) (1 - \\sigma(z))$$\n",
    "\n",
    "LetÂ´s say we have a value $y$ that represents the output of the sigmoid function $y=\\sigma(z)$, then\n",
    "\n",
    "$$\\frac{\\partial \\sigma(z)}{\\partial z} = y (1 - y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(500, 2)\n",
    "labels = (data[:, 0] + data[:, 1]) > 0\n",
    "\n",
    "print(data[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "# Define the mean squared error (MSE) loss function\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define the derivative of the mean squared error (MSE) loss function\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Set the size of the input, hidden, and output layers\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the weights and biases for the first (input) layer\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weight matrix connecting input and hidden layers\n",
    "b1 = np.zeros(hidden_size)  # Bias vector for the hidden layer\n",
    "\n",
    "# Initialize the weights and biases for the second (hidden) layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # Weight matrix connecting hidden and output layers\n",
    "b2 = np.zeros(output_size)  # Bias vector for the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then we do the backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden = sigmoid(np.dot(data, W1) + b1)\n",
    "    output = sigmoid(np.dot(hidden, W2) + b2)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = mse(labels.reshape(-1, 1), output)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = mse_derivative(labels.reshape(-1, 1), output) * sigmoid_derivative(output)\n",
    "    d_hidden = np.dot(d_output, W2.T) * sigmoid_derivative(hidden)\n",
    "\n",
    "    # Update weights and biases\n",
    "    W2 -= learning_rate * np.dot(hidden.T, d_output)\n",
    "    b2 -= learning_rate * np.sum(d_output, axis=0)\n",
    "    W1 -= learning_rate * np.dot(data.T, d_hidden)\n",
    "    b1 -= learning_rate * np.sum(d_hidden, axis=0)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d_output:**\n",
    "This line calculates the error gradient at the output layer.\n",
    "The first part, `mse_derivative(labels.reshape(-1, 1), output)`, computes the derivative of the mean squared error (MSE) loss function with respect to the predicted output.\n",
    "The second part, `sigmoid_derivative(output)`, calculates the derivative of the sigmoid activation function with respect to its input, which is the output of the neural network.\n",
    "The two gradients are multiplied element-wise to compute the overall gradient at the output layer.\n",
    "\n",
    "Mathematically, `d_output` represents the following computation:\n",
    "\n",
    "$$\\partial \\hat{y} = MSE'(y, \\hat{y}) \\cdot \\sigma'(\\hat{y})$$\n",
    "\n",
    "**d_hidden:**\n",
    "This line calculates the error gradient at the hidden layer.\n",
    "The first part, `np.dot(d_output, W2.T)`, calculates the gradient of the loss with respect to the hidden layer's output by backpropagating the output layer's gradient through the weights (W2) connecting the hidden layer and the output layer.\n",
    "The second part, `sigmoid_derivative(hidden)`, computes the derivative of the sigmoid activation function with respect to its input, which is the output of the hidden layer.\n",
    "The two gradients are multiplied element-wise to compute the overall gradient at the hidden layer.\n",
    "Mathematically, `d_hidden` represents the following computation:\n",
    "\n",
    "$$\\partial h  = (\\partial \\hat{y} \\cdot W2) \\cdot \\sigma'(h)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_data = np.array([[0.5, -0.5],\n",
    "                      [-0.5, 0.5],\n",
    "                      [0.5, 0.5],\n",
    "                      [-0.5, -0.5]])\n",
    "\n",
    "hidden = sigmoid(np.dot(test_data, W1) + b1)\n",
    "test_output = sigmoid(np.dot(hidden, W2) + b2)\n",
    "test_predictions = (test_output > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"Test predictions:\", test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
