{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "## Lecture 3\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) is a supervised machine learning technique used for both dimensionality reduction and classification tasks. \n",
    "It is a linear method that projects the data onto a lower-dimensional space while maximizing the separation between different classes. \n",
    "This makes LDA particularly useful in fields such as pattern recognition, image and speech recognition, and natural language processing. \n",
    "Note that this should not be confused with Latent Dirichlet Allocation, also abbreviated as LDA, which is an unsupervised topic modeling algorithm (most often used for text classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Idea Behind LDA\n",
    "\n",
    "The core idea behind LDA is to find a linear combination of features that best separates the classes in the dataset. \n",
    "LDA achieves this by maximizing the **between-class variance** while minimizing the **within-class variance**. \n",
    "This is often referred to as optimizing the \"Fisher criterion.\"\n",
    "\n",
    "The optimization objective can be mathematically expressed as follows:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S_B$ is the **between-class scatter matrix**:\n",
    "  $$\n",
    "  S_B = \\sum_{i=1}^{k} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T\n",
    "  $$\n",
    "  Here,  $k$ is the number of classes, $\\mu_i$ is the mean vector of class $i$, $\\mu$ is the overall mean vector of the data, and $N_i$ is the number of samples in class $i$.\n",
    "\n",
    "- $S_W$ is the **within-class scatter matrix**:\n",
    "  $$\n",
    "  S_W = \\sum_{i=1}^{k} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T\n",
    "  $$\n",
    "  Here, $C_i$ represents the set of samples in class $i$, and $x$ is a feature vector.\n",
    "\n",
    "- $w$ is the projection vector that maps the data onto the new lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in LDA\n",
    "\n",
    "1. **Compute the mean vectors** for each class and the overall mean of the data.\n",
    "2. **Compute the within-class scatter matrix $S_W$** and the between-class scatter matrix $S_B$.\n",
    "3. Solve the generalized eigenvalue problem for $S_W^{-1} S_B$:\n",
    "   $$\n",
    "   S_W^{-1} S_B w = \\lambda w\n",
    "   $$\n",
    "   Here, $\\lambda$ are the eigenvalues, and $w$ are the eigenvectors. The eigenvectors corresponding to the largest eigenvalues form the optimal discriminant axes.\n",
    "4. Select the top $m$ eigenvectors (where $m$ is the desired dimensionality) to construct the transformation matrix.\n",
    "5. Project the data onto the new lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of LDA\n",
    "\n",
    "In practice, LDA is frequently used as a preprocessing step before applying classifiers like logistic regression or support vector machines. By reducing the dimensionality, LDA simplifies the problem, improves classifier performance, and reduces computational cost. Additionally, LDA can serve as a standalone classifier, in which case the decision boundary is linear.\n",
    "\n",
    "When used as a classifier, LDA assumes that the data in each class follows a Gaussian distribution with a shared covariance matrix. Based on this assumption, the decision rule for classification is:\n",
    "\n",
    "$$\n",
    "\\text{Classify } x \\text{ into class } i \\text{ if: } P(C_i | x) > P(C_j | x), \\, \\forall j \\neq i\n",
    "$$\n",
    "\n",
    "The posterior probability $P(C_i | x)$ can be derived from Bayes' theorem.\n",
    "\n",
    "### Implementation in Scikit-Learn\n",
    "\n",
    "We will not dive into all the mathematical details of implementing LDA but will look at a practical example using Scikit-Learn. It provides a convenient implementation of LDA that can be used both for dimensionality reduction and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn example\n",
    "\n",
    "Let's use the `make_classification` from Scikit-learn to generate some toy data with 2 informative features and 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Generate some data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate some toy data\n",
    "# Parameters:\n",
    "# n_samples: The number of samples to generate\n",
    "# n_features: The number of features (independent variables)\n",
    "# n_redundant: The number of redundant features (features that are linear combinations of other features)\n",
    "# n_informative: The number of informative features (features that determine the target)\n",
    "# n_clusters_per_class: The number of clusters per class (class = target)\n",
    "# class_sep: The factor by which the classes are linearly separable (larger values make the problem easier)\n",
    "# random_state: The seed used by the random number generator\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, class_sep=0.75, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33, random_state=42)\n",
    "X_train[0:5], y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['navy', 'orange']\n",
    "target_names = list(set(y))\n",
    "for i, label in enumerate(target_names):\n",
    "    plt.scatter(X_train[y_train==i, 0], X_train[y_train==i, 1], label=label, c=colors[i])\n",
    "plt.legend()\n",
    "plt.title('Toy binary classification data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the lda\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Create an instance of the LDA model\n",
    "# n_components: The number of components to keep after transformation\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# Fit the model to the data\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# We can now project the data onto a lower dimensional space\n",
    "\n",
    "X_transformed = lda.transform(X_train)\n",
    "#X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can plot the LDA component\n",
    "\n",
    "plt.scatter(range(len(X_transformed)), X_transformed, c=y_train, cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.show()\n",
    "lda.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis on the iris dataset\n",
    "\n",
    "Here we load the Iris dataset from scikit-learn, which includes 150 samples of iris flowers, each with 4 features (sepal length, sepal width, petal length, and petal width). Next, we split the data into a training set and a test set.\n",
    "\n",
    "We create an instance of the LinearDiscriminantAnalysis class and set the number of components to 2. This reduces the dimensionality of the data from 4 to 2, allowing for easier visualization.\n",
    "\n",
    "Finally, we use the fit_transform method on the training data to fit the LDA model and transform the data. Then we use the transform method on the test data to transform it using the LDA model learned from the training set.\n",
    "\n",
    "It's important to note that LDA is a supervised method, and it requires the class labels to be passed as an argument in the fit method (y_train) in order to learn the class means and variances.\n",
    "\n",
    "We also create a scatter plot of the transformed data, where the x-axis corresponds to the first linear discriminant (LD1) and the y-axis corresponds to the second linear discriminant (LD2). The color of each point represents the class label of the corresponding sample.\n",
    "\n",
    "With this plot, it's easy to see how the LDA has separated the different classes of iris flowers based on the two linear discriminants. This can be useful for visualizing the structure of the data and for understanding how well the LDA algorithm is able to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # Importing the iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "X2, y2 = iris.data, iris.target\n",
    "\n",
    "# Split the data into a train and test set\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=10)\n",
    "\n",
    "# Creating an instance of LinearDiscriminantAnalysis and setting number of components to 2\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "# Fitting and transforming the training data\n",
    "X_train_lda = lda.fit_transform(X_train2, y_train2)\n",
    "\n",
    "# Transforming the test data using the model learned from training data\n",
    "X_test_lda = lda.transform(X_test2)\n",
    "\n",
    "# assign label to each type of flower\n",
    "target_names = iris.target_names\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, label in enumerate(target_names):\n",
    "    plt.scatter(X_train_lda[y_train2==i, 0], X_train_lda[y_train2==i, 1], label=label, c=colors[i])\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the separation of the test data\n",
    "\n",
    "# assign label to each type of flower\n",
    "target_names = iris.target_names\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, label in enumerate(target_names):\n",
    "    plt.scatter(X_test_lda[y_test2==i, 0], X_test_lda[y_test2==i, 1], label=label, c=colors[i])\n",
    "\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAFromScratch:\n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        A simple from-scratch implementation of Linear Discriminant Analysis (LDA).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_components : int or None\n",
    "            The number of linear discriminants to keep.\n",
    "            If None, it will be set to (num_classes - 1).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the LDA model given training data X and labels y.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        y : array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # 1. Store unique classes\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # 2. Compute class means and priors\n",
    "        self.means_ = {}\n",
    "        self.priors_ = {}\n",
    "        self.class_count_ = {}\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            self.means_[c] = np.mean(X_c, axis=0)\n",
    "            self.class_count_[c] = X_c.shape[0]\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            self.priors_[c] = self.class_count_[c] / n_samples\n",
    "        \n",
    "        # 3. Compute within-class scatter matrix (S_W)\n",
    "        S_W = np.zeros((n_features, n_features))\n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = self.means_[c].reshape(-1, 1)\n",
    "            for x in X_c:\n",
    "                x = x.reshape(-1, 1)\n",
    "                S_W += (x - mean_c) @ (x - mean_c).T\n",
    "        \n",
    "        # 4. Compute the pooled covariance\n",
    "        #    (Divide by (N - k) to get the unbiased estimate, consistent with sklearn)\n",
    "        self.cov_ = S_W / (n_samples - len(self.classes_))\n",
    "        self.cov_inv_ = np.linalg.inv(self.cov_)\n",
    "        \n",
    "        # 5. Compute between-class scatter matrix (S_B)\n",
    "        self.overall_mean_ = np.mean(X, axis=0).reshape(-1, 1)\n",
    "        S_B = np.zeros((n_features, n_features))\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            n_c = self.class_count_[c]\n",
    "            mean_c = self.means_[c].reshape(-1, 1)\n",
    "            mean_diff = (mean_c - self.overall_mean_)\n",
    "            S_B += n_c * (mean_diff @ mean_diff.T)\n",
    "        \n",
    "        # 6. Solve the eigenvalue problem for Sigma^{-1} S_B\n",
    "        #    We take the real parts because of possible tiny imaginary numerical issues.\n",
    "        A = np.linalg.inv(self.cov_) @ S_B\n",
    "        eigvals, eigvecs = np.linalg.eig(A)\n",
    "        eigvals = np.real(eigvals)\n",
    "        eigvecs = np.real(eigvecs)\n",
    "        \n",
    "        # 7. Sort eigenvalues/eigenvectors in descending order\n",
    "        idx = np.argsort(eigvals)[::-1]\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        # 8. Select the top n_components\n",
    "        if self.n_components is None:\n",
    "            self.n_components = len(self.classes_) - 1\n",
    "        self.W_ = eigvecs[:, :self.n_components]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for samples in X using the fitted model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y_pred = []\n",
    "        \n",
    "        # For classification, we use the linear discriminant rule:\n",
    "        # delta_c(x) = x^T * cov_inv_ * mean_c - 0.5 * mean_c^T * cov_inv_ * mean_c + log(prior_c)\n",
    "        for x in X:\n",
    "            best_score = -np.inf\n",
    "            best_class = None\n",
    "            for c in self.classes_:\n",
    "                mean_c = self.means_[c]\n",
    "                w_c = self.cov_inv_ @ mean_c\n",
    "                score_c = x @ w_c - 0.5 * (mean_c @ w_c) + np.log(self.priors_[c])\n",
    "                \n",
    "                if score_c > best_score:\n",
    "                    best_score = score_c\n",
    "                    best_class = c\n",
    "            y_pred.append(best_class)\n",
    "        \n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Project the data X onto the linear discriminants.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_lda : ndarray of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        # Typically we subtract the overall mean before projecting\n",
    "        X_centered = X - self.overall_mean_.ravel()\n",
    "        return X_centered @ self.W_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate our LDA from scratch\n",
    "lda_scratch = LDAFromScratch(n_components=1)\n",
    "\n",
    "# 2. Fit the model\n",
    "lda_scratch.fit(X_train, y_train)\n",
    "\n",
    "# 3. Transform the training data\n",
    "X_transformed_scratch = lda_scratch.transform(X_train)\n",
    "\n",
    "# 4. Compare visually with the scikit-learn transform:\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(range(len(X_transformed_scratch)), X_transformed_scratch, c=y_train, cmap=plt.cm.coolwarm)\n",
    "plt.title(\"LDA from Scratch: 1D Projection of the Training Data\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Plot the decision boundary on the training set\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z_scratch = lda_scratch.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_scratch = Z_scratch.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z_scratch, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolor='k')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary (Train) - LDA from Scratch')\n",
    "plt.show()\n",
    "\n",
    "# 6. Evaluate on the test set (decision boundary + accuracy)\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z_scratch_test = lda_scratch.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_scratch_test = Z_scratch_test.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z_scratch_test, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm, edgecolor='k')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary (Test) - LDA from Scratch')\n",
    "plt.show()\n",
    "\n",
    "# 7. Report accuracy\n",
    "y_pred_test_scratch = lda_scratch.predict(X_test)\n",
    "accuracy_scratch = np.mean(y_pred_test_scratch == y_test)\n",
    "print(\"Accuracy on test set (LDA from scratch):\", accuracy_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
