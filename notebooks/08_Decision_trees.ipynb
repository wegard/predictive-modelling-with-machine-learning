{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "## Lecture 4\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a type of algorithm that can be used for both classification and regression problems.\n",
    "They are a popular method for supervised learning because they are easy to understand and interpret, and can handle both continuous and categorical variables.\n",
    "\n",
    "A decision tree is built by recursively partitioning the data into subsets based on the values of the input features.\n",
    "At each internal node of the tree, a test is performed on one of the input features, and the data is split into two or more subsets based on the outcome of the test.\n",
    "The process is repeated for each subset until a stopping criteria is met.\n",
    "\n",
    "At each leaf node of the tree, a class label or a constant value (mean response value) is stored which is used to make predictions for the instances that reach that leaf.\n",
    "\n",
    "The goal is to find the splits that maximize the information gain or decrease the impurity of the subsets.\n",
    "There are different measures of impurity, such as Gini impurity or entropy, which can be used depending on the problem.\n",
    "\n",
    "Decision trees are sensitive to the noise in the data and can easily overfit if the tree is too deep.\n",
    "To avoid overfitting, one can use pruning or regularization techniques like cost complexity pruning or limiting the max depth of the tree.\n",
    "\n",
    "Decision trees can also be used as a base model for ensemble methods like Random Forest and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to determine the best split at each node in the tree.\n",
    "\n",
    "**Gini index** is used to evaluate the quality of a split between two sets of data. The smaller the Gini index, the more pure the split, meaning that the two sets are made up of more homogeneous classes.\n",
    "\n",
    "The Gini index is calculated as follows:\n",
    "\n",
    "$$Gini(S) = 1 - \\sum(p_i)^2$$\n",
    "\n",
    "Where $S$ is the set of data being split and $p_i$ is the proportion of items from class $i$ in the set.\n",
    "\n",
    "\n",
    "**Entropy** is used to evaluate the quality of a split between two sets of data. The greater the entropy, the more mixed the classes in the set, meaning that the set is more disordered. The goal of splitting the data is to reduce the entropy and increase the homogeneity of the sets, which results in a more accurate and stable decision tree.\n",
    "\n",
    "The entropy of a set $S$ is calculated as follows:\n",
    "\n",
    "$$ Entropy(S) = -\\sum(p_i \\cdot \\log(p_i))$$\n",
    "\n",
    "Where $S$ is the set of data being split, $p_i$ is the proportion of items from class $i$ in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the Iris dataset using decision trees in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "df = pd.read_csv(url, header=None)\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train a decision tree classifier using the gini index\n",
    "clf = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=df.columns[:-1], class_names=np.unique(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train a decision tree classifier using the entropy criterion\n",
    "clf = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=df.columns[:-1], class_names=np.unique(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning a decision tree\n",
    "\n",
    "Pruning a decision tree refers to removing branches that provide little contribution to the prediction accuracy of the tree.\n",
    "This results in a smaller and simpler tree that is less prone to overfitting.\n",
    "The process of pruning involves removing parts of the tree that lead to low information gain, reduce the size of the tree, and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning a decision tree using Scikit-learn\n",
    "\n",
    "We prune the tree by setting the `min_samples_leaf` hyperparameter to 5.\n",
    "This hyperparameter determines the minimum number of samples required to be at a leaf node.\n",
    "By setting this to 5, we effectively prevent the tree from splitting nodes that have less than 5 samples, which results in fewer and larger leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Prune the tree by setting the min_samples_leaf hyperparameter\n",
    "clf_pruned = DecisionTreeClassifier(min_samples_leaf=5, random_state=0)\n",
    "\n",
    "# Train the classifier\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "score_pruned = clf_pruned.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score_pruned.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf_pruned.fit(X, y)\n",
    "\n",
    "# Plot the (smaller) decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf_pruned, filled=True, feature_names=df.columns[:-1], class_names=np.unique(y))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
