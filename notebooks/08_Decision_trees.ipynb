{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Lecture 4: GRA 4160 - Predictive Modelling with Machine Learning\n",
    "### Lecturer: Vegard H. Larsen\n",
    "\n",
    "---\n",
    "### Overview\n",
    "- **What are Decision Trees?**\n",
    "- **Impurity measures** (Gini, Entropy)\n",
    "- **Building and visualizing a Decision Tree** using scikit-learn\n",
    "- **Pruning** to avoid overfitting\n",
    "\n",
    "Decision Trees are used in both **classification** and **regression** contexts. They are often chosen for their **interpretability** and straightforward approach to handling numeric and categorical features. However, trees can grow very deep and overfit unless carefully controlled (e.g., pruning, limiting depth).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree Intuition\n",
    "\n",
    "A decision tree is built by **recursively partitioning** the data into subsets based on certain splitting criteria. For example:\n",
    "\n",
    "1. **Root Node**: Contains the entire training set.\n",
    "2. **Split**: Evaluate each feature and possible thresholds to find a split that best separates the classes.\n",
    "3. **Child Nodes**: Data is partitioned into subsets based on the selected split.\n",
    "4. **Repeat** until stopping criteria are met (e.g., max depth, min samples per leaf, or no further improvement).\n",
    "\n",
    "In **classification** problems, leaf nodes represent class labels (the majority or predicted probability). In **regression**, leaf nodes contain a predicted numeric value (often the mean of that node's samples).\n",
    "\n",
    "> **Key advantage**: The resulting model is relatively easy to **interpret** (by following the path of splits from root to leaf). The splits can sometimes mimic human decision processes.\n",
    "\n",
    "> **Key disadvantage**: Decision Trees can **overfit** easily if allowed to grow too large or too specific, capturing noise instead of true patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impurity Measures\n",
    "\n",
    "### Gini Index\n",
    "For a set \\(S\\), the **Gini index** is:\n",
    "$$\\text{Gini}(S) = 1 - \\sum_{i=1}^{C} p_i^2,$$\n",
    "where \\(p_i\\) is the fraction of elements in \\(S\\) belonging to class \\(i\\), and \\(C\\) is the number of classes.\n",
    "\n",
    "- **Interpretation**: Measures the chance of misclassifying a sample if we randomly pick a label from the distribution of classes in \\(S\\).\n",
    "- Smaller Gini => more homogeneous set.\n",
    "\n",
    "### Entropy\n",
    "For a set \\(S\\), the **entropy** is:\n",
    "$$\\text{Entropy}(S) = -\\sum_{i=1}^{C} p_i \\log_2(p_i).$$\n",
    "\n",
    "- **Interpretation**: Measures the \"disorder\" of \\(S\\). The higher the entropy, the more mixed the classes.\n",
    "- Lower entropy => more homogeneous set => better purity.\n",
    "\n",
    "In scikit-learn, you can choose which criterion (`\"gini\"` or `\"entropy\"`) to use for splits. In practice, both often yield similar results, although `gini` is somewhat faster to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifying the Iris Dataset with Decision Trees\n",
    "\n",
    "The **Iris** dataset is a classic dataset with 150 samples, each belonging to one of three species of Iris (Setosa, Versicolor, Virginica). Each sample has 4 features (sepal length, sepal width, petal length, petal width). Our goal is to predict the species of Iris based on these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "df = pd.read_csv(url, header=None)\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Training and Test Sets\n",
    "We'll split 70% of the data for training and 30% for testing, ensuring we can measure how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Train-test split (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Train a default Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy with default DecisionTree:\", round(score, 4))\n",
    "\n",
    "# We'll also visualize how large the trained tree is (in terms of depth)\n",
    "print(\"Tree depth:\", clf.get_depth())\n",
    "print(\"Number of leaves:\", clf.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Criteria: Gini vs. Entropy\n",
    "The default criterion in scikit-learn is `\"gini\"`. If we want to use `\"entropy\"` (which effectively uses **information gain**), we can specify that as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training a Decision Tree with Gini index explicitly\n",
    "clf_gini = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
    "clf_gini.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plot_tree(\n",
    "    clf_gini,\n",
    "    filled=True,\n",
    "    feature_names=df.columns[:-1],\n",
    "    class_names=np.unique(y)\n",
    ")\n",
    "plt.title(\"Decision Tree using Gini\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training a Decision Tree with Entropy\n",
    "clf_entropy = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "clf_entropy.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plot_tree(\n",
    "    clf_entropy,\n",
    "    filled=True,\n",
    "    feature_names=df.columns[:-1],\n",
    "    class_names=np.unique(y)\n",
    ")\n",
    "plt.title(\"Decision Tree using Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Overfitting\n",
    "Decision Trees can grow very deep, essentially **memorizing** the training data if no constraints are used.\n",
    "\n",
    "Common ways to reduce overfitting:\n",
    "- **Pruning** (e.g., cost complexity pruning, post-pruning, or pre-pruning methods like `min_samples_leaf`)\n",
    "- **Limiting Tree Depth** (via `max_depth`)\n",
    "- **Minimum Samples per Split** (`min_samples_split`)\n",
    "- **Minimum Samples per Leaf** (`min_samples_leaf`)\n",
    "\n",
    "We'll demonstrate a simple approach by adjusting `min_samples_leaf` to prune the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pruning a Decision Tree\n",
    "Pruning removes branches that **contribute little** to reducing errors on unseen data, **simplifying** the tree.\n",
    "Below, we show how setting the `min_samples_leaf` hyperparameter to 5 can reduce the tree's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prune the tree by requiring at least 5 samples in each leaf\n",
    "clf_pruned = DecisionTreeClassifier(\n",
    "    min_samples_leaf=5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "score_pruned = clf_pruned.score(X_test, y_test)\n",
    "print(\"Accuracy with min_samples_leaf=5:\", round(score_pruned, 4))\n",
    "\n",
    "# Compare tree depth\n",
    "print(\"Tree depth (pruned):\", clf_pruned.get_depth())\n",
    "print(\"Number of leaves (pruned):\", clf_pruned.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize the pruned tree (trained on the ENTIRE dataset to see full structure)\n",
    "# NOTE: Typically, you'd want to fit on training set, but we'll do it on X, y for illustration.\n",
    "clf_pruned_full = DecisionTreeClassifier(\n",
    "    min_samples_leaf=5,\n",
    "    random_state=0\n",
    ")\n",
    "clf_pruned_full.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plot_tree(\n",
    "    clf_pruned_full,\n",
    "    filled=True,\n",
    "    feature_names=df.columns[:-1],\n",
    "    class_names=np.unique(y)\n",
    ")\n",
    "plt.title(\"Pruned Decision Tree with min_samples_leaf=5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Check if the pruned tree has **less depth** and if the accuracy remains acceptable.\n",
    "- In some cases, pruning can **improve** test-set accuracy by reducing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercises/Extensions\n",
    "1. **Try more pruning:** Adjust `min_samples_leaf` to different values (e.g., 1, 2, 10) or set a `max_depth`. How do the accuracy and tree structure change?\n",
    "2. **Cross-validation**: Instead of a single train-test split, use `GridSearchCV` or `RandomizedSearchCV` to systematically find the best combination of `max_depth`, `min_samples_leaf`, etc.\n",
    "3. **Feature importance**: Use `clf.feature_importances_` to see which features are most used by the tree.\n",
    "4. **Compare Gini vs. Entropy** performance on the same train-test split. Do you notice any consistent difference?\n",
    "5. **Try the model on another dataset** (e.g., Wine dataset) and see how the tree changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "In this notebook, we covered:\n",
    "- **Decision Tree fundamentals**: Node splitting, impurity measures, interpretability.\n",
    "- **Building** a Decision Tree using scikit-learn's `DecisionTreeClassifier`.\n",
    "- **Visualizing** the tree structure to understand the model's logic.\n",
    "- **Pruning** to reduce overfitting and simplify the model.\n",
    "\n",
    "Remember that while decision trees are powerful, they can be fragile if not pruned or regularized. This is why advanced \n",
    "ensemble methods such as **Random Forest** or **Gradient Boosting** often outperform a single tree in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
