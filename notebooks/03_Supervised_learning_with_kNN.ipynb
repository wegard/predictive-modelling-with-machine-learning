{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with $k$-NN\n",
    "\n",
    "## Lecture 2\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of supervised learning\n",
    "\n",
    "Supervised learning is a type of machine learning in which a model is trained on labeled data. This means that the data used to train the model includes both input data and corresponding correct output labels. The goal of supervised learning is to use the labeled training data to learn the relationship between the input and output data, and then make predictions on new, unseen data.\n",
    "\n",
    "During the training phase, the model is presented with the input data and the corresponding correct output labels. The model makes predictions based on the input data, and then compares its predictions to the correct labels to determine how accurate they are. The model is then adjusted, or \"trained\", based on the error between its predictions and the correct labels. This process is repeated multiple times, using different portions of the training data or different combinations of training and validation data, until the model reaches a satisfactory level of accuracy. Once the model is trained, it can be used to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of supervised learning problems\n",
    "\n",
    "1. **Classification**: The goal is to predict a discrete class label (e.g., \"spam\" or \"not spam\" for an email classification problem).\n",
    "2. **Regression**: The goal is to predict a continuous numerical value (e.g., the price of a house based on its characteristics).\n",
    "3. **Forecasting**: The goal is to predict future values of a given time series (e.g., stock prices or weather data).\n",
    "4. **Object recognition**: The goal is to classify objects in images or videos based on their characteristics.\n",
    "5. **Speech recognition**: The goal is to transcribe spoken language into written text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with $k$-nearest neighbors\n",
    "\n",
    "The k-nearest neighbors ($k$-NN) algorithm is a simple and widely used machine learning technique for classification. The basic idea is to find the $k$ training examples that are closest (i.e., most similar) to a given test point, and then assign the label or value of the majority of those $k$ examples to the test point.\n",
    "\n",
    "The most common way to measure the similarity between two points is to use the Euclidean distance, which is defined as the square root of the sum of the squared differences of their coordinates:\n",
    "\n",
    "$$d(x,y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2}$$\n",
    "\n",
    "where $x$ and $y$ are two points in $n$-dimensional space, and $x_i$ and $y_i$ are the $i$-th coordinates of $x$ and $y$.\n",
    "\n",
    "In a $k$-NN classification problem, given a test point $x$ and a set of training points with labels, the $k$-NN algorithm works as follows:\n",
    "\n",
    "1. For each training point, calculate the distance between it and $x$.\n",
    "2. Sort the training points by increasing distance to $x$.\n",
    "3. Select the $k$ training points that are closest to $x$.\n",
    "4. Assign the label of the majority of the $k$ selected points to $x$.\n",
    "\n",
    "This simple algorithm can be very effective for some types of problems, especially if the decision boundary between classes is relatively smooth and regular. However, it can be sensitive to the choice of $k$ and the presence of noisy or irrelevant features in the data.\n",
    "\n",
    "Let's use $k$-nearest neighbors on the iris datataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the training data and labels (10 first samples)\n",
    "X_train[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-NN from scratch\n",
    "\n",
    "We define two helper functions: `euclidean_distance` which calculates the euclidean distance between two points, and `predict_label` which finds the most common label among the k nearest neighbors of a test point in the training set. Finally, we test the classifier on the first test point with $k = 3$, and compare the predicted label to the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\"Calculate the euclidean distance between two points\"\"\"\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def predict(X_train, y_train, X_test, k):\n",
    "    \"\"\"Find the k nearest neighbors of x_test in X_train\"\"\"\n",
    "    distances = [euclidean_distance(X_test, x) for x in X_train]\n",
    "    k_nearest = np.argsort(distances)[:k]\n",
    "    return y_train[k_nearest]\n",
    "\n",
    "def predict_label(X_train, y_train, X_test, k):\n",
    "    \"\"\"Find the most common label among the k nearest neighbors of X_test in X_train\"\"\"\n",
    "    return np.bincount(predict(X_train, y_train, X_test, k)).argmax()\n",
    "\n",
    "# Test the classifier on the first test point with k = 3\n",
    "print(\"True label:\", y_test[0])\n",
    "print(\"Predicted label:\", predict_label(X_train, y_train, X_test[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the whole test set\n",
    "y_pred = [predict_label(X_train, y_train, x, 3) for x in X_test]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualize a test point on some simulated data with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create simulated data for visualization\n",
    "np.random.seed(0)\n",
    "X_train_sim, y_train_sim = make_blobs(n_samples=150, centers=3, n_features=2)\n",
    "X_test_sim = np.array([np.random.uniform(X_train_sim.min(), X_train_sim.max(), 2)])\n",
    "\n",
    "# Find the 3 nearest neighbors\n",
    "distances = [euclidean_distance(X_test_sim[0], x) for x in X_train_sim]\n",
    "k_nearest = np.argsort(distances)[:3]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_sim[:, 0], X_train_sim[:, 1], c=y_train_sim, cmap='viridis', label='Training data')\n",
    "plt.scatter(X_test_sim[:, 0], X_test_sim[:, 1], c='red', marker='*', s=200, label='Test point')\n",
    "plt.scatter(X_train_sim[k_nearest][:, 0], X_train_sim[k_nearest][:, 1], c='black', marker='o', s=100, label='Nearest Neighbors')\n",
    "\n",
    "plt.title('k-Nearest Neighbors Classifier Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn\n",
    "\n",
    "We create a $k$-NN classifier with 3 neighbors and fit it to the training data using the `.fit()` method. We make predictions on the test data using the `.predict()` method, and print the accuracy of the classifier by comparing the predicted labels to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a k-NN classifier with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "print(f\"Accuracy: {knn.score(X_test, y_test) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
