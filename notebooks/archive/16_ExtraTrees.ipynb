{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTrees classifier\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ExtraTrees (Extremely Randomized Trees)\n",
    "\n",
    "**Selecting a Subset of Features:** At each node in the tree, both Random Forest and Extra Trees algorithms start by selecting a random subset of the features (or predictors).\n",
    "\n",
    "**Determining the Split Point:**\n",
    "\n",
    "- *Random Forest:* Once the subset of features is selected, the Random Forest algorithm will search for the best possible split point among these features. This involves finding the value that best separates the data according to the target variable, often using a criterion like Gini impurity or entropy in classification tasks. This process is somewhat similar to what a standard decision tree does, but limited to a subset of features.\n",
    "\n",
    "- *Extra Trees:* In contrast, the Extra Trees algorithm introduces more randomness. After selecting a subset of features, instead of searching for the most optimal split based on some criterion, it randomly selects a split point for each feature. Then, among these randomly generated splits, it chooses one to split the node. This means that the algorithm does not necessarily choose the best split from a statistical perspective, but rather a random one.\n",
    "\n",
    "**Impact of Random Splits:**\n",
    "This increased randomness in choosing splits can lead to more diversified trees within the ensemble, as it reduces the likelihood of creating similar trees even if they are based on the same training data.\n",
    "\n",
    "As a result, the individual trees in an Extra Trees ensemble can have higher bias compared to those in a Random Forest, but when combined, the ensemble as a whole often has lower variance. This is because the random splits lead to less correlated trees, which is beneficial in an ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "df = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "df = df.dropna()\n",
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7391304347826086\n",
      "Precision: 0.7878787878787878\n",
      "Recall: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Create an Extra Trees classifier object\n",
    "etc = ExtraTreesClassifier(n_estimators=100, max_depth=3, random_state=1)\n",
    "\n",
    "# Train the Extra Trees classifier on the training data\n",
    "etc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = etc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the Extra Trees classifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate in test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f'Survival rate in test set: {y_test.sum()/len(y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you build a Extra Trees classifier using only the DecisionTreeClassifier class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class SimpleRandomSplitTree(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Check that y has acceptable targets\n",
    "        check_classification_targets(y)\n",
    "\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        self.tree_ = self._grow_tree(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        # Stopping criteria: if all targets are the same or if maximum depth is reached\n",
    "        if len(set(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "    \n",
    "        # Attempt to split until valid split is found or decide it's a leaf node\n",
    "        for _ in range(n_features):\n",
    "            feature_idx = np.random.randint(0, n_features)\n",
    "            unique_values = np.unique(X[:, feature_idx])\n",
    "\n",
    "            # If there's less than 2 unique values, can't split on this feature\n",
    "            if unique_values.size < 2:\n",
    "                continue\n",
    "\n",
    "            split_value = np.random.uniform(X[:, feature_idx].min(), X[:, feature_idx].max())\n",
    "\n",
    "            left_idx = X[:, feature_idx] < split_value\n",
    "            right_idx = ~left_idx\n",
    "\n",
    "            # Check if the split actually divides the dataset\n",
    "            if np.any(left_idx) and np.any(right_idx):\n",
    "                left_child = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "                right_child = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "                return (feature_idx, split_value, left_child, right_child)\n",
    "\n",
    "        # If no valid split found, return the most common target as leaf node\n",
    "        return np.argmax(np.bincount(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        predictions = [self._predict_one(x, self.tree_) for x in X]\n",
    "        return self.classes_[np.array(predictions)]\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        # If we have a leaf node\n",
    "        if not isinstance(node, tuple):\n",
    "            return node\n",
    "\n",
    "        # Decide whether to follow left or right child\n",
    "        feature_idx, split_value, left_child, right_child = node\n",
    "        if x[feature_idx] < split_value:\n",
    "            return self._predict_one(x, left_child)\n",
    "        else:\n",
    "            return self._predict_one(x, right_child)\n",
    "\n",
    "# Now we update the SimpleExtraTreesClassifier to use this new tree\n",
    "class SimpleExtraTreesClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = SimpleRandomSplitTree(max_depth=self.max_depth)\n",
    "\n",
    "            # Randomly select features\n",
    "            if self.max_features == 'sqrt':\n",
    "                size = int(np.sqrt(n_features))\n",
    "            elif self.max_features == 'log2':\n",
    "                size = int(np.log2(n_features))\n",
    "            else:\n",
    "                size = n_features\n",
    "\n",
    "            features_idx = np.random.choice(range(n_features), size=size, replace=False)\n",
    "            X_subset = X.iloc[:, features_idx]\n",
    "\n",
    "            # Train the tree\n",
    "            tree.fit(X_subset, y)\n",
    "            self.trees.append((tree, features_idx))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((self.n_estimators, X.shape[0]), dtype=np.int64)\n",
    "        for i, (tree, features_idx) in enumerate(self.trees):\n",
    "            predictions[i] = tree.predict(X.iloc[:, features_idx])\n",
    "\n",
    "        # Majority voting\n",
    "        final_predictions, _ = mode(predictions, axis=0)\n",
    "        return final_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/j79jztxx2dgfzd595frqz2fw0000gn/T/ipykernel_1791/4055373461.py:106: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_predictions, _ = mode(predictions, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "clf = SimpleExtraTreesClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6739130434782609\n",
      "Precision: 0.6818181818181818\n",
      "Recall: 0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
