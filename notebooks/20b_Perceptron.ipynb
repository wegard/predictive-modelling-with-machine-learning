{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "## Lecture 10\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The perceptron\n",
    "\n",
    "The perceptron is a fundamental building block of artificial neural networks and a simple computational model inspired by the structure and function of biological neurons.\n",
    "\n",
    "A perceptron is essentially a linear classifier that takes multiple input values (also known as features), processes them, and generates an output based on a linear combination of these inputs and their associated weights.\n",
    "The input values are multiplied by their corresponding weights, and the results are summed up.\n",
    "An activation function, usually a step function or a threshold function, is then applied to the weighted sum.\n",
    "If the sum exceeds a certain threshold, the perceptron outputs a value (typically 1), indicating that the input belongs to one class.\n",
    "If the sum is below the threshold, it outputs another value (usually 0), indicating that the input belongs to the other class.\n",
    "\n",
    "The perceptron learning algorithm involves iteratively updating the weights based on the difference between the predicted output and the actual output for each input instance.\n",
    "The algorithm adjusts the weights to minimize the classification error, allowing the perceptron to learn the best separating hyperplane between the two classes.\n",
    "\n",
    "It is important to note that the perceptron can only learn linearly separable patterns.\n",
    "If the data is not linearly separable, the perceptron will fail to converge to a solution.\n",
    "This limitation led to the development of more advanced neural network architectures, such as the multilayer perceptron (MLP), which can model complex, non-linear relationships between inputs and outputs through the use of multiple layers and non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The multilayer perceptron\n",
    "\n",
    "MLPs are widely used in machine learning and pattern recognition tasks, such as image and speech recognition, natural language processing, and data classification.\n",
    "The fundamental building block of an MLP is the artificial neuron, or perceptron, which is a simple computing unit designed to process and transmit information in a manner analogous to a biological neuron.\n",
    "The perceptron receives input from multiple sources, processes it, and generates an output based on a mathematical function, typically called an activation function.\n",
    "\n",
    "An MLP consists of multiple layers of perceptrons, each connected to the perceptrons in the layers immediately before and after it.\n",
    "These layers are classified into three types: the input layer, hidden layers, and the output layer.\n",
    "The input layer receives the input data, and the output layer produces the final prediction or classification.\n",
    "The hidden layers, which are sandwiched between the input and output layers, perform complex transformations and computations on the data.\n",
    "The number of hidden layers and perceptrons within each layer can vary, depending on the complexity of the problem being addressed.\n",
    "\n",
    "The connections between perceptrons in an MLP are associated with weights, which are numerical values that determine the strength and direction of the information flow between perceptrons.\n",
    "During the training phase, the MLP learns to adjust these weights using backpropagation, which minimizes the error between the network's predictions and the actual output.\n",
    "Backpropagation is an iterative optimization algorithm that computes the gradient of the error with respect to each weight by utilizing the chain rule from calculus.\n",
    "The weights are then updated in accordance with the computed gradient.\n",
    "\n",
    "Once trained, the MLP can be used to make predictions or classify new data by propagating the input through the network's layers.\n",
    "The input data is first fed into the input layer, where it is processed and passed to the first hidden layer.\n",
    "The perceptrons in the hidden layers apply the activation functions, and the processed information is forwarded from layer to layer until it reaches the output layer.\n",
    "The output layer generates the final prediction or classification, which can then be compared to the desired output to evaluate the performance of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
