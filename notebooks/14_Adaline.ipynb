{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb5ea15",
   "metadata": {},
   "source": [
    "# Implementing an Adaptive Linear Neuron (Adaline) in Python\n",
    "\n",
    "In this notebook, we will implement an **Adaptive Linear Neuron (Adaline)** using gradient descent. Adaline is a single-layer neural network that uses a linear activation function, and it was developed by Bernard Widrow and Ted Hoff.\n",
    "\n",
    "Unlike the classical perceptron (which uses a hard threshold for its activation), Adaline uses a continuous linear activation. It trains by minimizing a **mean squared error** (MSE) cost function rather than the misclassification rate directly. When the weights converge under gradient descent, Adaline can learn linear decision boundaries for binary classification tasks.\n",
    "\n",
    "We will apply Adaline on a subset of the **Iris dataset**, specifically using data from the Iris-setosa and Iris-versicolor classes. These data points can be separated by a linear boundary based on their sepal length and petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbbb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d5e4c",
   "metadata": {},
   "source": [
    "## The Iris dataset\n",
    "\n",
    "The [Iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris) is a very popular dataset in machine learning. It consists of 150 flower samples belonging to three different species of Iris (Iris-setosa, Iris-versicolor, Iris-virginica). For each flower, the following features are provided:\n",
    "\n",
    "1. Sepal length (in cm)\n",
    "2. Sepal width (in cm)\n",
    "3. Petal length (in cm)\n",
    "4. Petal width (in cm)\n",
    "\n",
    "In this notebook, we'll focus only on **Iris-setosa** and **Iris-versicolor**, i.e., the first 100 entries (50 of each). We'll use:\n",
    "- **Sepal length** (feature index 0)\n",
    "- **Petal length** (feature index 2)\n",
    "\n",
    "to build our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading-in the Iris data\n",
    "# Note: If the URL is not accessible, the code will try to read the file 'iris.data' locally.\n",
    "\n",
    "try:\n",
    "    s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    print('From URL:', s)\n",
    "    df = pd.read_csv(s,\n",
    "                     header=None,\n",
    "                     encoding='utf-8')\n",
    "except Exception:\n",
    "    s = 'iris.data'\n",
    "    print('From local Iris path:', s)\n",
    "    df = pd.read_csv(s,\n",
    "                     header=None,\n",
    "                     encoding='utf-8')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100854b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select only setosa and versicolor (first 100 rows)\n",
    "y = df.iloc[0:100, 4].values\n",
    "# Encode the class labels: Setosa=0, Versicolor=1\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "\n",
    "# Extract sepal length (col 0) and petal length (col 2)\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "\n",
    "# Plot the two classes\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label='Setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='s', label='Versicolor')\n",
    "\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plots the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = [n_examples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_examples]\n",
    "        True class labels.\n",
    "    classifier : object\n",
    "        Trained classifier with a .predict() method.\n",
    "    resolution : float (optional)\n",
    "        Step size for the meshgrid.\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    # Predict class labels for each point on the meshgrid\n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "\n",
    "    # Plot the decision surface (regions)\n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0],\n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8,\n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx],\n",
    "                    label=f'Class {cl}',\n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6b4ad",
   "metadata": {},
   "source": [
    "## Defining the Adaline algorithm\n",
    "\n",
    "Below is the implementation of a simple Adaline classifier using **gradient descent**. Key points:\n",
    "\n",
    "- We initialize the weights (including a bias term) to small random numbers.\n",
    "- We compute the **net input** as \\( z = X \\cdot w + b \\).  \n",
    "- The **activation** function for Adaline is the identity function (i.e., output is just the net input).  \n",
    "- We use **Mean Squared Error (MSE)** as the loss function.  \n",
    "- Each epoch, we update the weights by computing the gradient of the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD:\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes (epochs) over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight initialization.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    b_ : float\n",
    "      Bias term after fitting.\n",
    "    losses_ : list\n",
    "      Mean squared error (MSE) loss values at each epoch.\n",
    "    missed_ : list\n",
    "      Number of misclassified labels at each epoch (based on a 0.5 threshold).\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_examples, n_features]\n",
    "            Training vectors.\n",
    "        y : array-like, shape = [n_examples]\n",
    "            Target values (0 or 1 in our case).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        # Initialize weights and bias\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = 0.0\n",
    "\n",
    "        self.losses_ = []\n",
    "        self.missed_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            # Net input\n",
    "            net_input = self.net_input(X)\n",
    "            # Adaline output is the linear activation (identity)\n",
    "            output = self.activation(net_input)\n",
    "            # Errors\n",
    "            errors = (y - output)\n",
    "\n",
    "            # Count how many misclassifications at this epoch\n",
    "            # Using threshold 0.5: if activation >= 0.5 => class 1, else 0\n",
    "            self.missed_.append(np.sum(np.abs(y - self.predict(X))))\n",
    "\n",
    "            # Gradient update\n",
    "            # Update each weight using the average gradient\n",
    "            for j in range(self.w_.shape[0]):\n",
    "                self.w_[j] += self.eta * (X[:, j] * errors).mean()\n",
    "            self.b_ += self.eta * errors.mean()\n",
    "\n",
    "            # Compute the Mean Squared Error (MSE)\n",
    "            loss = (errors ** 2).mean()\n",
    "            self.losses_.append(loss)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate the net input z = Xw + b.\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Linear (identity) activation function for Adaline.\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step at threshold 0.5\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72811869",
   "metadata": {},
   "source": [
    "## Training Adaline on the Iris dataset\n",
    "\n",
    "We now train the Adaline classifier on the two-class Iris problem using the raw (unscaled) features. Keep in mind that if the learning rate is too large, the algorithm might not converge, and if it's too small, the algorithm will converge slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44873e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "ada = AdalineGD(n_iter=15, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0dfdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58261aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(ada.losses_) + 1), ada.losses_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title(f'Adaline - Learning rate {eta}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(ada.missed_) + 1), ada.missed_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Misclassified labels')\n",
    "plt.title(f'Adaline - Learning rate {eta}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ada)\n",
    "plt.title('Adaline - Gradient Descent (Unscaled Features)')\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48d679",
   "metadata": {},
   "source": [
    "### Changing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ada1 = AdalineGD(n_iter=15, eta=0.1).fit(X, y)\n",
    "ax[0].plot(range(1, len(ada1.losses_) + 1), ada1.losses_, marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Mean squared error')\n",
    "ax[0].set_title('Adaline - Learning rate 0.1')\n",
    "\n",
    "ada2 = AdalineGD(n_iter=1000, eta=0.0001).fit(X, y)\n",
    "ax[1].plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean squared error')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408aa8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ax[0].plot(range(1, len(ada1.missed_) + 1), ada1.missed_, marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Misclassified labels')\n",
    "ax[0].set_title('Adaline - Learning rate 0.1')\n",
    "\n",
    "ax[1].plot(range(1, len(ada2.missed_) + 1), ada2.missed_, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Misclassified labels')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ada1)\n",
    "plt.title('Adaline - Gradient Descent (Learning rate 0.1)')\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394230d",
   "metadata": {},
   "source": [
    "## Improving gradient descent through feature scaling\n",
    "\n",
    "When features have very different scales (e.g., sepal length in centimeters vs. smaller or larger ranges), gradient descent can take a long time to converge. **Standardization** often helps the algorithm to converge faster by scaling all features to a mean of 0 and standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56451c18",
   "metadata": {},
   "source": [
    "### Standardizing the features\n",
    "\n",
    "We compute the standardized value for a feature $x$ as:\n",
    "$$x_{\\text{std}} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f941ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n",
    "\n",
    "X_std[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886dd9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada_gd = AdalineGD(n_iter=15, eta=0.1)\n",
    "ada_gd.fit(X_std, y)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(ada_gd.losses_) + 1), ada_gd.losses_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Adaline - Gradient Descent (Standardized Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f85d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(ada_gd.missed_) + 1), ada_gd.missed_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Misclassified labels')\n",
    "plt.title('Adaline - Learning rate 0.1 (Standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91035c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_decision_regions(X_std, y, classifier=ada_gd)\n",
    "plt.title('Adaline - Gradient Descent (Standardized)')\n",
    "plt.xlabel('Sepal length [standardized]')\n",
    "plt.ylabel('Petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2dd11a",
   "metadata": {},
   "source": [
    "With standardized features, the algorithm converges much more smoothly and typically achieves better performance when the learning rate is chosen appropriately.\n",
    "\n",
    "---\n",
    "## Conclusions\n",
    "- Adaline uses a linear activation function and MSE loss.\n",
    "- Feature standardization can drastically improve convergence speed.\n",
    "- The learning rate ($\\eta$) must be chosen carefully. A rate too large can cause divergence, while a very small rate might slow down convergence.\n",
    "\n",
    "Next steps could include trying different feature subsets, adding regularization, or experimenting with alternative optimization techniques (like stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
