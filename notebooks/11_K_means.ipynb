{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $K$-means\n",
    "\n",
    "## Lecture 6\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-means is a simple and widely used clustering algorithm that partitions the data into $K$ clusters based on similarity of data points.\n",
    "The algorithm starts by randomly selecting $K$ cluster centers, then iteratively assigns each data point to the nearest center, and updates the centers to the mean of the assigned data points.\n",
    "This process is repeated until the clusters are stable.\n",
    "\n",
    "$K$-means assumes that the clusters are spherical and have equal variance, which may not always be the case.\n",
    "The algorithm is also sensitive to the initial placement of the cluster centers, and may get stuck in local optima.\n",
    "Therefore, multiple initialization may be needed to find the global optimum.\n",
    "\n",
    "It's important to preprocess the data before applying $K$-means clustering.\n",
    "This may include handling missing values, encoding categorical variables, and normalizing the features to a common scale.\n",
    "Normalization can help improve the performance of the algorithm.\n",
    "\n",
    "One of the most important considerations in $k$-means clustering is determining the number of clusters.\n",
    "This can be done using a variety of methods, such as the elbow method or the silhouette method, which evaluate the within-cluster sum of squares and the quality of the clustering respectively.\n",
    "\n",
    "It's important to evaluate the performance of the clustering to determine whether it has successfully identified meaningful patterns in the data.\n",
    "This can be done using metrics such as the within-cluster sum of squares or the silhouette coefficient.\n",
    "\n",
    "Once the clusters have been identified, it's important to interpret and visualize the results.\n",
    "This may involve analyzing the key features that distinguish the clusters, visualizing the clusters using scatter plots or other visualization techniques, and investigating any patterns or insights in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the iris data set\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                 header=None, \n",
    "                 names=['SepalLengthCm', 'SepalWidthCm', \n",
    "                        'PetalLengthCm', 'PetalWidthCm', 'Species'])\n",
    "\n",
    "# Subset the data set to include only petal length and width\n",
    "X = df[['PetalLengthCm', 'PetalWidthCm']]\n",
    "\n",
    "# Apply k-means clustering with 3 clusters (one for each species of iris)\n",
    "# the n_init parameter specifies the number of time the k-means algorithm will be run with different centroid seeds. \n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10).fit(X)\n",
    "\n",
    "# Visualize the clusters using a scatter plot\n",
    "plt.scatter(X['PetalLengthCm'], X['PetalWidthCm'], c=kmeans.labels_)\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('K-means Clustering of Iris Flowers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "The Within-Cluster Sum of Squares (WCSS) is a metric used to evaluate the performance of clustering algorithms, particularly the $K$-means clustering algorithm.\n",
    "It measures the sum of the squared distances between each data point and its assigned cluster centroid. In other words, it measures the compactness of the clusters.\n",
    "\n",
    "The WCSS is computed by summing the squared Euclidean distances between each data point and its assigned cluster centroid, for all data points in all clusters. \n",
    "Mathematically, it is given by:\n",
    "\n",
    "$$WCSS = \\sum\\left(\\sum((X - centroid)^2)\\right)$$\n",
    "\n",
    "where $X$ is the data matrix, centroid is the centroid matrix (containing the coordinates of the centroids for each cluster), and the inner sum is taken over all data points in all clusters.\n",
    "\n",
    "The goal of the $K$-means clustering algorithm is to minimize the WCSS, which corresponds to finding the cluster centroids that are closest to the data points. \n",
    "The optimal number of clusters can be determined by finding the \"elbow\" point in the WCSS curve, where the curve starts to level off.\n",
    "\n",
    "In practice, the WCSS is often used in conjunction with other metrics, such as the silhouette coefficient, to evaluate the performance of clustering algorithms and determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method involves plotting the within-cluster sum of squares (WCSS) as a function of the number of clusters. \n",
    "As the number of clusters increases, the WCSS typically decreases, because more clusters means the data points can be fitted more closely to their respective centroids. \n",
    "However, at some point the gain in WCSS reduction becomes less significant, and adding more clusters doesn't lead to significant improvement in clustering performance. \n",
    "This point is called the \"elbow\" of the WCSS curve, and the number of clusters at the elbow is often chosen as the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Score\n",
    "\n",
    "The Silhouette Score is another metric used to evaluate the performance of the K-means clustering algorithm.\n",
    "It measures the degree of similarity of each data point to its own cluster compared to other clusters.\n",
    "In other words, it measures the quality of the clusters.\n",
    "\n",
    "The Silhouette Score ranges from -1 to 1, where a score of 1 indicates that the data point is very similar to its own cluster and very dissimilar to other clusters, and a score of -1 indicates the opposite (i.e., the data point is very dissimilar to its own cluster and very similar to other clusters). A score of 0 indicates that the data point is equally similar to its own cluster and to other clusters.\n",
    "\n",
    "The Silhouette Score is computed by taking the mean Silhouette Coefficient of all data points in the data set. The Silhouette Coefficient for each data point is given by:\n",
    "\n",
    "$$s = \\frac{(b - a)}{\\max(a, b)}$$\n",
    "\n",
    "where $a$ is the mean distance between the data point and all other data points in its own cluster, and $b$ is the mean distance between the data point and all data points in the nearest cluster (i.e., the cluster that is most dissimilar to the data point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette method is a more direct approach than the elbow method to determine the optimal number of clusters. It measures the quality of the clustering by computing the silhouette score for each data point. The average silhouette score is computed for each value of $k$ (the number of clusters) and plotted as a function of $k$. The value of $k$ that maximizes the average silhouette coefficient is chosen as the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the elbow method and the silhouette method to determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Vary the number of clusters and evaluate the performance of the clustering\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
    "\n",
    "# Visualize the performance of the clustering using a plot of WCSS and Silhouette scores\n",
    "plt.plot(range(2, 10), wcss)\n",
    "plt.title('Within-Cluster Sum of Squares (WCSS) vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(2, 10), silhouette_scores)\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply $K$-means with the \"optimal\" number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply k-means clustering with 3 clusters (one for each species of iris)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=10).fit(X)\n",
    "\n",
    "# Visualize the clusters using a scatter plot\n",
    "plt.scatter(X['PetalLengthCm'], X['PetalWidthCm'], c=kmeans.labels_)\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('K-means Clustering of Iris Flowers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For more details, see: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
