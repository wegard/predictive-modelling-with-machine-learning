{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Bagging\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation (Bagging)\n",
    "\n",
    "- Bagging stands for Bootstrap Aggregation\n",
    "- It is an ensemble technique that combines multiple models trained on different subsets of the training data\n",
    "- Bagging reduces overfitting by aggregating the results of many base models trained on different subsets of the training data, leading to a more generalized model.\n",
    "- In bagging, each model is trained independently, and the final prediction is the average (in regression) or majority vote (in classification) of the predictions of the individual models.\n",
    "- One of the most popular bagging algorithms is the Random Forest algorithm (will be covered in the next lecture), which builds a collection of decision trees using random subsets of the features and training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "Bootstrap works by creating multiple samples from the original data by randomly sampling with replacement.\n",
    "In each bootstrap sample, some data points are selected multiple times, while others are not selected at all.\n",
    "This results in a new dataset that has the same size as the original dataset but with some variability in the data points.\n",
    "\n",
    "By creating multiple bootstrap samples and training the model on each sample, we can estimate the variability of the model's performance and parameter estimates, and compute confidence intervals for the model's predictions.\n",
    "This is particularly useful when the sample size is small, or when the distribution of the data is unknown or complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Breast Cancer with Bagging\n",
    "\n",
    "Here we will classify breast cancer based on a dataset of patient information.\n",
    "The dataset contains information on patient features such as their age, tumor size, and number of positive lymph nodes, as well as whether the patient's cancer has recurred.\n",
    "We train a classifier to predict whether a new patient's cancer is likely to recur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the Bagging Classifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of the BaggingClassifier\n",
    "bag_clf = BaggingClassifier(base_estimator=tree_clf, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier on the training set\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the confusion matrix of the classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Evaluate the classification report of the classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Hyperparameters\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of the BaggingClassifier with different hyperparameters\n",
    "bag_clf1 = BaggingClassifier(base_estimator=tree_clf, n_estimators=100, max_samples=10, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf2 = BaggingClassifier(base_estimator=tree_clf, n_estimators=200, max_samples=50, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf3 = BaggingClassifier(base_estimator=tree_clf, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf4 = BaggingClassifier(base_estimator=tree_clf, n_estimators=1000, max_samples=200, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train each BaggingClassifier on the training set\n",
    "bag_clf1.fit(X_train, y_train)\n",
    "bag_clf2.fit(X_train, y_train)\n",
    "bag_clf3.fit(X_train, y_train)\n",
    "bag_clf4.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of each BaggingClassifier on the testing set\n",
    "print(\"Bagging Classifier 1:\")\n",
    "print(\"Accuracy:\", bag_clf1.score(X_test, y_test))\n",
    "print(\"Bagging Classifier 2:\")\n",
    "print(\"Accuracy:\", bag_clf2.score(X_test, y_test))\n",
    "print(\"Bagging Classifier 3:\")\n",
    "print(\"Accuracy:\", bag_clf3.score(X_test, y_test))\n",
    "print(\"Bagging Classifier 4:\")\n",
    "print(\"Accuracy:\", bag_clf4.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Feature importances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the feature importances\n",
    "feature_importances = tree_clf.feature_importances_\n",
    "plt.barh(range(data.data.shape[1]), feature_importances)\n",
    "plt.yticks(range(data.data.shape[1]), data.feature_names)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
