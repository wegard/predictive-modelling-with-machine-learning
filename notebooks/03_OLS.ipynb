{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) regression\n",
    "\n",
    "## Lecture 2\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by generating some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the number of regressors or futures\n",
    "n_features = 1\n",
    "\n",
    "# Set the number of observations\n",
    "n_samples = 100\n",
    "\n",
    "# Set intercept to True/False\n",
    "has_intercept = False\n",
    "\n",
    "# Generate some random futures from a Uniform(0,1)\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Generate the true coefficients, also from a Uniform(0,1)\n",
    "coeffs = np.random.rand(n_features).round(2)\n",
    "\n",
    "# The data generating process is y = intercept + sum_i coeff_i * x_i + 0.2*u_i\n",
    "# where u_i is normally distributed error term,  N(O,1).\n",
    "if has_intercept:\n",
    "    intercept = np.random.rand(1).round(2)[0]\n",
    "    y = intercept + np.dot(X, coeffs) + 0.2*np.random.randn(n_samples)\n",
    "    y_true = intercept + np.dot(X, coeffs)\n",
    "else:\n",
    "    intercept = 0\n",
    "    y = np.dot(X, coeffs) + 0.2*np.random.randn(n_samples)\n",
    "    y_true = np.dot(X, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'The true coefficients are/is: {coeffs}')\n",
    "if has_intercept:\n",
    "    print(f'The true intercept is: {intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming n_features, X, y, y_true, intercept, coeffs are already defined\n",
    "if n_features == 1:\n",
    "    plt.scatter(X, y, label='Data')\n",
    "    plt.plot(X, y_true, color='red', label=f'DGP ($y={intercept} + {coeffs[0]}x$)')\n",
    "\n",
    "    # Randomly select 20 points\n",
    "    indices = np.random.choice(range(len(X)), 20, replace=False)\n",
    "    for i in indices:\n",
    "        # Draw line from point to regression line\n",
    "        plt.plot([X[i], X[i]], [y[i], y_true[i]], color='green', linestyle='--')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x$-value')\n",
    "    plt.ylabel('$y$-value')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Can only plot a two-dimensional figure.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to recover the data generating process (DGP) by using the observed data, $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic OLS recap:\n",
    "\n",
    "The ordinary least squares (OLS) method is a linear regression technique that finds the coefficients (also called weights or parameters) that minimize the sum of the squared residuals between the predicted values and the true values. Mathematically, this can be written as the following optimization problem\n",
    "\n",
    "$$\\arg\\min_{\\beta} \\sum_{i=1}^n (y_i - \\beta^T x_i)^2$$\n",
    "\n",
    "where $x_i$ is the $i$-th feature vector, $y_i$ is the $i$-th target, $\\beta$ is the coefficient vector (also called the betas). The optimal values of $\\beta$ are the OLS coefficients.\n",
    "\n",
    "To solve this optimization problem, we can set the derivative of the objective function with respect to $\\beta$ to zero and solve for $\\beta$. This leads to the following closed-form solution for the OLS coefficients:\n",
    "\n",
    "$$\\beta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "where $X$ is the feature matrix with shape (n_samples, n_features), $y$ is the target vector with shape (n_samples).\n",
    "\n",
    "We can easily solve for $\\beta$ using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "betas = np.linalg.inv(X.T@X)@(X.T@y)\n",
    "print(f'The estimated coefficients are: { betas.round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write a few more lines of code using OOP\n",
    "\n",
    "We will now use object-oriented programming (OOP) when coding up the ordinary least squares (OLS) method. This is the way every serious package in Python is written, and it is smart to start to think in this framework right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coeffs = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add intercept term to X if fit_intercept is True\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "        # Solve the least squares problem\n",
    "        XTX = np.dot(X.T, X)\n",
    "        XTY = np.dot(X.T, y)\n",
    "        self.coeffs = np.linalg.solve(XTX, XTY)\n",
    "\n",
    "        # Extract intercept term if fit_intercept is True\n",
    "        if self.fit_intercept:\n",
    "            self.intercept = self.coeffs[0]\n",
    "            self.coeffs = self.coeffs[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add intercept term to X if fit_intercept is True\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "            return np.dot(X, np.hstack([self.intercept, self.coeffs]))\n",
    "        return np.dot(X, self.coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = OLS(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "print(f'The estimated coefficients are: {model.coeffs.round(3)}')\n",
    "print(f'The estimated intercept is: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
