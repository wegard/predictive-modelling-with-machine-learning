{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection with information criteria and cross validation\n",
    "\n",
    "## Lecture 5\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso model selection: Diabetes dataset from Scikit-learn\n",
    "These notes are based on: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html\n",
    "\n",
    "The diabetes dataset is a pre-processed and cleaned version of the diabetes database from the National Institute of Diabetes and Digestive and Kidney Diseases.\n",
    "It consists of 442 samples, where each sample represents a patient with diabetes.\n",
    "The dataset has ten continuous features that describe various factors of each patient:\n",
    "\n",
    "- age\n",
    "- sex\n",
    "- body mass index (BMI)\n",
    "- average blood pressure (BP)\n",
    "- six blood serum measurements (S1, S2, S3, S4, S5, S6)\n",
    "\n",
    "The target variable is a quantitative measure of disease progression one year after baseline, as measured by the level of serum glucose. This target variable is a real-valued number ranging from 25 to 346.\n",
    "\n",
    "The dataset has been preprocessed so that each feature has been mean-centered and we will scale to unit variance. This means that each feature has zero mean and unit variance across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X = X/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(10)\n",
    "n_random_features = 8\n",
    "X_random = pd.DataFrame(\n",
    "    rng.randn(X.shape[0], n_random_features),\n",
    "    columns=[f\"random_{i:02d}\" for i in range(n_random_features)])\n",
    "X = pd.concat([X, X_random], axis=1)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection with information criteria\n",
    "\n",
    "Information criteria (IC) is a tool for model selection.\n",
    "ICs are mathematical formulas used to evaluate the quality of a statistical model and compare different models.\n",
    "\n",
    "They help to address the trade-off between model complexity and goodness of fit by penalizing overly complex models.\n",
    "Information criteria can be used for model selection, regularization, and feature selection.\n",
    "\n",
    "There are several commonly used information criteria:\n",
    "\n",
    "**Akaike Information Criterion (AIC)**:\n",
    "AIC measures the goodness of fit of the model while taking into account the number of parameters in the model. It penalizes models with more parameters, so it is useful in avoiding overfitting.\n",
    "\n",
    "$$AIC = 2k - 2\\log(L)$$\n",
    "\n",
    "**Bayesian Information Criterion (BIC)**:\n",
    "BIC is similar to AIC, but it places a stronger penalty on models with a large number of parameters, making it less likely to choose overly complex models.\n",
    "\n",
    "$$BIC = k\\log(n) - 2\\log(L)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "lasso_lars_ic = make_pipeline(StandardScaler(), LassoLarsIC(criterion=\"aic\")).fit(X, y)\n",
    "\n",
    "# Create a Pandas DataFrame to store the results\n",
    "# The DataFrame has one column for the alpha values and two columns for the AIC and BIC criteria\n",
    "results = pd.DataFrame({\"alphas\": lasso_lars_ic[-1].alphas_,\n",
    "                        \"AIC criterion\": lasso_lars_ic[-1].criterion_}).set_index(\"alphas\")\n",
    "\n",
    "# Get the alpha value that minimizes the AIC criterion\n",
    "alpha_aic = lasso_lars_ic[-1].alpha_\n",
    "\n",
    "# Get the coefficients of the Lasso model with the best alpha value\n",
    "coefs = lasso_lars_ic[-1].coef_\n",
    "\n",
    "# Print the features that have zero coefficients\n",
    "zero_coefs = np.where(coefs == 0)[0]\n",
    "if zero_coefs.size == 0:\n",
    "    print(\"No features have been set to zero by Lasso\")\n",
    "else:\n",
    "    print(\"The following features have been set to zero by Lasso under AIC:\")\n",
    "    print(zero_coefs)\n",
    "\n",
    "# Update the LassoLarsIC estimator to use the BIC criterion\n",
    "# This is done by setting the criterion parameter of the LassoLarsIC estimator to \"bic\"\n",
    "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\n",
    "\n",
    "# Add the BIC criterion to the DataFrame\n",
    "results[\"BIC criterion\"] = lasso_lars_ic[-1].criterion_\n",
    "\n",
    "# Get the alpha value that minimizes the BIC criterion\n",
    "alpha_bic = lasso_lars_ic[-1].alpha_\n",
    "\n",
    "# Get the coefficients of the Lasso model with the best alpha value\n",
    "coefs = lasso_lars_ic[-1].coef_\n",
    "\n",
    "# Print the features that have zero coefficients\n",
    "zero_coefs = np.where(coefs == 0)[0]\n",
    "if zero_coefs.size == 0:\n",
    "    print(\"No features have been set to zero by Lasso\")\n",
    "else:\n",
    "    print(\"The following features have been set to zero by Lasso under BIC:\")\n",
    "    print(zero_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ax = results.plot()\n",
    "ax.vlines(alpha_aic, results[\"AIC criterion\"].min(), results[\"AIC criterion\"].max(),\n",
    "    label=\"alpha: AIC estimate\", linestyles=\"--\", color=\"tab:blue\")\n",
    "ax.vlines(alpha_bic, results[\"BIC criterion\"].min(), results[\"BIC criterion\"].max(),\n",
    "    label=\"alpha: BIC estimate\", linestyle=\"--\", color=\"tab:orange\")\n",
    "\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(\"criterion\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend()\n",
    "ax.set_title(f\"Information-criterion for model selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection with cross validation\n",
    "\n",
    "Cross-validation is a technique used to evaluate the performance of a machine learning model and to tune its hyperparameters.\n",
    "It's called \"cross-validation\" because it involves dividing the dataset into multiple \"folds\" and training the model on different subsets of the data, while evaluating its performance on the remaining part of the data.\n",
    "\n",
    "The idea is to use a different subset of the data as the validation set in each iteration, so that the model is trained and evaluated on different portions of the data.\n",
    "\n",
    "There are several types of cross-validation, including $k$-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and others.\n",
    "The specific approach depends on the nature of the data and the goals of the analysis.\n",
    "\n",
    "The purpose of cross-validation is to get an estimate of the model's performance that is more robust and less prone to overfitting than evaluating the model on a single train/test split of the data.\n",
    "By training and evaluating the model on different parts of the data, we get a better sense of how well the model will generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation is Scikit-learn\n",
    "\n",
    "In Scikit-learn the Lasso estimator can be implemented with different solvers: coordinate descent and least angle regression.\n",
    "They differ in regard to their execution speed and sources of numerical errors.\n",
    "\n",
    "LassoCV and LassoLarsCV that respectively solve the problem with coordinate descent and least angle regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the LassoCV class from the linear_model module of the scikit-learn library\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create a pipeline that standardizes the features and applies LassoCV with 10-fold cross-validation\n",
    "model = make_pipeline(StandardScaler(), LassoCV(cv=10)).fit(X, y)\n",
    "\n",
    "# Set the y-axis limits of the plot\n",
    "ymin, ymax = 2300, 3800\n",
    "\n",
    "# Extract the LassoCV model from the pipeline\n",
    "lasso = model[-1]\n",
    "\n",
    "# Plot the MSE path of the LassoCV model for different values of alpha on a logarithmic x-axis\n",
    "plt.semilogx(lasso.alphas_, lasso.mse_path_, linestyle=\":\")\n",
    "\n",
    "# Plot the average MSE across the folds for each value of alpha as a black line\n",
    "plt.plot(lasso.alphas_, lasso.mse_path_.mean(axis=-1),\n",
    "    color=\"black\", label=\"Average across the folds\", linewidth=2)\n",
    "\n",
    "# Add a vertical line to show the estimated alpha value selected by cross-validation\n",
    "plt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha: CV estimate\")\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.legend()\n",
    "plt.title(f\"Mean square error on each fold: coordinate descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the LassoLarsCV class from the linear_model module of the scikit-learn library\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "\n",
    "# Create a pipeline that standardizes the features and applies LassoLarsCV with 10-fold cross-validation\n",
    "model = make_pipeline(StandardScaler(), LassoLarsCV(cv=20)).fit(X, y)\n",
    "\n",
    "# Extract the LassoLarsCV model from the pipeline\n",
    "lasso = model[-1]\n",
    "\n",
    "# Plot the MSE path of the LassoLarsCV model for different values of alpha on a logarithmic x-axis\n",
    "plt.semilogx(lasso.cv_alphas_, lasso.mse_path_, \":\")\n",
    "\n",
    "# Plot the average MSE across the folds for each value of alpha as a black line\n",
    "plt.semilogx(lasso.cv_alphas_, lasso.mse_path_.mean(axis=-1),\n",
    "    color=\"black\", label=\"Average across the folds\", linewidth=2)\n",
    "\n",
    "# Add a vertical line to show the estimated alpha value selected by cross-validation\n",
    "plt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha CV\")\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.legend()\n",
    "plt.title(f\"Mean square error on each fold: Lars\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
