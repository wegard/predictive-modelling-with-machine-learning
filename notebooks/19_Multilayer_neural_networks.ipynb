{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer neural networks\n",
    "\n",
    "## Lecture 10\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Display 10 random example images\n",
    "num_images = 10\n",
    "random_indices = np.random.choice(len(train_images), num_images)\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.imshow(train_images[idx], cmap='gray')\n",
    "    plt.title(train_labels[idx])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# LetÂ´ reshape the data\n",
    "\n",
    "# This line reshapes the training dataset x_train from its original shape of (60000, 28, 28) to a new shape of (60000, 784).\n",
    "# The original shape represents 60,000 images, each of size 28x28 pixels.\n",
    "# The new shape flattens each image into a one-dimensional array of 784 elements (28 * 28).\n",
    "# This is done because the neural network expects a one-dimensional input for each sample.\n",
    "x_train = x_train.reshape((60000, 28 * 28))\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "\n",
    "# This line first converts the data type of x_train to float32.\n",
    "# Then, it divides each element by 255.\n",
    "# The purpose of this operation is to normalize the pixel values, which originally range from 0 (black) to 255 (white), to a range between 0 and 1.\n",
    "# Normalizing the input data usually helps the neural network learn more effectively and converge faster.\n",
    "x_test = x_test.reshape((10000, 28 * 28))\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the output data\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert the labels to categorical one-hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We then create a multilayer neural network with two hidden layers.\n",
    "# The first hidden layer has 128 neurons and a ReLU activation function, while the second hidden layer has 64 neurons and a ReLU activation function.\n",
    "# The output layer has 10 neurons with a softmax activation function, corresponding to the 10 possible digit classes (0-9).\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation=\"relu\", input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive moment estimation (adam) optimizer\n",
    "\n",
    "Adam is an adaptive optimization algorithm that combines the ideas of two other optimization algorithms: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp).\n",
    "It computes adaptive learning rates for each weight based on the first moment (mean) and the second moment (variance) of the gradients.\n",
    "\n",
    "Adam maintains exponential moving averages of the gradients and the squared gradients, denoted by $m_t$ and $v_t$, respectively.\n",
    "The weight update in Adam can be described as:\n",
    "\n",
    "$$w_{t+1} = w_{t} - \\frac{\\nu \\cdot m_t}{\\sqrt(v_t) + \\epsilon}$$\n",
    "\n",
    "where $w_t$ represents the weights at time $t$, the learning rate, $\\nu$, is a hyperparameter that controls the initial step size of the updates, $m_t$ and $v_t$ are the exponential moving averages of the gradients and squared gradients, and $\\epsilon$ is a small constant to prevent division by zero.\n",
    "\n",
    "Adam automatically adjusts the learning rate for each weight during training, making it more robust and effective in handling complex optimization landscapes.\n",
    "It usually converges faster than SGD and is less sensitive to the initial learning rate.\n",
    "\n",
    "In practice, Adam is often the preferred choice due to its adaptive nature and faster convergence.\n",
    "However, it's essential to experiment with different optimizers and hyperparameters to find the best configuration for a specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
