{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized regressions\n",
    "\n",
    "## Lecture 3\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to linear methods\n",
    "\n",
    "Linear methods are a class of machine learning algorithms that are based on the idea of finding a linear relationship between the input features and the target variable.\n",
    "They are widely used for both regression and classification tasks and are particularly useful when the data is linearly separable.\n",
    "\n",
    "Linear regression is the most well-known linear method.\n",
    "It is a supervised learning algorithm that is used to predict a continuous target variable from a set of input features.\n",
    "In linear regression, the goal is to find the line of best fit that minimizes the distance between the predicted values and the actual values.\n",
    "\n",
    "Logistic regression is another widely used linear method.\n",
    "It is a classification algorithm that is used to predict a binary outcome (e.g. 0 or 1, Yes or No).\n",
    "Like linear regression, it is based on the idea of finding the line of best fit, but the line is used to divide the input space into two regions, one for each class.\n",
    "Logistic regression is often used as a baseline model in classification tasks because it is relatively simple and fast to train, we will look closer at logistic regression in lecture 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model might not work well in situations with many predictors. In the case with more predictors than observations it will not work at all ($p > n$).\n",
    "\n",
    "Here are som potential problems when the number of predictors is large:\n",
    "\n",
    "1. **Overfitting**: It may be able to fit the noise in the data as well as the underlying signal. This can lead to poor generalization performance when making predictions on new unseen data.\n",
    "2. **High variance**: More likely to have a high variance, meaning that the model's predictions will change a lot depending on the specific training data used.\n",
    "3. **Difficult to interpret**: Can be difficult to understand which parameters are most important for explaining the outcome, and which parameters are not important.\n",
    "4. **Hard to estimate the model**: It can be difficult to estimate the parameters accurately from a limited amount of data. This can lead to unreliable parameter estimates and poor model performance.\n",
    "5. **Multicollinearity**: It is more difficult to identify and deal with multicollinearity, a problem where predictors are correlated with each other. This can lead to instability in the parameter estimates, and make it difficult to identify the true relationship between the predictors and the outcome.\n",
    "6. **Computational complexity**: Computationally expensive to fit the model and make predictions with it, especially when the data set is large.\n",
    "\n",
    "Regularization techniques such as Ridge, Lasso and Elastic Net can be used to address these issues by shrinking the coefficients of less important predictors towards zero, thus reducing the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate som toy data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "\n",
    "# Generate toy data using make_regression\n",
    "#X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=5, noise=10)\n",
    "\n",
    "# Generate som toy data manually\n",
    "\n",
    "# The true vector of coefficients [5.5, 1.75, 1, 0, -2, 0, 0.6, 0, -9.6, 0]\n",
    "np.random.seed(15)\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "y = 5.5*X[:,0] + 1.75*X[:,1] + 1*X[:,2] - 2*X[:,4] + 0.6*X[:,6] - 9.6*X[:,8] + 5.0*np.random.randn(n_samples)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression (recap)\n",
    "\n",
    "Linear regression is a method for modeling the relationship between a dependent variable $y$ and one or more independent variables (or predictors) denoted by $X$.\n",
    "It assumes that the relationship between $y$ and $X$ can be represented by a linear equation.\n",
    "\n",
    "For multiple linear regression with $n$ predictor variables, the equation can be written as:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2x_2 + ... + \\beta_n x_n$$\n",
    "\n",
    "where $x_1, x_2, ..., x_n$ are the independent variables and $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients of the respective independent variables.\n",
    "\n",
    "The goal of linear regression is to find the best-fitting line that minimizes the residual sum of squares (RSS) between the observed values of $y$ and the predicted values of $y$, $\\hat{y}$.\n",
    "\n",
    "$$RSS = \\sum_i(y_i - \\hat{y}_i)^2 = \\sum_i\\left(y_i - \\beta_0 - \\beta_1x_{i1} - \\beta_2x_{i2} - ... - \\beta_n x_{in}\\right) = \\sum_i\\left(y_i - \\beta_0 - \\sum_j\\beta_jx_{ij}\\right)^2$$\n",
    "\n",
    "where $y_i$ is the $i$-th observed value of $y$ and $\\hat{y}_i$ is the corresponding predicted value of $y$.\n",
    "\n",
    "The coefficients $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ can be estimated using the method of least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized regression is a technique used to improve the generalization ability of a linear model by adding a regularization term to the model equation.\n",
    "The regularization term is a penalty that is applied to the model coefficients/weights and is controlled by a hyperparameter.\n",
    "\n",
    "The main idea behind regularized regression is to prevent overfitting, which is when a model performs well on the training data but poorly on unseen data.\n",
    "Overfitting can occur when the model has too many degrees of freedom and is able to fit the training data perfectly, but is not able to generalize to new data.\n",
    "\n",
    "There are several types of regularized regression, including ridge regression, lasso regression, and elastic net.\n",
    "Each type of regularized regression uses a different form of the regularization term, which results in different behavior and properties of the model.\n",
    "Regularization is achieved by adding a term to the cost function that penalizes the model for having large coefficients.\n",
    "This term is typically a function of the magnitude of the coefficients, such as the L1 norm (Lasso) or the L2 norm (Ridge).\n",
    "By adding this term, the optimization algorithm will balance the goal of fitting the training data well with the goal of having small coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "The penalty term, known as the \"ridge parameter\" $\\lambda$, controls the strength of the regularization.\n",
    "When $\\lambda$ is set to zero, Ridge Regression becomes the same as linear regression, and as $\\lambda$ increases, the coefficients of the model become smaller, which can help to reduce overfitting.\n",
    "\n",
    "In Ridge Regression, the cost function is the sum of the squared residuals (also known as the \"least squares\") and the \"L2 regularization term\".\n",
    "The L2 regularization term is the sum of the squares of the coefficients, multiplied by the ridge parameter.\n",
    "\n",
    "The cost function for Ridge Regression is defined as:\n",
    "\n",
    "$$J^{\\text{ridge}}(\\mathbf{\\beta}) = \\frac{1}{2n}\\sum_i\\left(y_i - \\beta_0 - \\sum_j\\beta_jx_{ij}\\right)^2 + \\lambda_r \\sum_j \\beta_j^2,$$\n",
    "\n",
    "where $\\mathbf{\\beta}$ is the vector of coefficients, $x_{ij}$ is the $j$th feature of the ith observation, $y_i$ is the target variable for the $i$th observation, $n$ is the number of observations and $\\lambda_r$ is the ridge parameter.\n",
    "\n",
    "The Ridge Regression algorithm finds the values of the coefficients that minimize the cost function.\n",
    "This is done by using optimization techniques such as gradient descent, or by solving the normal equations.\n",
    "As with the linear regression model, ridge regression has a closed form solution.\n",
    "Once the optimal values of the coefficients have been found, the Ridge Regression model can be used to make predictions on new data.\n",
    "\n",
    "Ridge Regression is helps with overfitting and is also useful in cases where the number of features is larger than the number of observations, as it can help to prevent the model from becoming over-specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.8)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Absolute Shrinkage and Selection Operator (LASSO) regression\n",
    "\n",
    "Lasso regression is also a type of linear regression that adds a different type of regularization term to the cost function.\n",
    "The regularization term is the absolute value of the sum of the coefficients, multiplied by the regularization parameter.\n",
    "The goal of lasso regression is to minimize the sum of the squared errors of the model, subject to the constraint that the sum of the absolute values of the coefficients is less than a fixed value.\n",
    "\n",
    "The cost function for Lasso Regression is defined as:\n",
    "\n",
    "$$J^{\\text{lasso}}(\\mathbf{\\beta}) = \\frac{1}{2n}\\sum_i\\left(y_i - \\beta_0 - \\sum_j\\beta_jx_{ij}\\right)^2 + \\lambda_l \\sum_j|\\beta_j|,$$\n",
    "\n",
    "where where $\\mathbf{\\beta}$ is the vector of coefficients, $x_{ij}$ is the $j$th feature of the ith observation, $y_i$ is the target variable for the ith observation, $n$ is the number of observations and $\\lambda_l$ is the lasso parameter.\n",
    "\n",
    "The lasso regression algorithm works by iteratively adjusting the coefficients of the model in such a way as to minimize the cost function.\n",
    "At each iteration, the algorithm looks at the gradient of the cost function with respect to each coefficient, and adjusts the coefficient in the opposite direction of the gradient, subject to the constraint that the sum of the absolute values of the coefficients is less than a fixed value.\n",
    "This process continues until the cost function is minimized, or until a maximum number of iterations is reached.\n",
    "\n",
    "The main advantage of lasso regression is that it can be used to automatically select a subset of features that are most important in predicting the target variable.\n",
    "Because the lasso algorithm shrinks the coefficients of less important features towards zero, it effectively eliminates those features from the model.\n",
    "This feature selection is especially useful when the number of features is large, as it can help to reduce overfitting and improve the interpretability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "Elastic Net regression combines the L1 regularization term of Lasso regression and the L2 regularization term of Ridge regression into one model.\n",
    "The cost function for Elastic Net regression is a combination of the mean squared error (MSE) and the L1 and L2 regularization terms.\n",
    "The L1 regularization term is the sum of the absolute values of the coefficients, and the L2 regularization term is the sum of the squares of the coefficients.\n",
    "\n",
    "The cost function for Elastic net regression is defined as:\n",
    "\n",
    "$$J^{\\text{elastic net}}(\\mathbf{\\beta}) = \\frac{1}{2n}\\sum_i\\left(y_i - \\beta_0 - \\sum_j\\beta_jx_{ij}\\right)^2 + \\lambda_{e} \\left( (1-\\alpha)\\sum_j \\beta_j^2 + \\alpha \\sum_j|\\beta_j|\\right),$$\n",
    "\n",
    "where where $\\mathbf{\\beta}$ is the vector of coefficients, $x_{ij}$ is the $j$th feature of the ith observation, $y_i$ is the target variable for the ith observation, $n$ is the number of observations.\n",
    "$\\lambda_e$ is the regularization parameter, which controls the strength of the regularization term.\n",
    "It is a scalar value that is chosen by the user.\n",
    "$\\alpha$ is the mixing parameter between L1 and L2 regularization. When $\\alpha=0$, it becomes a Ridge Regression and when $\\alpha=1$, it becomes Lasso Regression\n",
    "\n",
    "The goal of Elastic Net regression is to minimize the cost function, subject to the constraint that the sum of the absolute values of the coefficients and the sum of the squares of the coefficients are less than fixed values.\n",
    "\n",
    "The Elastic Net regression algorithm works by iteratively adjusting the coefficients of the model in such a way as to minimize the cost function in the same way as for lasso regression.\n",
    "\n",
    "The main advantage of Elastic Net regression is that it can handle correlated predictors better than Lasso regression, as Lasso tends to select only one of the correlated predictors, whereas Elastic Net will give non-zero coefficients to both.\n",
    "Additionally, Elastic Net is a good choice when the number of predictors is relatively large compared to the number of observations because it can help to reduce overfitting and improve the interpretability of the model.\n",
    "The parameter $\\alpha$ controls the balance between L1 and L2 regularization term.\n",
    "\n",
    "Note that in Scikit-learn the parameter `alpha` is the same as we here call $\\lambda$. What we call $\\alpha$ is called `l1_ratio` in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "en.fit(X_train, y_train)\n",
    "print(en.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Mean Squared Error of the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred_lm = lm.predict(X_test)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "y_pred_en = en.predict(X_test)\n",
    "\n",
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "\n",
    "print(\"Mean Squared Error (Linear model):\", mse_lm.round(4))\n",
    "print(\"Mean Squared Error (Ridge):\", mse_ridge.round(4))\n",
    "print(\"Mean Squared Error (Lasso):\", mse_lasso.round(4))\n",
    "print(\"Mean Squared Error (Elastic Net):\", mse_en.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the hyperparameters  (the $\\lambda$'s and $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARS (Least Angle Regression)\n",
    "\n",
    "LARS (Least Angle Regression) is an algorithm for fitting linear models to high-dimensional data, and is particularly useful for estimating Lasso models (L1 regularized linear regression).\n",
    "The LARS algorithm is similar to the Lasso algorithm, but it has the computational advantage of being much faster than traditional optimization methods for estimating Lasso models.\n",
    "\n",
    "The LARS algorithm works by iteratively adding one variable at a time to the model, where each variable is chosen based on the correlation with the response variable.\n",
    "The algorithm starts with an empty model and at each step, it finds the variable that has the highest correlation with the residuals of the current model.\n",
    "This variable is then added to the model and the coefficients are updated by moving along the L1 constraint boundary.\n",
    "\n",
    "The LARS algorithm stops when all the variables have been added to the model, or when the number of variables reaches a pre-specified maximum.\n",
    "The LARS algorithm also allows for the estimation of the Lasso path, which is a set of Lasso models for different values of the regularization parameter.\n",
    "\n",
    "The LARS algorithm has some important properties:\n",
    "\n",
    "1. As it is a forward stepwise method, it is computationally efficient.\n",
    "2. It is numerically stable, and it can handle highly correlated predictor variables.\n",
    "3. It is useful for variable selection as it can handle a large number of features, and it does not require a tuning parameter.\n",
    "\n",
    "Scikit-learn has a Lars class in `sklearn.linear_model` which implements LARS algorithm.\n",
    "\n",
    "It has several options to control the behavior of the algorithm, including the ability to specify the maximum number of variables to include in the model, and the ability to calculate the Lasso path.\n",
    "\n",
    "Let us calculate the Lasso path using the LARS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path\n",
    "\n",
    "_, _, coefs = lars_path(X_train, y_train, method=\"lasso\", verbose=True)\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(xx, coefs.T, lw=2)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle=\"dashed\")\n",
    "plt.xlabel(\"|coef| / max|coef|\")\n",
    "plt.ylabel(\"Coefficients\")\n",
    "plt.title(\"LASSO Path\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# What features survive\n",
    "import pandas as pd\n",
    "pd.DataFrame(coefs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
