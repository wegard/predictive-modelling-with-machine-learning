{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "## Lecture 6\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is commonly used in machine learning and statistics.\n",
    "The goal of PCA is to find the most important underlying structure in a dataset, which is often a lower-dimensional subspace.\n",
    "\n",
    "One way to mathematically describe PCA is through the following steps:\n",
    "\n",
    "1. Center the data by subtracting the mean from each feature\n",
    "2. Compute the covariance matrix of the centered data\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix\n",
    "4. Select the eigenvectors with the largest eigenvalues, these are the principal components\n",
    "5. Project the data onto the subspace spanned by the principal components\n",
    "\n",
    "The centered data is represented as $\\bar{X}$, the covariance matrix is represented as $\\Sigma$, the eigenvectors are represented as $V$ and the eigenvalues are represented as $\\Lambda$:\n",
    "\n",
    "$$\\bar{X} = X - mean(X)$$\n",
    "$$\\Sigma = (1/n) \\bar{X}'\\bar{X}$$\n",
    "$$ \\Lambda, V = eig(\\Sigma)$$\n",
    "\n",
    "where $X$ is the original data matrix, $mean(X)$ is the mean of all columns of $X$, $n$ is the number of observations in $X$. \n",
    "The $eig()$ function is used to calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "These principal components can be used to project the data onto a lower dimensional subspace, resulting in a new matrix called $\\hat{X}$.\n",
    "\n",
    "$$\\hat{X} = \\bar{X} V$$\n",
    "\n",
    "This new matrix $\\hat{X}$ will have the same number of rows as the original data but fewer columns and will contain the most important information of the original data.\n",
    "\n",
    "<p>\n",
    "\n",
    "### Alternative: Estimation using SVD\n",
    "\n",
    "Singular Value Decomposition (SVD) is another technique that can be used to estimate the principal components of a dataset. \n",
    "SVD is a factorization of the matrix, and it can be used to decompose a matrix into simpler, more interpretable components.\n",
    "\n",
    "The SVD decomposition of a matrix X can be written as:\n",
    "\n",
    "$$X = U \\Sigma V'$$\n",
    "\n",
    "Where $U$ and $V$ are orthonormal matrices and $\\Sigma$ is a diagonal matrix with non-negative real numbers on the diagonal, called the singular values.\n",
    "\n",
    "To estimate the principal components using SVD, we can follow these steps:\n",
    "\n",
    "1. Compute the SVD of the data matrix $X$: $X = U \\Sigma V'$\n",
    "2. The columns of $V$ are the right singular vectors, and they correspond to the principal components of the data\n",
    "3. The diagonal elements of the matrix $\\Sigma$ are the singular values, and they represent the amount of variance explained by each principal component\n",
    "\n",
    "Just like PCA, we can use the principal components obtained from SVD to project the data onto a lower dimensional subspace, resulting in a new matrix called $\\hat{X}$: $$\\hat{X} = XV$$\n",
    "\n",
    "Let's look at an example of how to use PCA to reduce the dimensionality of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeds dataset\n",
    "\n",
    "The \"seeds.csv\" dataset is commonly used to demonstrate principal component analysis (PCA) because it contains measurements of the geometric properties of kernels belonging to two different varieties of wheat.\n",
    "The dataset contains 7 attributes (features) and 140 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV data\n",
    "seeds = pd.read_csv('../data/seeds.csv')\n",
    "\n",
    "# Extract the features and apply PCA to reduce the dimensionality of the data set to 2 principal components\n",
    "X = seeds[['area', 'perimeter', 'compactness', \n",
    "           'length', 'width', 'asymmetry_coefficient', 'grove_length']]\n",
    "\n",
    "# Visualize the data set using a scatter plot of the first two features\n",
    "sns.scatterplot(data=seeds, x='area', y='perimeter',\n",
    "                hue='type', sizes=(20, 200))\n",
    "plt.title('Scatter plot of the first two features')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Perimeter')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the data set using a scatter plot of the last two features\n",
    "sns.scatterplot(data=seeds, x='asymmetry_coefficient', y='grove_length',\n",
    "                hue='type', sizes=(20, 200))\n",
    "plt.title('Scatter plot of the last two features')\n",
    "plt.xlabel('Asymmetry coefficient')\n",
    "plt.ylabel('Grove length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the PCA class from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "principal_components = pca.components_\n",
    "\n",
    "# Project the data onto the first two principal components \n",
    "# and create a data frame with the projected data\n",
    "data_projected = pd.DataFrame(pca.transform(X), \n",
    "                              columns=['first', 'second'])\n",
    "data_projected['label'] = seeds['type']\n",
    "\n",
    "# Visualize the data set using a scatter plot of the first two principal components\n",
    "sns.scatterplot(data=data_projected, x='first', y='second', \n",
    "                hue='label', sizes=(20, 200))\n",
    "plt.title('Scatter plot of the first two principal components')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the explained variance ratio\n",
    "\n",
    "print(f'The explained variance ratio is: {sum(pca.explained_variance_ratio_.round(2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms.\n",
    "As we have talked about earlier, standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
    "\n",
    "In PCA we are interested in the components that maximize the variance.\n",
    "If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the weight axis, if those features are not scaled.\n",
    "As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is not what we want.\n",
    "\n",
    "Reference: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The wine dataset\n",
    "\n",
    "The wine dataset is a multiclass classification dataset that contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\n",
    "The cultivars are identified as \"class_0\", \"class_1\", and \"class_2\" in the dataset.\n",
    "\n",
    "The wine dataset contains a total of 178 samples, each with 13 features.\n",
    "The features are the results of the chemical analysis, including the alcohol content, malic acid, ash, alcalinity of ash, magnesium, total phenols, flavonoids, nonflavanoid phenols, proanthocyanins, color intensity, hue, OD280/OD315 of diluted wines, and proline.\n",
    "\n",
    "The target variable in the wine dataset is the cultivar, which is a categorical variable with three possible values: \"class_0\", \"class_1\", and \"class_2\".\n",
    "The dataset is commonly used as a benchmark dataset for testing classification algorithms.\n",
    "\n",
    "Scikit-learn provides the wine dataset as part of its datasets module, and can be loaded using the load_wine() function from the datasets module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the sample data\n",
    "features, target = load_wine(return_X_y=True)\n",
    "\n",
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize PCA components with scaling\n",
    "\n",
    "The figures below show the 1st and 2nd components with the original data set and then with the scaled (standardized) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Make a train/test split using 30% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.30)\n",
    "\n",
    "# Fit to data and predict using pipelined GNB and PCA.\n",
    "unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n",
    "unscaled_clf.fit(X_train, y_train)\n",
    "pred_test = unscaled_clf.predict(X_test)\n",
    "\n",
    "# Fit to data and predict using pipelined scaling, GNB and PCA.\n",
    "std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\n",
    "std_clf.fit(X_train, y_train)\n",
    "pred_test_std = std_clf.predict(X_test)\n",
    "\n",
    "# Extract PCA from pipeline\n",
    "pca = unscaled_clf.named_steps['pca']\n",
    "pca_std = std_clf.named_steps['pca']\n",
    "\n",
    "# Scale and use PCA on X_train data for visualization.\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "X_train_std = pca_std.transform(scaler.transform(X_train))\n",
    "\n",
    "# Visualize original vs standarized dataset with PCA performed\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train[y_train == l, 0], X_train[y_train == l, 1],\n",
    "                color=c, label='class %s' % l, alpha=0.5, marker=m)\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std[y_train == l, 0], X_train_std[y_train == l, 1],\n",
    "                color=c, label='class %s' % l, alpha=0.5, marker=m)\n",
    "\n",
    "ax1.set_title('Training dataset after PCA')\n",
    "ax2.set_title('Standardized training dataset after PCA')\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check prediction performance with and without feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Show prediction accuracies in scaled and unscaled data.\n",
    "norm_pred_accuracy = metrics.accuracy_score(y_test, pred_test)\n",
    "std_pred_accuracy = metrics.accuracy_score(y_test, pred_test_std)\n",
    "print(f'Prediction accuracy for the normal test dataset with PCA: {norm_pred_accuracy}')\n",
    "print(f'Prediction accuracy for the standardized test dataset with PCA: {std_pred_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
