{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Lecture 4\n",
    "\n",
    "### GRA 4160: Predictive Modelling with Machine Learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen\n",
    "\n",
    "---\n",
    "### Overview\n",
    "- Basic theory of Logistic Regression\n",
    "- Binary classification interpretation\n",
    "- Multi-class extension\n",
    "- Titanic survival prediction example\n",
    "\n",
    "In this notebook, we will revisit the fundamental concept of **Logistic Regression**, a popular classification algorithm, and demonstrate how to apply it to a well-known dataset: **Titanic** passenger survival data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Fundamentals\n",
    "\n",
    "**Logistic Regression** is commonly used for binary classification (e.g., 0 or 1, \"yes\" or \"no\"). Despite its name, it is actually a **classification** technique rather than a regression technique.\n",
    "\n",
    "1. **Model Form**:\n",
    "    $$\n",
    "    \\hat{y} = \\frac{1}{1 + e^{-z}}\\quad \\text{where}\\quad z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p.\n",
    "    $$\n",
    "    \n",
    "    - $\\hat{y}$ represents the **predicted probability** of the positive class (labelled as 1).\n",
    "    - $z$ is the linear combination of input features and parameters (weights).\n",
    "\n",
    "2. **Interpreting $\\beta_j$**:\n",
    "    - Each coefficient $\\beta_j$ corresponds to the change in the **log-odds** of the outcome per unit change in the associated feature $x_j$.\n",
    "    - \"Log-odds\" means $\\ln\\left(\\frac{p}{1-p}\\right)$, where $p$ is the probability of the positive class.\n",
    "\n",
    "3. **Likelihood Maximization**:\n",
    "    - Logistic Regression is typically fit by **maximizing** the **likelihood** (or equivalently, **minimizing** the **negative log-likelihood**).\n",
    "    - Common optimization methods include **Gradient Descent**, **Newton-CG**, **L-BFGS**, etc.\n",
    "\n",
    "4. **Decision Threshold**:\n",
    "    - By default, $\\hat{y} > 0.5$ is classified as 1, else 0.\n",
    "    - This threshold can be adjusted based on problem context (e.g., wanting fewer false positives vs. fewer false negatives).\n",
    "\n",
    "5. **Multi-class Extensions**:\n",
    "    - **One-vs-All (OvA)**: Train a separate binary classifier per class.\n",
    "    - **Multinomial (Softmax) Regression**: Model all classes simultaneously with a softmax output.\n",
    "\n",
    "Next, we will explore an example using the Titanic dataset, aiming to predict passenger survival (1) or death (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Predicting Titanic Survival\n",
    "\n",
    "We will:\n",
    "- Load the dataset\n",
    "- Perform basic preprocessing\n",
    "- Train-test split\n",
    "- Fit a Logistic Regression model and evaluate accuracy\n",
    "\n",
    "### About the Titanic dataset\n",
    "Each row corresponds to a passenger, with columns such as:\n",
    "- **Survived** (1 = yes, 0 = no)\n",
    "- **Pclass** (Passenger class, 1 = upper, 2 = middle, 3 = lower)\n",
    "- **Sex** (male or female)\n",
    "- **Age** (Passenger age)\n",
    "- **SibSp** (Number of siblings/spouses aboard)\n",
    "- **Parch** (Number of parents/children aboard)\n",
    "- **Fare** (Ticket fare cost)\n",
    "\n",
    "Our goal: **Predict whether a passenger survived** (`Survived`) using their class, sex, age, fare, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress some warning messages\n",
    "\n",
    "# Load Titanic data\n",
    "df = pd.read_csv('../data/titanic/train.csv')\n",
    "\n",
    "# Let's take a quick look at the dataset structure\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Feature Engineering\n",
    "To simplify, we:\n",
    "- **Drop** rows with missing values (note: in practice, we might want a more sophisticated approach to missing data)\n",
    "- Convert `Sex` to a numeric variable (1 = male, 0 = female)\n",
    "- Select relevant features for our model\n",
    "\n",
    "Finally, we **split** the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (146, 6)\n",
      "Test set size:\t (37, 6)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert 'Sex' to numeric: male=1, female=0\n",
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Define our features (X) and target (y)\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=15\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\\t\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model\n",
    "We use **scikit-learn**'s `LogisticRegression` class with the `lbfgs` solver for optimization.\n",
    "\n",
    "After fitting, we:\n",
    "- **Predict** on the test set\n",
    "- Compare the predictions to the **true** labels using **accuracy**.\n",
    "  - Accuracy = Number of correct predictions / Total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.892\n",
      "Survival rate in test set: 0.541\n",
      "If we predicted no survivors always, Accuracy would be: 0.459\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", round(score, 3))\n",
    "print(\"Survival rate in test set:\", round(y_test.mean(), 3))\n",
    "\n",
    "# Observing baseline: if we guess 'no survival' for everyone, we'd get\n",
    "no_survival_acc = round((1 - y_test.mean()), 3)\n",
    "print(\"If we predicted no survivors always, Accuracy would be:\", no_survival_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Look at Coefficients\n",
    "In Logistic Regression, `coef_` reflects the effect of each feature on the **log-odds** of survival. A **positive** coefficient increases the log-odds (thus the probability of survival), while a **negative** coefficient lowers it.\n",
    "\n",
    "Remember:\n",
    "- `Sex` is coded as 1 for male, 0 for female, so a **negative** coefficient for `Sex` means being male **reduces** the log-odds of survival compared to female.\n",
    "- `Intercept` is stored in `intercept_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.32578</td>\n",
       "      <td>-2.136423</td>\n",
       "      <td>-0.021278</td>\n",
       "      <td>0.136447</td>\n",
       "      <td>-0.242426</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>3.254719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass       Sex       Age     SibSp     Parch      Fare  Intercept\n",
       "0 -0.32578 -2.136423 -0.021278  0.136447 -0.242426  0.002133   3.254719"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients and intercept\n",
    "coef_df = pd.DataFrame(\n",
    "    log_reg.coef_,\n",
    "    columns=X_train.columns\n",
    ")\n",
    "coef_df['Intercept'] = log_reg.intercept_\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: Confusion Matrix & Classification Report\n",
    "Besides accuracy, it's helpful to see **where** mistakes occur.\n",
    "\n",
    "- **Confusion Matrix**: Compares predicted vs. actual classes (TP, FP, TN, FN).\n",
    "- **Classification Report**: Provides precision, recall, and F1-score for each class.\n",
    "\n",
    "In many real-world problems, especially those with **imbalanced** data, these metrics can be more informative than simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[13  4]\n",
      " [ 0 20]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.87        17\n",
      "           1       0.83      1.00      0.91        20\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.92      0.88      0.89        37\n",
      "weighted avg       0.91      0.89      0.89        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. **Interpretation**: Based on the coefficients, which features appear most influential for survival?\n",
    "2. **Threshold Adjustment**: By default, predictions are 1 if $\\hat{y} > 0.5$. What if we change this threshold (e.g., 0.3 or 0.7)? How does that affect confusion matrix/accuracy?\n",
    "3. **Feature Engineering**: Are there other variables you could create (e.g., traveling alone vs. with family) that might improve accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting Probability on Synthetic Data\n",
    "Below is a small demonstration to show how **predict_proba** returns probabilities of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities for the new point:\n",
      "[[0.00632882 0.99367118]]\n",
      "Predicted class: [1]\n"
     ]
    }
   ],
   "source": [
    "# Example for illustrate predict_proba\n",
    "X_synth = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])\n",
    "y_synth = np.array([0, 0, 1, 1])\n",
    "\n",
    "clf_synth = LogisticRegression(solver='lbfgs').fit(X_synth, y_synth)\n",
    "\n",
    "# Let's create a new data point\n",
    "x_new = np.array([[5, 10]])\n",
    "probabilities = clf_synth.predict_proba(x_new)\n",
    "\n",
    "print(\"Predicted probabilities for the new point:\")\n",
    "print(probabilities)\n",
    "print(\"Predicted class:\", clf_synth.predict(x_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. How do these probabilities correspond to the logistic function (sigmoid) we covered theoretically?\n",
    "2. Compare `predict_proba(x)` with `decision_function(x)` in scikit-learn. What does `decision_function` return?\n",
    "3. Change the input to `[10, 20]`. How do you expect the probabilities to shift?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Logistic Regression provides **probabilistic** outputs for binary classification, mapping linear combinations of features to probabilities via the **sigmoid** function.\n",
    "- **Coefficients** represent **log-odds** contributions of each feature.\n",
    "- Evaluation metrics like **accuracy**, **confusion matrix**, **precision**, **recall**, and **F1-score** offer a more complete picture than a single metric.\n",
    "- Real-world performance may depend on data quality, balanced vs. imbalanced classes, and thoughtful **feature engineering**.\n",
    "\n",
    "With these ideas in mind, you should now be more comfortable using Logistic Regression on datasets like Titanic and beyond!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
