{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Lecture 4\n",
    "\n",
    "### GRA 4160: Predictive Modelling with Machine Learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen\n",
    "\n",
    "---\n",
    "### Overview\n",
    "- Basic theory of Logistic Regression\n",
    "- Binary classification interpretation\n",
    "- Multi-class extension\n",
    "- Titanic survival prediction example\n",
    "\n",
    "In this notebook, we will revisit the fundamental concept of **Logistic Regression**, a popular classification algorithm, and demonstrate how to apply it to a well-known dataset: **Titanic** passenger survival data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Fundamentals\n",
    "\n",
    "**Logistic Regression** is commonly used for binary classification (e.g., 0 or 1, \"yes\" or \"no\"). Despite its name, it is actually a **classification** technique rather than a regression technique.\n",
    "\n",
    "1. **Model Form**:\n",
    "    $$\n",
    "    \\hat{y} = \\frac{1}{1 + e^{-z}}\\quad \\text{where}\\quad z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p.\n",
    "    $$\n",
    "    \n",
    "    - $\\hat{y}$ represents the **predicted probability** of the positive class (labelled as 1).\n",
    "    - $z$ is the linear combination of input features and parameters (weights).\n",
    "\n",
    "2. **Interpreting $\\beta_j$**:\n",
    "    - Each coefficient $\\beta_j$ corresponds to the change in the **log-odds** of the outcome per unit change in the associated feature $x_j$.\n",
    "    - \"Log-odds\" means $\\ln\\left(\\frac{p}{1-p}\\right)$, where $p$ is the probability of the positive class.\n",
    "\n",
    "3. **Likelihood Maximization**:\n",
    "    - Logistic Regression is typically fit by **maximizing** the **likelihood** (or equivalently, **minimizing** the **negative log-likelihood**).\n",
    "    - Common optimization methods include **Gradient Descent**, **Newton-CG**, **L-BFGS**, etc.\n",
    "\n",
    "4. **Decision Threshold**:\n",
    "    - By default, $\\hat{y} > 0.5$ is classified as 1, else 0.\n",
    "    - This threshold can be adjusted based on problem context (e.g., wanting fewer false positives vs. fewer false negatives).\n",
    "\n",
    "5. **Multi-class Extensions**:\n",
    "    - **One-vs-All (OvA)**: Train a separate binary classifier per class.\n",
    "    - **Multinomial (Softmax) Regression**: Model all classes simultaneously with a softmax output.\n",
    "\n",
    "Next, we will explore an example using the Titanic dataset, aiming to predict passenger survival (1) or death (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Predicting Titanic Survival\n",
    "\n",
    "We will:\n",
    "- Load the dataset\n",
    "- Perform basic preprocessing\n",
    "- Train-test split\n",
    "- Fit a Logistic Regression model and evaluate accuracy\n",
    "\n",
    "### About the Titanic dataset\n",
    "Each row corresponds to a passenger, with columns such as:\n",
    "- **Survived** (1 = yes, 0 = no)\n",
    "- **Pclass** (Passenger class, 1 = upper, 2 = middle, 3 = lower)\n",
    "- **Sex** (male or female)\n",
    "- **Age** (Passenger age)\n",
    "- **SibSp** (Number of siblings/spouses aboard)\n",
    "- **Parch** (Number of parents/children aboard)\n",
    "- **Fare** (Ticket fare cost)\n",
    "\n",
    "Our goal: **Predict whether a passenger survived** (`Survived`) using their class, sex, age, fare, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress some warning messages\n",
    "\n",
    "# Load Titanic data\n",
    "df = pd.read_csv('../data/titanic/train.csv')\n",
    "\n",
    "# Let's take a quick look at the dataset structure\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Feature Engineering\n",
    "To simplify, we:\n",
    "- **Drop** rows with missing values (note: in practice, we might want a more sophisticated approach to missing data)\n",
    "- Convert `Sex` to a numeric variable (1 = male, 0 = female)\n",
    "- Select relevant features for our model\n",
    "\n",
    "Finally, we **split** the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert 'Sex' to numeric: male=1, female=0\n",
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Define our features (X) and target (y)\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=15\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\\t\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model\n",
    "We use **scikit-learn**'s `LogisticRegression` class with the `lbfgs` solver for optimization.\n",
    "\n",
    "After fitting, we:\n",
    "- **Predict** on the test set\n",
    "- Compare the predictions to the **true** labels using **accuracy**.\n",
    "  - Accuracy = Number of correct predictions / Total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", round(score, 3))\n",
    "print(\"Survival rate in test set:\", round(y_test.mean(), 3))\n",
    "\n",
    "# Observing baseline: if we guess 'no survival' for everyone, we'd get\n",
    "no_survival_acc = round((1 - y_test.mean()), 3)\n",
    "print(\"If we predicted no survivors always, Accuracy would be:\", no_survival_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Look at Coefficients\n",
    "In Logistic Regression, `coef_` reflects the effect of each feature on the **log-odds** of survival. A **positive** coefficient increases the log-odds (thus the probability of survival), while a **negative** coefficient lowers it.\n",
    "\n",
    "Remember:\n",
    "- `Sex` is coded as 1 for male, 0 for female, so a **negative** coefficient for `Sex` means being male **reduces** the log-odds of survival compared to female.\n",
    "- `Intercept` is stored in `intercept_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Coefficients and intercept\n",
    "coef_df = pd.DataFrame(\n",
    "    log_reg.coef_,\n",
    "    columns=X_train.columns\n",
    ")\n",
    "coef_df['Intercept'] = log_reg.intercept_\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: Confusion Matrix & Classification Report\n",
    "Besides accuracy, it's helpful to see **where** mistakes occur.\n",
    "\n",
    "- **Confusion Matrix**: Compares predicted vs. actual classes (TP, FP, TN, FN).\n",
    "- **Classification Report**: Provides precision, recall, and F1-score for each class.\n",
    "\n",
    "In many real-world problems, especially those with **imbalanced** data, these metrics can be more informative than simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. **Interpretation**: Based on the coefficients, which features appear most influential for survival?\n",
    "2. **Threshold Adjustment**: By default, predictions are 1 if $\\hat{y} > 0.5$. What if we change this threshold (e.g., 0.3 or 0.7)? How does that affect confusion matrix/accuracy?\n",
    "3. **Feature Engineering**: Are there other variables you could create (e.g., traveling alone vs. with family) that might improve accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting Probability on Synthetic Data\n",
    "Below is a small demonstration to show how **predict_proba** returns probabilities of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example for illustrate predict_proba\n",
    "X_synth = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])\n",
    "y_synth = np.array([0, 0, 1, 1])\n",
    "\n",
    "clf_synth = LogisticRegression(solver='lbfgs').fit(X_synth, y_synth)\n",
    "\n",
    "# Let's create a new data point\n",
    "x_new = np.array([[5, 10]])\n",
    "probabilities = clf_synth.predict_proba(x_new)\n",
    "\n",
    "print(\"Predicted probabilities for the new point:\")\n",
    "print(probabilities)\n",
    "print(\"Predicted class:\", clf_synth.predict(x_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. How do these probabilities correspond to the logistic function (sigmoid) we covered theoretically?\n",
    "2. Compare `predict_proba(x)` with `decision_function(x)` in scikit-learn. What does `decision_function` return?\n",
    "3. Change the input to `[10, 20]`. How do you expect the probabilities to shift?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Logistic Regression provides **probabilistic** outputs for binary classification, mapping linear combinations of features to probabilities via the **sigmoid** function.\n",
    "- **Coefficients** represent **log-odds** contributions of each feature.\n",
    "- Evaluation metrics like **accuracy**, **confusion matrix**, **precision**, **recall**, and **F1-score** offer a more complete picture than a single metric.\n",
    "- Real-world performance may depend on data quality, balanced vs. imbalanced classes, and thoughtful **feature engineering**.\n",
    "\n",
    "With these ideas in mind, you should now be more comfortable using Logistic Regression on datasets like Titanic and beyond!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
