{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests classifier\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "from scipy.stats import mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a random forest from the `DecisionTreeClassifier` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>83.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146.5208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch      Fare\n",
       "230       1    0  35.0      1      0   83.4750\n",
       "724       1    1  27.0      1      0   53.1000\n",
       "257       1    0  30.0      0      0   86.5000\n",
       "434       1    1  50.0      1      0   55.9000\n",
       "195       1    0  58.0      0      0  146.5208"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a pandas dataframe\n",
    "df = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "df = df.dropna()\n",
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Print the first 5 rows of the training data\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to generate a random subset of features\n",
    "def random_subset(n_features):\n",
    "\n",
    "    # Determine the number of features to consider at each split\n",
    "    k = int(np.sqrt(n_features))\n",
    "\n",
    "    # Select a random subset of k features without replacement\n",
    "    features = np.random.choice(n_features, size=k, replace=False)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to train a decision tree on a bootstrapped sample of the data\n",
    "def train_tree(X_train, y_train, n_features):\n",
    "\n",
    "    # Create a bootstrapped sample of the data\n",
    "    n_samples = X_train.shape[0]\n",
    "    sample_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    X_boot = X_train.iloc[sample_indices]\n",
    "    y_boot = y_train.iloc[sample_indices]\n",
    "\n",
    "    # Select a random subset of features\n",
    "    features = random_subset(n_features)\n",
    "    X_boot_subset = X_boot.iloc[:, features]\n",
    "\n",
    "    # Train a decision tree on the bootstrapped sample\n",
    "    dt = DecisionTreeClassifier(max_features=None, random_state=1)\n",
    "    dt.fit(X_boot_subset, y_boot)\n",
    "    return dt, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to predict the class labels for a new data point\n",
    "def predict(X, trees):\n",
    "\n",
    "    # Predict the class label for each tree and aggregate the predictions\n",
    "    y_pred = np.zeros((X.shape[0], len(trees)))\n",
    "    for i, tree in enumerate(trees):\n",
    "        features = tree[1]\n",
    "        X_subset = X.iloc[:, features]\n",
    "        y_pred[:, i] = tree[0].predict(X_subset)\n",
    "\n",
    "    # Convert the predictions to integer type\n",
    "    y_pred = y_pred.astype(int)\n",
    "    y_pred_agg = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=y_pred)\n",
    "    return y_pred_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train multiple decision trees\n",
    "n_trees = 100\n",
    "max_depth = 3\n",
    "trees = []\n",
    "n_features = X_train.shape[1]\n",
    "for i in range(n_trees):\n",
    "    dt, features = train_tree(X_train, y_train, n_features)\n",
    "    trees.append((dt, features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "\n",
    "y_pred = predict(X_test, trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7608695652173914\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.9032258064516129\n",
      "F1: 0.835820895522388\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the random forest (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the precision of the random forest (TP /(TP+ FP))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate the recall of the random forest (TP /(TP+ FN))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate the F1 score of the random forest (2 * precision * recall / (precision + recall))\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate in test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f'Survival rate in test set: {y_test.sum()/len(y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier object\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=30)\n",
    "\n",
    "# Train the random forest classifier on the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7608695652173914\n",
      "Precision: 0.7631578947368421\n",
      "Recall: 0.9354838709677419\n",
      "F1: 0.8405797101449275\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the random forest (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the precision of the random forest (TP /(TP+ FP))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate the recall of the random forest (TP /(TP+ FN))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate the F1 score of the random forest (2 * precision * recall / (precision + recall))\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Building ExtraTrees (Extremely Randomized Trees)\n",
    "\n",
    "**Selecting a Subset of Features:** At each node in the tree, both Random Forest and Extra Trees algorithms start by selecting a random subset of the features (or predictors).\n",
    "\n",
    "**Determining the Split Point:**\n",
    "\n",
    "- *Random Forest:* Once the subset of features is selected, the Random Forest algorithm will search for the best possible split point among these features. This involves finding the value that best separates the data according to the target variable, often using a criterion like Gini impurity or entropy in classification tasks. This process is somewhat similar to what a standard decision tree does, but limited to a subset of features.\n",
    "\n",
    "- *Extra Trees:* In contrast, the Extra Trees algorithm introduces more randomness. After selecting a subset of features, instead of searching for the most optimal split based on some criterion, it randomly selects a split point for each feature. Then, among these randomly generated splits, it chooses one to split the node. This means that the algorithm does not necessarily choose the best split from a statistical perspective, but rather a random one.\n",
    "\n",
    "**Impact of Random Splits:**\n",
    "This increased randomness in choosing splits can lead to more diversified trees within the ensemble, as it reduces the likelihood of creating similar trees even if they are based on the same training data.\n",
    "\n",
    "As a result, the individual trees in an Extra Trees ensemble can have higher bias compared to those in a Random Forest, but when combined, the ensemble as a whole often has lower variance. This is because the random splits lead to less correlated trees, which is beneficial in an ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas dataframe\n",
    "df = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "df = df.dropna()\n",
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7391304347826086\n",
      "Precision: 0.7878787878787878\n",
      "Recall: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "# Create an Extra Trees classifier object\n",
    "etc = ExtraTreesClassifier(n_estimators=100, max_depth=3, random_state=1)\n",
    "\n",
    "# Train the Extra Trees classifier on the training data\n",
    "etc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = etc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate in test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f'Survival rate in test set: {y_test.sum()/len(y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you build a Extra Trees classifier using only the DecisionTreeClassifier class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRandomSplitTree(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Check that y has acceptable targets\n",
    "        check_classification_targets(y)\n",
    "\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        self.tree_ = self._grow_tree(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        # Stopping criteria: if all targets are the same or if maximum depth is reached\n",
    "        if len(set(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "    \n",
    "        # Attempt to split until valid split is found or decide it's a leaf node\n",
    "        for _ in range(n_features):\n",
    "            feature_idx = np.random.randint(0, n_features)\n",
    "            unique_values = np.unique(X[:, feature_idx])\n",
    "\n",
    "            # If there's less than 2 unique values, can't split on this feature\n",
    "            if unique_values.size < 2:\n",
    "                continue\n",
    "\n",
    "            split_value = np.random.uniform(X[:, feature_idx].min(), X[:, feature_idx].max())\n",
    "\n",
    "            left_idx = X[:, feature_idx] < split_value\n",
    "            right_idx = ~left_idx\n",
    "\n",
    "            # Check if the split actually divides the dataset\n",
    "            if np.any(left_idx) and np.any(right_idx):\n",
    "                left_child = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "                right_child = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "                return (feature_idx, split_value, left_child, right_child)\n",
    "\n",
    "        # If no valid split found, return the most common target as leaf node\n",
    "        return np.argmax(np.bincount(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        predictions = [self._predict_one(x, self.tree_) for x in X]\n",
    "        return self.classes_[np.array(predictions)]\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        # If we have a leaf node\n",
    "        if not isinstance(node, tuple):\n",
    "            return node\n",
    "\n",
    "        # Decide whether to follow left or right child\n",
    "        feature_idx, split_value, left_child, right_child = node\n",
    "        if x[feature_idx] < split_value:\n",
    "            return self._predict_one(x, left_child)\n",
    "        else:\n",
    "            return self._predict_one(x, right_child)\n",
    "\n",
    "# Now we update the SimpleExtraTreesClassifier to use this new tree\n",
    "class SimpleExtraTreesClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = SimpleRandomSplitTree(max_depth=self.max_depth)\n",
    "\n",
    "            # Randomly select features\n",
    "            if self.max_features == 'sqrt':\n",
    "                size = int(np.sqrt(n_features))\n",
    "            elif self.max_features == 'log2':\n",
    "                size = int(np.log2(n_features))\n",
    "            else:\n",
    "                size = n_features\n",
    "\n",
    "            features_idx = np.random.choice(range(n_features), size=size, replace=False)\n",
    "            X_subset = X.iloc[:, features_idx]\n",
    "\n",
    "            # Train the tree\n",
    "            tree.fit(X_subset, y)\n",
    "            self.trees.append((tree, features_idx))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((self.n_estimators, len(X)), dtype=np.int64)\n",
    "        for i, (tree, features_idx) in enumerate(self.trees):\n",
    "            X_subset = X.iloc[:, features_idx]\n",
    "            predictions[i] = tree.predict(X_subset)\n",
    "\n",
    "        # Take mode along axis 0 (across all trees)\n",
    "        # Returns (array([predictions]), array([counts]))\n",
    "        final_predictions = mode(predictions, axis=0)\n",
    "        # Return just the predictions array\n",
    "        return final_predictions.mode.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "clf = SimpleExtraTreesClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6739130434782609\n",
      "Precision: 0.6818181818181818\n",
      "Recall: 0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gra4160",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
