{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 1. Load and Preprocess Data\n",
    "# ----------------------------\n",
    "\n",
    "# Load the Iris dataset from scikit-learn.\n",
    "# The dataset has 150 samples, each with 4 features.\n",
    "# There are 3 classes (0, 1, 2) that we want to classify.\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data      # shape: (150, 4)\n",
    "y = iris.target    # shape: (150,)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Standardize features (zero mean, unit variance) to improve training performance.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors.\n",
    "# We use torch.float32 for input features and torch.long for labels.\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Optionally, split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. Initialize Network Parameters\n",
    "# ----------------------------\n",
    "\n",
    "# Define the network architecture.\n",
    "input_size = 4     # Four features from the iris dataset.\n",
    "hidden_size = 8   # Number of neurons in the hidden layer (an arbitrary choice).\n",
    "output_size = 3    # Three output classes.\n",
    "\n",
    "# Manually initialize weights and biases.\n",
    "# We use basic tensors with requires_grad=True so that PyTorch tracks operations for automatic differentiation.\n",
    "W1 = torch.randn(input_size, hidden_size, dtype=torch.float32, requires_grad=True)\n",
    "b1 = torch.randn(hidden_size, dtype=torch.float32, requires_grad=True)\n",
    "W2 = torch.randn(hidden_size, output_size, dtype=torch.float32, requires_grad=True)\n",
    "b2 = torch.randn(output_size, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Define the Forward Pass\n",
    "# ----------------------------\n",
    "def forward(x):\n",
    "    \"\"\"\n",
    "    Perform a forward pass through the network.\n",
    "    \n",
    "    A forward pass means taking the input data and computing the output of the network.\n",
    "    Here, we first compute a linear transformation using W1 and b1, apply a non-linear ReLU activation,\n",
    "    then compute a second linear transformation using W2 and b2 to get the final raw output scores (logits).\n",
    "    \"\"\"\n",
    "    # First layer: linear transformation (matrix multiplication) followed by bias addition.\n",
    "    z1 = torch.mm(x, W1) + b1  # Shape: (n_samples, hidden_size)\n",
    "    \n",
    "    # ReLU activation: sets negative values to zero.\n",
    "    a1 = z1.clamp(min=0)\n",
    "    \n",
    "    # Second layer: another linear transformation producing logits.\n",
    "    logits = torch.mm(a1, W2) + b2  # Shape: (n_samples, output_size)\n",
    "    return logits\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute the softmax of the logits to obtain class probabilities.\n",
    "    \n",
    "    Softmax converts raw scores into probabilities that sum to 1.\n",
    "    We subtract the max for numerical stability.\n",
    "    \"\"\"\n",
    "    exp_logits = torch.exp(logits - torch.max(logits, dim=1, keepdim=True)[0])\n",
    "    probs = exp_logits / torch.sum(exp_logits, dim=1, keepdim=True)\n",
    "    return probs\n",
    "\n",
    "def cross_entropy_loss(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss between the predicted logits and the true labels.\n",
    "    \n",
    "    The cross-entropy loss measures the difference between the predicted probability distribution (obtained by softmax)\n",
    "    and the true distribution (which assigns a probability of 1 to the correct class). Lower loss indicates better predictions.\n",
    "    \"\"\"\n",
    "    probs = softmax(logits)\n",
    "    n_samples = logits.shape[0]\n",
    "    # For each sample, select the probability corresponding to the true label and take the negative log.\n",
    "    correct_logprobs = -torch.log(probs[range(n_samples), y_true])\n",
    "    loss = torch.sum(correct_logprobs) / n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Training the Network\n",
    "# ----------------------------\n",
    "\n",
    "# Hyperparameters for training.\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop.\n",
    "for epoch in range(num_epochs):\n",
    "    # ----- Forward Pass -----\n",
    "    # Compute the raw predictions (logits) for the current batch.\n",
    "    logits = forward(X_train)\n",
    "    \n",
    "    # Compute the loss between the predictions and the true labels.\n",
    "    loss = cross_entropy_loss(logits, y_train)\n",
    "    \n",
    "    # ----- Backward Pass -----\n",
    "    # A backward pass computes the gradients of the loss with respect to each parameter.\n",
    "    # It applies the chain rule to propagate the error backwards through the network.\n",
    "    loss.backward()\n",
    "    \n",
    "    # ----- Update Parameters -----\n",
    "    # We update each parameter using gradient descent: new_param = old_param - learning_rate * gradient\n",
    "    with torch.no_grad():\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "    \n",
    "    # Zero the gradients after updating so that they do not accumulate.\n",
    "    W1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    W2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "    \n",
    "    # Print the loss every 50 epochs for monitoring.\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Evaluate the Model\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    # Compute predictions on the test set.\n",
    "    test_logits = forward(X_test)\n",
    "    test_probs = softmax(test_logits)\n",
    "    # Choose the class with the highest probability.\n",
    "    predictions = torch.argmax(test_probs, dim=1)\n",
    "    \n",
    "    # Calculate accuracy.\n",
    "    accuracy = (predictions == y_test).float().mean()\n",
    "    print(f\"\\nTest Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
