{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing ensemble methods\n",
    "\n",
    "## Lecture 8\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble methods are a type of machine learning technique that combine the predictions of multiple models to make more accurate predictions than any individual model could. These methods are particularly useful in situations where the individual models have high variance or make strong, complex predictions. There are several types of ensemble methods, including boosting, bagging, and bootstrapped ensembles.\n",
    "\n",
    "One popular type of ensemble method is boosting, in which a series of weak models are trained sequentially, with each model attempting to correct the mistakes of the previous model. The final prediction is made by combining the predictions of all the models in the ensemble. Boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "Another type of ensemble method is bagging, in which a group of models are trained independently on different random subsets of the training data. The final prediction is made by averaging the predictions of all the models in the ensemble. Bagging algorithms include Random Forests and Extra Trees. Ensemble methods have been successful in a wide range of applications, including image classification and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "\n",
    "Combines multiple classifiers and uses a voting scheme to make predictions. The voting scheme can be either **hard** or **soft**, depending on how the final prediction is made.\n",
    "\n",
    "In a hard voting scheme, the final prediction is the mode of the predictions of the individual classifiers.\n",
    "In other words, each classifier casts a \"vote\" for its predicted class, and the class that receives the most votes is chosen as the final prediction.\n",
    "This is equivalent to a simple majority vote.\n",
    "\n",
    "In a soft voting scheme, the final prediction is the class with the highest probability of being predicted by the individual classifiers.\n",
    "In other words, each classifier produces a set of probabilities for each class, and the probabilities are averaged across all the classifiers.\n",
    "The class with the highest average probability is chosen as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset and split it into training and testing sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "# Define the base classifiers\n",
    "clf1 = LogisticRegression(random_state=10, solver='lbfgs', max_iter=1000)\n",
    "clf2 = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the VotingClassifier with hard voting\n",
    "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n",
    "\n",
    "# Train the LogisticRegression and RandomForestClassifier\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the LogisticRegression and RandomForestClassifier on the testing set\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "# Train the VotingClassifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the VotingClassifier on the testing set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy logistic regression: {accuracy1:.3f}')\n",
    "print(f'Accuracy random forest: {accuracy2:.3f}')\n",
    "print(f'Accuracy voting classifier: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week and strong learners\n",
    "\n",
    "A weak learner is a model that performs only slightly better than random guessing.\n",
    "For example, a decision tree with only one split or a linear regression model with a low degree polynomial can be considered weak learners.\n",
    "Although weak learners may not perform well individually, they can be combined in various ways to create a strong learner.\n",
    "\n",
    "A strong learner, on the other hand, is a model that can make accurate predictions on a given task with high confidence.\n",
    "A strong learner can be created by combining multiple weak learners using ensemble methods such as boosting, bagging, and stacking.\n",
    "\n",
    "In boosting, weak learners are trained sequentially, with each subsequent learner focused on the samples that the previous learner got wrong.\n",
    "By doing so, boosting can increase the accuracy of the model and create a strong learner from a collection of weak learners.\n",
    "Examples of boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "In bagging, weak learners are trained independently on different subsets of the data, and their predictions are aggregated using a voting scheme or an average.\n",
    "Bagging can reduce the variance of the model and create a strong learner from a collection of unstable weak learners.\n",
    "Examples of bagging algorithms include Bagging classifier (today) Random Forest and Extra Trees (will be covered next lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Replace the `DecisionTreeClassifier` with another classifier (like a Support Vector Machine or a Random forest) or add an additional classifier to the ensemble. Observe how this change impacts the overall accuracy of the VotingClassifier.\n",
    "\n",
    "2. Change the voting scheme from `hard` to `soft` and observe how this change impacts the overall accuracy of the VotingClassifier.\n",
    "\n",
    "3. Use additional performance metrics (like precision, recall, F1-score, or confusion matrix) to evaluate the LogisticRegression, RandomForestClassifier, and VotingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
