{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent\n",
    "\n",
    "## Lecture 9\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "A batch refers to a subset of the dataset used to update the model's parameters during training.\n",
    "The batch size is the number of data points in a batch.\n",
    "\n",
    "There are different gradient descent algorithms based on batch size:\n",
    "\n",
    "1. Batch Gradient Descent: The entire dataset is used for each parameter update. This can lead to more accurate updates but is computationally expensive, especially for large datasets.\n",
    "2. Stochastic Gradient Descent (SGD): A single data point is used for each parameter update. This is computationally efficient but can result in noisy and less accurate updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs\n",
    "\n",
    "An epoch is a single iteration through the entire dataset during training of a model.\n",
    "During an epoch, the model processes each data point or batch of data points and updates its parameters based on the calculated gradients.\n",
    "The purpose of multiple epochs is to improve the model's performance iteratively by refining its parameters on the same dataset.\n",
    "\n",
    "E.g, if you have a dataset of 1,000 data points, and you use a batch size of 100, each epoch would consist of 10 iterations (batches) through the dataset.\n",
    "If you run the training for 50 epochs, the model would have gone through the dataset 50 times, updating its parameters on each pass.\n",
    "Typically, the model's performance improves with more epochs, up to a certain point, after which it may start to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "The learning rate is a hyperparameter that determines the step size for updating the model's parameters during training.\n",
    "It controls how much the model's parameters are adjusted in response to the calculated gradients for each update.\n",
    "\n",
    "A smaller learning rate leads to smaller steps, which can result in a slower convergence towards the optimal solution, but it may provide more accurate parameter updates.\n",
    "A larger learning rate, leads to bigger steps, which can speed up the convergence process but may overshoot the optimal solution or cause the model to oscillate between different solutions.\n",
    "\n",
    "Choosing an appropriate learning rate is crucial for the successful training of a machine learning model.\n",
    "If the learning rate is too small, the model might take a long time to converge or get stuck in a suboptimal solution.\n",
    "If the learning rate is too large, the model might fail to converge or diverge altogether.\n",
    "Often, the learning rate is chosen through experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate a linear regression model with Stochastic Gradient Descent\n",
    "\n",
    "This example below generates a synthetic dataset and applies Stochastic Gradient Descent to find the best-fit line using linear regression.\n",
    "The dataset contains 100 data points, and the model is trained for 50 epochs with a learning rate of 0.1.\n",
    "The code calculates the gradients and updates the parameters $m$ and $b$ accordingly.\n",
    "The final learned line is plotted along with the original data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(10)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 4 + 3.8 * x + np.random.randn(100, 1)\n",
    "\n",
    "# Define the cost function (Mean Squared Error)\n",
    "def cost_function(y_true, y_pred):\n",
    "    # Calculate the mean squared error between the true values and predicted values\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "# Define the gradient function\n",
    "def gradients(x, y, y_pred):\n",
    "    n = len(y)\n",
    "    # Calculate the gradients of the cost function with respect to m and b\n",
    "    grad_m = -2/n * np.sum(x * (y - y_pred))\n",
    "    grad_b = -2/n * np.sum(y - y_pred)\n",
    "    return grad_m, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cost_function` calculates the mean squared error (MSE) between the true values (`y_true`) and predicted values (`y_pred`).\n",
    "The MSE is a commonly used cost function in regression problems and measures the average squared difference between the predicted and true values. \n",
    "The smaller the MSE, the better the model performance.\n",
    "\n",
    "The `gradients` function calculates the gradients of the cost function with respect to the model parameters (in this case, the slope $m$ and intercept $b$ of a linear regression model).\n",
    "These gradients are used by an optimizer (such as stochastic gradient descent) to update the parameters during training, in order to minimize the cost function and improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "def sgd(x, y, learning_rate=0.1, n_epochs=50):\n",
    "    # Initialize the slope and intercept with random values\n",
    "    m = np.random.randn(1)\n",
    "    b = np.random.randn(1)\n",
    "    n = len(y)\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle the indices to randomly sample the data\n",
    "        shuffled_indices = np.random.permutation(n)\n",
    "        x_shuffled = x[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        # Loop over the data in batches (in this one piece at a time)\n",
    "        for i in range(0, n, 1):\n",
    "            # Select a batch of data\n",
    "            x_batch = x_shuffled[i:i+1]\n",
    "            y_batch = y_shuffled[i:i+1]\n",
    "\n",
    "            # Predict the output values using the current parameters\n",
    "            y_pred = m * x_batch + b\n",
    "\n",
    "            # Calculate the gradients of the cost function with respect to m and b\n",
    "            grad_m, grad_b = gradients(x_batch, y_batch, y_pred)\n",
    "\n",
    "            # Update the parameters using the gradients and learning rate\n",
    "            m -= learning_rate * grad_m\n",
    "            b -= learning_rate * grad_b\n",
    "\n",
    "    # Return the final parameters\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_batch_gradient_descent(x, y, learning_rate=0.1, n_epochs=50):\n",
    "    # Initialize the slope and intercept with random values\n",
    "    m = np.random.randn(1)\n",
    "    b = np.random.randn(1)\n",
    "    n = len(y)\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Predict the output values using the current parameters for the full batch\n",
    "        y_pred = m * x + b\n",
    "\n",
    "        # Calculate the gradients of the cost function with respect to m and b for the full batch\n",
    "        grad_m, grad_b = gradients(x, y, y_pred)\n",
    "\n",
    "        # Update the parameters using the gradients and learning rate\n",
    "        m -= learning_rate * grad_m\n",
    "        b -= learning_rate * grad_b\n",
    "\n",
    "    # Return the final parameters\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sgd function implements the stochastic gradient descent (SGD) algorithm for linear regression.\n",
    "SGD is an optimization algorithm that uses a small batch of randomly sampled data points to estimate the gradient of the cost function at each iteration.\n",
    "It is often used in large-scale machine learning problems because it can be more computationally efficient than traditional batch gradient descent.\n",
    "\n",
    "The function takes as input the input data x, the output data y, the learning rate learning_rate, the number of epochs n_epochs, and the batch size `batch_size`.\n",
    "It initializes the slope m and intercept b with random values, shuffles the data randomly at each epoch, and loops over the data in batches of size `batch_size`.\n",
    "\n",
    "For each batch of data, the function predicts the output values `y_pred` using the current parameter values, calculates the gradients of the cost function with respect to $m$ and $b$ using the gradients function, and updates the parameters using the gradients and learning rate.\n",
    "This process is repeated for the specified number of epochs.\n",
    "\n",
    "The function returns the final values of $m$ and $b$, which represent the optimal parameters of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "m, b = sgd(x, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = m * x + b\n",
    "\n",
    "# Calculate the cost\n",
    "cost = cost_function(y, y_pred)\n",
    "print(f\"Final cost: {cost:.4f}\")\n",
    "print(f\"Learned parameters: m={m[0]:.4f}, b={b[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data and the learned line\n",
    "plt.scatter(x, y, label=\"Data points\")\n",
    "plt.plot(x, y_pred, color=\"red\", label=\"Learned line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "m, b = full_batch_gradient_descent(x, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = m * x + b\n",
    "\n",
    "# Calculate the cost\n",
    "cost = cost_function(y, y_pred)\n",
    "print(f\"Final cost: {cost:.4f}\")\n",
    "print(f\"Learned parameters: m={m[0]:.4f}, b={b[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the learned line\n",
    "plt.scatter(x, y, label=\"Data points\")\n",
    "plt.plot(x, y_pred, color=\"red\", label=\"Learned line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
