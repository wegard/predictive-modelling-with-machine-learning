{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ae256f",
   "metadata": {},
   "source": [
    "# Logistic Regression from an Adaline-like approach\n",
    "\n",
    "In this notebook, we demonstrate how a gradient-descent-based Logistic Regression model can be implemented by making slight modifications to the Adaline gradient descent.\n",
    "\n",
    "We will walk through reading and preparing the Iris dataset, implementing our Logistic Regression class, fitting it on a subset of Iris data for binary classification, and finally extending the same approach to multiple classes using a One-vs-All (OvA) scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be029d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a80f92a",
   "metadata": {},
   "source": [
    "### Loading the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading-in the Iris data.\n",
    "# We'll attempt to read it from the UCI Machine Learning Repository.\n",
    "# If that fails, we revert to reading from a local path (if available). \n",
    "\n",
    "try:\n",
    "    from urllib.error import HTTPError  # For completeness, in case not imported\n",
    "    s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    print('From URL:', s)\n",
    "    df = pd.read_csv(s, header=None, encoding='utf-8')\n",
    "\n",
    "except HTTPError:\n",
    "    s = 'iris.data'\n",
    "    print('From local Iris path:', s)\n",
    "    df = pd.read_csv(s, header=None, encoding='utf-8')\n",
    "\n",
    "# The dataframe df now holds the Iris data, with the last column containing class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697a510",
   "metadata": {},
   "source": [
    "### Implementing Logistic Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    \"\"\"Gradient descent-based logistic regression classifier.\n",
    "    \n",
    "    This class implements a logistic regression classifier that uses\n",
    "    gradient descent to learn the parameters. It is closely related to \n",
    "    the Adaline implementation, with a different activation function \n",
    "    and loss calculation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float, default=0.01\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int, default=50\n",
    "        Number of passes over the training dataset.\n",
    "    random_state : int, default=1\n",
    "        Seed for random weight initialization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1D ndarray\n",
    "        Weights after training.\n",
    "    b_ : float\n",
    "        Bias term after fitting.\n",
    "    losses_ : list\n",
    "        List of loss values at each epoch (cross-entropy here).\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (binary: 0 or 1).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            The fitted logistic regression model.\n",
    "        \"\"\"\n",
    "        # For reproducibility\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "\n",
    "        # Initialize weights to small random numbers\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        # Initialize bias as zero\n",
    "        self.b_ = 0.0\n",
    "\n",
    "        self.losses_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            # Compute net input\n",
    "            net_input = self.net_input(X)\n",
    "\n",
    "            # Pass net input through sigmoid activation\n",
    "            output = self.activation(net_input)\n",
    "\n",
    "            # Compute errors (difference between actual and predicted)\n",
    "            errors = (y - output)\n",
    "\n",
    "            # Update weights via gradient descent rule\n",
    "            # The dot product yields sum of X_j * error_i, normalized by number of samples\n",
    "            self.w_ += self.eta * X.T.dot(errors) / X.shape[0]\n",
    "\n",
    "            # Update bias\n",
    "            self.b_ += self.eta * errors.mean()\n",
    "\n",
    "            # Cross-entropy loss (Logistic Loss)\n",
    "            # We add a small clip to avoid log(0) issues if needed.\n",
    "            # Output is already clipped in self.activation, but this is an extra measure.\n",
    "            output_clipped = np.clip(output, 1e-10, 1.0 - 1e-10)\n",
    "            loss = -y.dot(np.log(output_clipped)) - (1 - y).dot(np.log(1 - output_clipped))\n",
    "            loss /= X.shape[0]\n",
    "\n",
    "            self.losses_.append(loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input (linear combination of weights and input).\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"Compute logistic sigmoid activation.\n",
    "            We also apply np.clip(...) to prevent overflow in np.exp().\"\"\"\n",
    "        z_clipped = np.clip(z, -250, 250)\n",
    "        return 1. / (1. + np.exp(-z_clipped))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class label (0 or 1) based on 0.5 threshold.\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d7969",
   "metadata": {},
   "source": [
    "### Helper Function to Visualize Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plots the decision regions for a 2D dataset (two chosen features).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = [n_samples, 2]\n",
    "        Feature matrix for visualization (2 features).\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target labels.\n",
    "    classifier : object\n",
    "        A classifier with a .predict() method.\n",
    "    resolution : float\n",
    "        Step size in the mesh.\n",
    "\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # find the min and max values for the two features (with a little extra margin)\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    # create a 2D grid of points from these min/max values\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    # predict the label for each point in the grid\n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "\n",
    "    # create a contour plot for the decision boundaries\n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot the data points by class\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0],\n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8,\n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx],\n",
    "                    label=f'Class {cl}',\n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde959c6",
   "metadata": {},
   "source": [
    "## EXAMPLE 1: Two classes, two features\n",
    "\n",
    "Here, we will classify the Iris dataset's first 100 samples, which correspond to *Iris-setosa* and *Iris-versicolor*. We only take two features (for easier plotting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ccbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll select only the first 100 rows of Iris, as they correspond\n",
    "# to Setosa and Versicolor classes.\n",
    "# Then we create a binary label: 0 for setosa, 1 for versicolor.\n",
    "\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "\n",
    "# For the sake of easy plotting, we'll pick just two features:\n",
    "# - Column 0: sepal length\n",
    "# - Column 2: petal length\n",
    "X = df.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e5377",
   "metadata": {},
   "source": [
    "#### Visualizing these two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73078bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with Setosa in red, Versicolor in blue\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label='Setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='s', label='Versicolor')\n",
    "\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc676da2",
   "metadata": {},
   "source": [
    "#### Fitting the Logistic Regression classifier on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our logistic regression (GD-based) with a higher learning rate.\n",
    "lrgd = LogisticRegressionGD(eta=0.3, n_iter=1000, random_state=1)\n",
    "\n",
    "# Fit the model on our two-feature data\n",
    "lrgd.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affaafa",
   "metadata": {},
   "source": [
    "#### Visualizing the predictions vs. actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c79e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predicted labels from the model\n",
    "y_pred = lrgd.predict(X)\n",
    "\n",
    "# Let's plot the data again, but also highlight predicted labels.\n",
    "# We'll add small black markers: '.' for predicted class 0, 'x' for predicted class 1.\n",
    "\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label='Setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='*', label='Versicolor')\n",
    "\n",
    "dict_iris = {0: '.', 1: 'x'}  # dictionary for predicted markers\n",
    "\n",
    "for j in range(100):\n",
    "    plt.scatter(X[j, 0], X[j, 1],\n",
    "                color='black', marker=dict_iris[y_pred[j]])\n",
    "\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cca9f",
   "metadata": {},
   "source": [
    "#### Plotting the decision boundary\n",
    "Below, we use the `plot_decision_regions` helper to directly visualize the boundary that our logistic regression model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X=X, y=y, classifier=lrgd)\n",
    "\n",
    "plt.xlabel('Petal length [cm]')\n",
    "plt.ylabel('Sepal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8326af",
   "metadata": {},
   "source": [
    "## EXAMPLE 2: Two classes, **all** features\n",
    "\n",
    "We now take the same two classes from the Iris dataset, but use **all four features** (sepal length, sepal width, petal length, petal width). We won't be able to directly visualize the 4D boundary, but we can pick two features to visualize afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c284f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the same 100 rows, but keep all 4 features:\n",
    "X_all = df.iloc[0:100, :4].values\n",
    "# y remains the same: 0 (Setosa) or 1 (Versicolor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the logistic regression model on all 4 features\n",
    "lrgd = LogisticRegressionGD(eta=0.3, n_iter=1000, random_state=1)\n",
    "lrgd.fit(X_all, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059cb11",
   "metadata": {},
   "source": [
    "#### Checking predictions and plotting (for two selected features only)\n",
    "We can still pick any two features out of the four to see how the classification might look, even though the model uses all 4 features internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions on the training data\n",
    "y_pred_all = lrgd.predict(X_all)\n",
    "print(\"Predictions on the 4-feature set:\")\n",
    "print(y_pred_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the class probabilities (sigmoid outputs):\n",
    "probs_all = lrgd.activation(lrgd.net_input(X_all))\n",
    "print(\"Class Probabilities (for '1') on the 4-feature set (first 10 samples):\")\n",
    "print(probs_all[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1dd9c4",
   "metadata": {},
   "source": [
    "#### Plotting the classification using only two of the features for display\n",
    "The model still uses all 4 features to decide, but we'll plot using only 2. We re-use the same code as before, but we skip using the decision region function since it expects a 2D dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10168e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's choose features 0 and 2 just for plotting:\n",
    "ftp = [0, 2]\n",
    "\n",
    "# Plot the original data for the same 100 samples.\n",
    "plt.scatter(X_all[:50, ftp[0]], X_all[:50, ftp[1]],\n",
    "            color='red', marker='o', label='Setosa')\n",
    "plt.scatter(X_all[50:100, ftp[0]], X_all[50:100, ftp[1]],\n",
    "            color='blue', marker='*', label='Versicolor')\n",
    "\n",
    "# Now overlay predicted labels in black.\n",
    "dict_iris = {0: '.', 1: 'x'}\n",
    "for j in range(100):\n",
    "    plt.scatter(X_all[j, ftp[0]], X_all[j, ftp[1]],\n",
    "                color='black', marker=dict_iris[y_pred_all[j]])\n",
    "\n",
    "dict_features = {0: 'Sepal length [cm]',\n",
    "                 1: 'Sepal width [cm]',\n",
    "                 2: 'Petal length [cm]',\n",
    "                 3: 'Petal width [cm]'}\n",
    "\n",
    "plt.xlabel(dict_features[ftp[0]])\n",
    "plt.ylabel(dict_features[ftp[1]])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ee02c",
   "metadata": {},
   "source": [
    "## EXAMPLE 3: One-vs-All for three classes\n",
    "\n",
    "Next, we extend the logistic regression approach to more than 2 classes. A simple approach is the *One-vs-All* (OvA) scheme: we train one classifier for each class, labeling samples of that class as '1' and the rest as '0'. Then, we take the class that has the highest probability as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now use the entire 150-sample Iris dataset for all 3 classes.\n",
    "# 0 -> Iris-setosa, 1 -> Iris-versicolor, 2 -> Iris-virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6708f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string labels to integer labels 0, 1, or 2\n",
    "y_all = df.iloc[:, 4].values\n",
    "y_all = np.where(y_all == 'Iris-setosa', 0, y_all)\n",
    "y_all = np.where(y_all == 'Iris-versicolor', 1, y_all)\n",
    "y_all = np.where(y_all == 'Iris-virginica', 2, y_all)\n",
    "\n",
    "# Extract all 4 features now.\n",
    "X_all2 = df.iloc[:, :4].values\n",
    "\n",
    "print(\"Class labels:\", np.unique(y_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd468",
   "metadata": {},
   "source": [
    "#### One-vs-All approach\n",
    "We will create three separate binary logistic regression models, each trained to detect one class vs. all others (e.g., Class=0 vs. not-0, Class=1 vs. not-1, Class=2 vs. not-2). Then, we store the probabilities and pick the class with the highest probability for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4432549",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = np.zeros((X_all2.shape[0], 3))\n",
    "\n",
    "for k in range(3):\n",
    "    # Build a binary label for class k\n",
    "    y_k = np.where(y_all == k, 1, 0)\n",
    "\n",
    "    # Create and fit a logistic regression model\n",
    "    lrgd_k = LogisticRegressionGD(eta=0.3, n_iter=1000, random_state=1)\n",
    "    lrgd_k.fit(X_all2, y_k)\n",
    "\n",
    "    # Store the predicted probabilities for class k\n",
    "    probabilities[:, k] = lrgd_k.activation(lrgd_k.net_input(X_all2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b3496",
   "metadata": {},
   "source": [
    "Now, each row of `probabilities` has 3 values, one for each class's logistic regression model. Note that in practice, these do not sum to 1, since each classifier is doing a separate binary detection. \n",
    "\n",
    "To get the final predicted class, we take the `argmax` across the 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probabilities array shape:\", probabilities.shape)\n",
    "print(\"Probabilities (first 5 samples):\\n\", probabilities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cac4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's check the probabilities for some rows and the corresponding argmax\n",
    "some_rows = [100, 40, 120, 50, 0]\n",
    "print(\"Probabilities for rows\", some_rows, \":\\n\", probabilities[some_rows, :])\n",
    "\n",
    "predicted_classes = np.argmax(probabilities[some_rows, :], axis=1)\n",
    "print(\"Predicted classes for these rows:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the predicted class across *all* samples:\n",
    "predicted_all = np.argmax(probabilities, axis=1)\n",
    "print(\"Full predicted classes (first 25 shown):\\n\", predicted_all[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db01570",
   "metadata": {},
   "source": [
    "Note that if we wanted a valid set of 'class probabilities' that sum to 1, we might consider *softmax regression*, but the OvA approach here simply uses separate binary logistic regressions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587787f5",
   "metadata": {},
   "source": [
    "### Example: Another Minimal Implementation (for reference)\n",
    "The following minimal class `OneModel` is conceptually the same logistic regression approach but coded more succinctly, purely for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneModel:\n",
    "    \"\"\"A minimal logistic regression model using gradient descent.\"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model parameters to the given training data.\"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = 0.0\n",
    "        self.losses_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "\n",
    "            self.w_ += self.eta * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * errors.mean()\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            output_clipped = np.clip(output, 1e-10, 1 - 1e-10)\n",
    "            loss = -y.dot(np.log(output_clipped)) - (1 - y).dot(np.log(1 - output_clipped))\n",
    "            loss /= X.shape[0]\n",
    "            self.losses_.append(loss)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, z):\n",
    "        z_clipped = np.clip(z, -250, 250)\n",
    "        return 1. / (1. + np.exp(-z_clipped))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb294a-b940-4709-819e-f11a649af3a6",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression\n",
    "\n",
    "For multi-class classification with $K$ classes, we use a one-hot encoding for the labels. Let $y_{ik}$ be 1 if sample $i$ belongs to class $k$ and 0 otherwise. For each class $k$, we have a corresponding linear predictor:\n",
    "$$\n",
    "z_{ik} = \\mathbf{w}_k^\\top \\mathbf{x}_i + b_k.\n",
    "$$\n",
    "\n",
    "The softmax function is then used to convert these scores into probabilities:\n",
    "$$\n",
    "\\hat{p}_{ik} = \\frac{\\exp(z_{ik})}{\\sum_{j=1}^{K} \\exp(z_{ij})}.\n",
    "$$\n",
    "\n",
    "The multinomial (or categorical) cross-entropy loss is:\n",
    "$$\n",
    "\\mathcal{L}(\\{\\mathbf{w}_k, b_k\\}) = - \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{p}_{ik}) = - \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log\\left(\\frac{\\exp(\\mathbf{w}_k^\\top \\mathbf{x}_i + b_k)}{\\sum_{j=1}^{K} \\exp(\\mathbf{w}_j^\\top \\mathbf{x}_i + b_j)}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab6da4-b475-4ca5-bbb7-c8665d4af4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "# Initialize and train the multinomial logistic regression model.\n",
    "# The 'lbfgs' solver supports multinomial loss; you can also experiment with others like 'saga'.\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b31df0-ef2a-4757-b0d1-adefb7533f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gra4150",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
