{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "## Lecture 3\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) is a supervised machine learning technique used for dimensionality reduction and classification problems.\n",
    "It is a linear method that project the data onto a lower-dimensional space while maximizing the separation between different classes.\n",
    "LDA is a popular technique in the field of pattern recognition and is often used for image and speech recognition, as well as natural language processing.\n",
    "Should not be confused with Latent Dirichlet Allocation, also called LDA, which is an unsupervised clustering algorithm.\n",
    "\n",
    "The basic idea behind LDA is to find a linear combination of features that separates different classes as much as possible.\n",
    "It does this by maximizing the ratio of between-class variance to within-class variance, known as the \"Fisher criterion\".\n",
    "This results in a new set of features, called \"discriminant functions\", that can be used to separate the classes.\n",
    "\n",
    "In practice, LDA is often used as a preprocessing step before applying a classifier such as logistic regression or support vector machines.\n",
    "By reducing the dimensionality of the data, LDA can improve the performance of the classifier and make it more efficient.\n",
    "Additionally, LDA can also be used as a standalone classifier, in which case the decision boundary is linear.\n",
    "\n",
    "We wil not go into the details of how LDA works but we will look at how it can be implemented in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn example\n",
    "\n",
    "Let's use the `make_classification` from Scikit-learn to generate some toy data with 2 informative features and 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Generate some data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate some toy data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=1, class_sep=0.75, random_state=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33, random_state=1)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "colors = ['navy', 'orange']\n",
    "target_names = list(set(y))\n",
    "for i, label in enumerate(target_names):\n",
    "    plt.scatter(X_train[y_train==i, 0], X_train[y_train==i, 1], label=label, c=colors[i])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the lda\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Create an instance of the LDA model\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# Fit the model to the data\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# We can now project the data onto a lower dimensional space\n",
    "\n",
    "X_transformed = lda.transform(X_train)\n",
    "#X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can plot the LDA component\n",
    "plt.scatter(range(len(X_transformed)), X_transformed, c=y_train, cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot the original data points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.show()\n",
    "lda.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis of the iris dataset\n",
    "\n",
    "Here we load the Iris dataset from scikit-learn, which includes 150 samples of iris flowers, each with 4 features (sepal length, sepal width, petal length, and petal width). Next, we split the data into a training set and a test set.\n",
    "\n",
    "We create an instance of the LinearDiscriminantAnalysis class and set the number of components to 2. This reduces the dimensionality of the data from 4 to 2, allowing for easier visualization.\n",
    "\n",
    "Finally, we use the fit_transform method on the training data to fit the LDA model and transform the data. Then we use the transform method on the test data to transform it using the LDA model learned from the training set.\n",
    "\n",
    "It's important to note that LDA is a supervised method, and it requires the class labels to be passed as an argument in the fit method (y_train) in order to learn the class means and variances.\n",
    "\n",
    "We also create a scatter plot of the transformed data, where the x-axis corresponds to the first linear discriminant (LD1) and the y-axis corresponds to the second linear discriminant (LD2). The color of each point represents the class label of the corresponding sample.\n",
    "\n",
    "With this plot, it's easy to see how the LDA has separated the different classes of iris flowers based on the two linear discriminants. This can be useful for visualizing the structure of the data and for understanding how well the LDA algorithm is able to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # Importing the iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "X2, y2 = iris.data, iris.target\n",
    "\n",
    "# Split the data into a train and test set\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=10)\n",
    "\n",
    "# Creating an instance of LinearDiscriminantAnalysis and setting number of components to 2\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "# Fitting and transforming the training data\n",
    "X_train_lda = lda.fit_transform(X_train2, y_train2)\n",
    "\n",
    "# Transforming the test data using the model learned from training data\n",
    "X_test_lda = lda.transform(X_test2)\n",
    "\n",
    "# assign label to each type of flower\n",
    "target_names = iris.target_names\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, label in enumerate(target_names):\n",
    "    plt.scatter(X_train_lda[y_train2==i, 0], X_train_lda[y_train2==i, 1], label=label, c=colors[i])\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the separation of the test data\n",
    "\n",
    "# assign label to each type of flower\n",
    "target_names = iris.target_names\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, label in enumerate(target_names):\n",
    "    plt.scatter(X_test_lda[y_test2==i, 0], X_test_lda[y_test2==i, 1], label=label, c=colors[i])\n",
    "\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
