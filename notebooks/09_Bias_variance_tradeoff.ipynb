{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias variance tradeoff\n",
    "\n",
    "## Lecture 5\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept that refers to the tradeoff between two sources of model error, that is: **bias** and **variance**.\n",
    "\n",
    "Bias refers to the error that is introduced by approximating a real-world problem with a simpler model.\n",
    "Models with high bias tend to make strong assumptions about the underlying data and therefore have limited capacity to capture complex patterns in the data.\n",
    "As a result, models with high bias tend to underfit the data, meaning that they perform poorly on both the training data and new, unseen data.\n",
    "\n",
    "Variance refers to the error that is introduced by the model's sensitivity to small fluctuations in the training data.\n",
    "Models with high variance tend to be very flexible and capture many details of the training data, but they may overfit the data, meaning that they perform well on the training data but poorly on new, unseen data.\n",
    "\n",
    "The goal of machine learning is to find a balance between bias and variance to produce a model that generalizes well to new data.\n",
    "\n",
    "If the model has high bias, increasing its complexity will reduce the bias but increase the variance.\n",
    "\n",
    "If the model has high variance, reducing its complexity will reduce the variance but increase the bias.\n",
    "\n",
    "The challenge is to find the right balance that minimizes the overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting vs underfitting:\n",
    "Here we generate some noisy non-linear data and fit it with polynomial regression models of increasing degree.\n",
    "\n",
    "- As the degree of the polynomial increases, the model becomes more flexible and captures more of the noise in the data, leading to overfitting.\n",
    "- As the degree decreases, the model becomes less flexible and has a high bias, leading to underfitting.\n",
    "\n",
    "The code bellow generates some noisy sinusoidal data and fits polynomial regression models of different degrees to the data.\n",
    "\n",
    "The models are fit using 10-fold cross-validation to estimate the mean squared error, and the results are plotted.\n",
    "\n",
    "You can see how as the degree of the polynomial increases, the model becomes more flexible and captures more of the noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generate some noisy data\n",
    "np.random.seed(10)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + 0.2 * np.random.randn(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(X, y, 'o', label='data', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_regression(degree, cv=10):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                 ('linear', LinearRegression(fit_intercept=False))])\n",
    "    scores = cross_val_score(model, X, y, cv=cv)\n",
    "    y_pred = model.fit(X, y).predict(X)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(X, y, 'o', label='data', markersize=10)\n",
    "    plt.plot(X, y_pred, lw=4, linestyle='--', label=f'PR with degree={degree} (error={np.mean(np.abs(scores)):.3f})')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit polynomial regression models of degree 1\n",
    "\n",
    "polynomial_regression(1)\n",
    "plt.savefig('../tex/figures/bias_variance_tradeoff_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plot that the model predictions are not a good fit to the observed data.\n",
    "The model is too simple, or has too few parameters, to capture the underlying pattern in the data.\n",
    "\n",
    "This is an example of high bias.\n",
    "\n",
    "LetÂ´s increase the model complexity by adding more parameters to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit polynomial regression models of degree 2\n",
    "\n",
    "polynomial_regression(2)\n",
    "plt.savefig('../tex/figures/bias_variance_tradeoff_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit polynomial regression models of degree 3\n",
    "\n",
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit polynomial regression models of degree 4\n",
    "\n",
    "polynomial_regression(4)\n",
    "plt.savefig('../tex/figures/bias_variance_tradeoff_4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit polynomial regression models of degree 8\n",
    "\n",
    "polynomial_regression(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit polynomial regression models of degree 20\n",
    "\n",
    "polynomial_regression(20)\n",
    "plt.savefig('../tex/figures/bias_variance_tradeoff_20.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plot that the model predictions are overfitting the observed data, and are capturing the noise in the data as well as the underlying pattern.\n",
    "The model is too complex, has too many parameters, and is not generalizing well to unseen data.\n",
    "This is an example of high variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
