{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Lecture 4\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a popular supervised learning algorithm used in various applications, including binary classification.\n",
    "It is used to model the relationship between a set of input features and a binary outcome (0 or 1) by using a logistic function.\n",
    "\n",
    "The logistic regression model can be represented mathematically by the following equation:\n",
    "\n",
    "$$ \\hat{y} = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "where $z$ is a linear combination of the input features and parameters (weights), represented as:\n",
    "\n",
    "$$ z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p $$\n",
    "\n",
    "The parameters (weights) are learned during the training process, where the logistic regression model is fit to the training data by minimizing the loss function.\n",
    "\n",
    "The likelihood function for logistic regression models is a probability function that measures the goodness of fit of the model to the observed data.\n",
    "It is used to find the parameters of the model that maximize the probability of observing the training data, given the model.\n",
    "\n",
    "Mathematically, the likelihood function for a logistic regression model with $n$ data points and $m$ input features is given by:\n",
    "\n",
    "$$ L(w) = \\prod_{i=1}^n [y_i = 1]p_i + [y_i = 0](1 - p_i) $$\n",
    "\n",
    "where w is the vector of model parameters, $y_i$ is the binary class label for the $i$-th data point, and $p_i$ is the predicted probability of the positive class for the $i$-th data point, given by the logistic function:\n",
    "\n",
    "$$ p_i = \\frac{1}{1 + e^{-z_i}} $$\n",
    "\n",
    "and $z_i$ is the linear combination of the input features and the model parameters for the $i$-th data point:\n",
    "\n",
    "$$ z_i = \\beta_0 + âˆ‘_{j=1}^m \\beta_j x_{ij} $$\n",
    "\n",
    "The goal of logistic regression is to find the values of the model parameters that maximize the likelihood function, given the observed data.\n",
    "This is typically done using optimization algorithms, such as gradient descent or L-BFGS.\n",
    "\n",
    "The predicted outcome, $\\hat{y}$, represents the probability of the positive class (1).\n",
    "In binary classification problems, a threshold of 0.5 is often used to determine the class label, with values greater than 0.5 being classified as 1 and values less than 0.5 being classified as 0.\n",
    "The logistic function produces a probability between 0 and 1, which is transformed into binary class predictions through the use of a threshold.\n",
    "\n",
    "When there are more than two outcomes, logistic regression can be extended to a multiclass classification problem using one of the following techniques:\n",
    "\n",
    "1. **One vs All (OvA)**: This involves training multiple binary classifiers, each one making a binary decision between one of the classes as positive and all other classes as negative. The class with the highest predicted probability is chosen as the final prediction.\n",
    "2. **Softmax Regression (Multinomial Logistic Regression)**: This involves directly modeling the probability distribution over all classes using the softmax function. The softmax function computes the exponential of each input and then normalizes the result to produce a probability distribution over the classes.\n",
    "\n",
    "Both of these methods allow logistic regression to be applied to multiclass classification problems, and the choice between them often depends on the size and structure of the data, as well as computational considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the survival rate of passengers on the Titanic using logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('../data/titanic/train.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df = df.dropna()\n",
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a logistic regression model\n",
    "\n",
    "Training a logistic regression model involves finding the coefficients of the model that best fit the training data.\n",
    "The goal is to find the coefficients that maximize the likelihood of observing the training data, given the model.\n",
    "The coefficients are estimated using optimization algorithms, such as gradient descent or the limited-memory BFGS method.\n",
    "The optimization problem is solved iteratively, with the algorithm updating the coefficients at each iteration based on the gradient of the likelihood function.\n",
    "\n",
    "In scikit-learn, the following solvers can be used for training logistic regression models:\n",
    "\n",
    "- \"newton-cg\" - Uses the Newton-CG method.\n",
    "- \"lbfgs\" - Uses the limited-memory BFGS method.\n",
    "- \"liblinear\" - Uses a library for large linear classification problems.\n",
    "- \"sag\" - Uses the Stochastic Average Gradient descent.\n",
    "- \"saga\" - Uses the Stochastic Average Gradient descent with an optimized scaling for the step size.\n",
    "\n",
    "The choice of solver will depend on the size and characteristics of your data and the specific requirements of your problem.\n",
    "\n",
    "Once the coefficients have been estimated, the model can be used to make predictions on new data.\n",
    "To make a prediction for a new data point, the input features are multiplied by the estimated coefficients and passed through the logistic function.\n",
    "The output of the logistic function represents the predicted probability of the positive class.\n",
    "A threshold is then applied to the predicted probability to determine the final binary class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.892\n",
      "Survival rate in test set: 0.541\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcome on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", score.round(3))\n",
    "print(\"Survival rate in test set:\", y_test.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.325814</td>\n",
       "      <td>-2.136436</td>\n",
       "      <td>-0.021277</td>\n",
       "      <td>0.136485</td>\n",
       "      <td>-0.242407</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0 -0.325814 -2.136436 -0.021277  0.136485 -0.242407  0.002133"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the model's parameters\n",
    "\n",
    "pd.DataFrame(log_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Access the probability of the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00632882 0.99367118]]\n"
     ]
    }
   ],
   "source": [
    "# Fit the Logistic Regression model\n",
    "X = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "clf = LogisticRegression(solver='lbfgs').fit(X, y)\n",
    "\n",
    "# Obtain predicted probabilities for a set of predictor variables\n",
    "x_new = np.array([[5, 10]])\n",
    "probabilities = clf.predict_proba(x_new)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
