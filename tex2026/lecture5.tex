% !TEX TS-program = XeLaTeX
% !TEX spellcheck = en-US
\documentclass[aspectratio=169, 10pt]{beamer}

% Standard neat theme suggestions (if 'example' doesn't work)
\usetheme{example} 
% \usecolortheme{beaver} % Optional: changes color scheme to red/grey

\usepackage{booktabs} % For nicer tables
\usepackage{multirow} % For the confusion matrix table

\title{Lecture 5:\\ Model Selection, Evaluation, and Assessment}
\institute{GRA4160: Predictive Modelling with Machine Learning}
\date{February 4th, 2026}
\author{Vegard H\o ghaug Larsen}

\begin{document}

\maketitle

%---------------------------------------------------------
\frame{
  \frametitle{Plan for Today}
  \tableofcontents
  % Alternatively, keep your manual enumeration if you prefer:
  % \begin{enumerate}
  %   \item Model Selection vs. Evaluation
  %   \item Metrics \& Diagnostic Tools
  %   \item Information Criteria (AIC/BIC)
  %   \item Cross-Validation
  %   \item The Bias-Variance Trade-off
  % \end{enumerate}
}

%---------------------------------------------------------
\section{Selection \& Evaluation}

\frame{
  \frametitle{Model Selection vs. Evaluation}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \begin{block}{Model Selection}
        The process of choosing the best model (or hyperparameters) from a set of candidates.
        \begin{itemize}
            \item \textbf{Goal:} Select the model structure.
            \item \textbf{Tool:} Validation sets, Information Criteria.
        \end{itemize}
      \end{block}
    \end{column}

    \begin{column}{0.48\textwidth}
      \begin{block}{Model Evaluation}
        Measuring how well the \textit{final} model performs on unseen data.
        \begin{itemize}
            \item \textbf{Goal:} Estimate real-world error.
            \item \textbf{Tool:} Test sets, Cross-Validation.
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}
  
  \vspace{1em}
  \begin{alertblock}{The Pitfall}
    Overusing a single validation set for selection can lead to overfitting that specific subset of data.
  \end{alertblock}
}

%---------------------------------------------------------
\frame{
  \frametitle{The Confusion Matrix}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      A confusion matrix summarizes a classification modelâ€™s performance by comparing predicted vs. actual values.
      
      \vspace{1em}
      \begin{itemize}
        \item \textbf{TP:} Correctly identified positive.
        \item \textbf{TN:} Correctly identified negative.
        \item \textbf{FP (Type I):} False alarm.
        \item \textbf{FN (Type II):} Missed detection.
      \end{itemize}
    \end{column}
    
    \begin{column}{0.4\textwidth}
      \centering
      \renewcommand{\arraystretch}{1.5}
      \begin{tabular}{cc|cc}
        & & \multicolumn{2}{c}{\textbf{Predicted}} \\
        & & Pos & Neg \\ \hline
        \multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{Actual}}} 
        & Pos & \textbf{\color{green!60!black} TP} & \textbf{\color{red} FN} \\
        & Neg & \textbf{\color{red} FP} & \textbf{\color{green!60!black} TN} \\
      \end{tabular}
    \end{column}
  \end{columns}
}

%---------------------------------------------------------
\frame{
  \frametitle{Common Metrics for Model Evaluation}
  \small
  \begin{itemize}
    \item \textbf{Accuracy:} Fraction of correct predictions. \textit{(Caution: Misleading in imbalanced datasets).}
    $$ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} $$

    \item \textbf{Precision:} Purity of positive predictions. Vital when \textbf{False Positives} are costly (e.g., spam filters).
    $$ \text{Precision} = \frac{TP}{TP + FP} $$

    \item \textbf{Recall (Sensitivity):} Ability to find positive instances. Vital when \textbf{False Negatives} are dangerous (e.g., cancer detection).
    $$ \text{Recall} = \frac{TP}{TP + FN} $$

    \item \textbf{F1-Score:} The harmonic mean of Precision and Recall. Useful when you need a balance between the two.

    \item \textbf{ROC \& AUC:} Plots TPR vs. FPR. AUC (Area Under Curve) summarizes performance across \textit{all} thresholds.
  \end{itemize}
}

%---------------------------------------------------------
\section{Information Criteria}

\frame{
  \frametitle{Introduction to Information Criteria}
  \textit{Goal: Select a model that explains the data well without being overly complex.}
  
  \vspace{1em}
  Information criteria penalize the likelihood based on the number of parameters ($k$).
  
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \begin{block}{Akaike Information Criterion (AIC)}
        $$ AIC = 2k - 2\ln(L) $$
        \begin{itemize}
            \item Estimates out-of-sample prediction error.
            \item Penalizes complexity ($2k$).
        \end{itemize}
      \end{block}
    \end{column}

    \begin{column}{0.48\textwidth}
      \begin{block}{Bayesian Information Criterion (BIC)}
        $$ BIC = \ln(n)k - 2\ln(L) $$
        \begin{itemize}
            \item penalizes complexity more heavily when $n$ is large.
            \item Tends to favor simpler models than AIC.
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}
  
  \vspace{1em}
  \centering \small{\textbf{Rule of thumb:} Lower values are better. Compare only on the same dataset.}
}

%---------------------------------------------------------
\section{Cross-Validation}

\frame{
  \frametitle{\(k\)-Fold Cross-Validation}
    \textbf{The Procedure:}
    \begin{enumerate}
        \item Shuffle the dataset.
        \item Split into \( k \) groups (folds).
        \item For each unique group:
        \begin{itemize}
            \item Take the group as a hold-out or test data set.
            \item Take the remaining groups as a training data set.
            \item Fit a model on the training set and evaluate it on the test set.
        \end{itemize}
        \item Summarize the skill of the model using the sample of model evaluation scores.
    \end{enumerate}
}

\frame{

  \frametitle{\(k\)-Fold Cross-Validation}

  \begin{center}

    \includegraphics[scale=0.5]{figures/kFold_validation.png}

  \end{center}

  }
\frame{
  \frametitle{Advanced CV Techniques}
  \begin{description}
    \item[Stratified \(k\)-fold] \hfill \\
    Ensures each fold maintains the same class distribution as the full dataset. \textbf{Essential for imbalanced classes.}
    
    \vspace{1em} 
    \item[Leave-One-Out (LOOCV)] \hfill \\
    \( k = n \). Uses all but one observation for training.
    \begin{itemize}
        \item Pro: Nearly unbiased.
        \item Con: Computationally expensive.
    \end{itemize}

    \vspace{1em}
    \item[Repeated CV] \hfill \\
    Repeats the \( k \)-fold process multiple times with different splits to reduce variance in the estimate.
  \end{description}
}

%---------------------------------------------------------
\section{Bias-Variance Trade-off}

\frame{
  \frametitle{The Bias-Variance Trade-off}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \begin{alertblock}{Bias (Underfitting)}
        Error from erroneous assumptions (e.g., assuming data is linear when it's quadratic).
        \begin{itemize}
            \item Misses relevant relations.
            \item \textbf{Fix:} Increase complexity, add features.
        \end{itemize}
      \end{alertblock}
    \end{column}

    \begin{column}{0.48\textwidth}
      \begin{exampleblock}{Variance (Overfitting)}
        Error from sensitivity to small fluctuations in the training set.
        \begin{itemize}
            \item Models the random noise.
            \item \textbf{Fix:} Regularization, more data, simplify model.
        \end{itemize}
      \end{exampleblock}
    \end{column}
  \end{columns}

  \vspace{1em}
  \centering
  \textbf{Goal:} Find the sweet spot where Total Error (Bias\(^2\) + Variance) is minimized.
}

%---------------------------------------------------------
\frame{
  \frametitle{Summary and Key Takeaways}
  \begin{itemize}
    \item \textbf{Selection vs Evaluation:} Don't use your test set to tune parameters!
    \item \textbf{Metrics:} Accuracy is not enough. Use Precision/Recall/AUC for a complete picture.
    \item \textbf{Information Criteria:} AIC and BIC allow model comparison without a hold-out set, penalizing complexity.
    \item \textbf{Robustness:} Cross-Validation (especially Stratified) is the gold standard for performance estimation.
    \item \textbf{The Trade-off:} Machine Learning is largely the art of balancing Bias and Variance.
  \end{itemize}
}

\end{document}
