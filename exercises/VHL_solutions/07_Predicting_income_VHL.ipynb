{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting income\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "### GRA 4160\n",
    "### Advanced Regression and Classification Analysis, Ensemble Methods And Neural Networks\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble technique that trains a sequence of models, each one correcting the errors of the previous model.\n",
    "Boosting focuses on learning from the mistakes of the previous models and adjusts the weights of the training samples to emphasize the harder samples in the next round of training.\n",
    "In boosting, each model is trained on a weighted version of the training data, with more weight assigned to the samples that were misclassified by the previous models.\n",
    "The final prediction is the weighted sum of the predictions of the individual models. One of the most popular boosting algorithms is the Gradient Boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `GradientBoostingClassifier` in Scikit-Learn\n",
    "\n",
    "The gradient boosting algorithm is a popular ensemble learning method that combines multiple weak learners to form a strong learner.\n",
    "\n",
    "The `GradientBoostingClassifier` builds an ensemble of decision trees sequentially, where each subsequent tree aims to correct the errors made by the previous tree.\n",
    "Specifically, it minimizes a loss function by using gradient descent to update the parameters of the model, such as the weights of the features and the thresholds for the decision nodes.\n",
    "\n",
    "The `GradientBoostingClassifier` has several hyperparameters that can be tuned to control the complexity of the model and prevent overfitting, including:\n",
    "\n",
    "- **n_estimators**: The number of decision trees in the ensemble\n",
    "- **learning_rate**: The step size of the gradient descent algorithm\n",
    "- **max_depth**: The maximum depth of each decision tree in the ensemble\n",
    "- **min_samples_split**: The minimum number of samples required to split a decision node.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "The GradientBoostingClassifier also supports several loss functions, such as the deviance loss (used for binary classification) and the exponential loss (used for AdaBoost).\n",
    "Additionally, it provides several methods for making predictions, such as predict for obtaining the class labels and predict_proba for obtaining the class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "\n",
    "The Adult dataset, also commonly referred to as the \"Census Income\" dataset, is a popular resource for machine learning, especially for tasks involving classification and pattern recognition. This dataset was extracted from the 1994 Census database by Barry Becker and contains demographic information about adults from various backgrounds.\n",
    "\n",
    "Key features of the dataset include:\n",
    "\n",
    "- age: The age of the individual.\n",
    "- workclass: The type of employment of the individual (e.g., Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, etc.).\n",
    "- fnlwgt: Final weight. The number of people the census believes the entry represents.\n",
    "- education: The highest level of education achieved by the individual (e.g., Bachelors, Some-college, 11th, HS-grad, Prof-school, etc.).\n",
    "- education-num: The highest level of education in numerical form.\n",
    "- marital-status: Marital status of the individual.\n",
    "- occupation: The individual's occupation (e.g., Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, etc.).\n",
    "- relationship: Describes the individual's role in the family (e.g., Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried).\n",
    "- race: Race of the individual (e.g., White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black).\n",
    "- sex: The sex of the individual (Male, Female).\n",
    "- capital-gain: Income from investment sources, apart from wages/salary.\n",
    "- capital-loss: Losses from investment sources.\n",
    "- hours-per-week: Number of hours worked per week.\n",
    "- native-country: Country of origin for the individual.\n",
    "- income: Whether the individual earns more than $50K/year.\n",
    "\n",
    "The dataset is often used for predictive modeling and binary classification tasks, such as predicting whether an individual's income exceeds a certain threshold based on their demographic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education-num  marital-status  \\\n",
       "0   39          7   77516          9             13               4   \n",
       "1   50          6   83311          9             13               2   \n",
       "2   38          4  215646         11              9               0   \n",
       "3   53          4  234721          1              7               2   \n",
       "4   28          4  338409          9             13               2   \n",
       "\n",
       "   occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
       "0           1             1     4    1          2174             0   \n",
       "1           4             0     4    1             0             0   \n",
       "2           6             1     4    1             0             0   \n",
       "3           6             0     2    1             0             0   \n",
       "4          10             5     2    0             0             0   \n",
       "\n",
       "   hours-per-week  native-country  income  \n",
       "0              40              39       0  \n",
       "1              13              39       0  \n",
       "2              40              39       0  \n",
       "3              40              39       0  \n",
       "4              40               5       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                 header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                                     'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                                     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "\n",
    "# Replace '?' values with NaN\n",
    "df = df.replace('?', pd.NaT)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                        'relationship', 'race', 'sex', 'native-country']\n",
    "encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    df[feature] = encoder.fit_transform(df[feature])\n",
    "\n",
    "# Encode target variable\n",
    "df['income'] = df['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = df.drop(columns=['income'])\n",
    "y = df['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Train a Gradient Boosting Classifier using the \"Adult\" dataset and evaluate its performance using the area under the ROC curve (AUC).\n",
    "2. Experiment wih different hyperparameters of the Gradient Boosting Classifier, such as the number of trees, learning rate, and maximum depth of each tree, and observe how they affect the model's performance.\n",
    "3. Train an AdaBoost Classifier using the \"Adult\" dataset and compare its performance to the Gradient Boosting Classifier. You can also try with XGBoost Classifier (need to be installed first).\n",
    "4. Perform feature selection using the Gradient Boosting Classifier and evaluate the performance of the model using only the top-k most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.923775181024132\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train a GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and calculate the AUC\n",
    "y_pred = gb.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trees=50, learning_rate=0.01, max_depth=3, accuracy=0.81\n",
      "n_trees=50, learning_rate=0.01, max_depth=5, accuracy=0.81\n",
      "n_trees=50, learning_rate=0.01, max_depth=7, accuracy=0.82\n",
      "n_trees=50, learning_rate=0.1, max_depth=3, accuracy=0.86\n",
      "n_trees=50, learning_rate=0.1, max_depth=5, accuracy=0.87\n",
      "n_trees=50, learning_rate=0.1, max_depth=7, accuracy=0.88\n",
      "n_trees=50, learning_rate=1, max_depth=3, accuracy=0.87\n",
      "n_trees=50, learning_rate=1, max_depth=5, accuracy=0.86\n",
      "n_trees=50, learning_rate=1, max_depth=7, accuracy=0.84\n",
      "n_trees=100, learning_rate=0.01, max_depth=3, accuracy=0.84\n",
      "n_trees=100, learning_rate=0.01, max_depth=5, accuracy=0.85\n",
      "n_trees=100, learning_rate=0.01, max_depth=7, accuracy=0.86\n",
      "n_trees=100, learning_rate=0.1, max_depth=3, accuracy=0.87\n",
      "n_trees=100, learning_rate=0.1, max_depth=5, accuracy=0.88\n",
      "n_trees=100, learning_rate=0.1, max_depth=7, accuracy=0.88\n",
      "n_trees=100, learning_rate=1, max_depth=3, accuracy=0.87\n",
      "n_trees=100, learning_rate=1, max_depth=5, accuracy=0.85\n",
      "n_trees=100, learning_rate=1, max_depth=7, accuracy=0.84\n",
      "n_trees=200, learning_rate=0.01, max_depth=3, accuracy=0.85\n",
      "n_trees=200, learning_rate=0.01, max_depth=5, accuracy=0.86\n",
      "n_trees=200, learning_rate=0.01, max_depth=7, accuracy=0.86\n",
      "n_trees=200, learning_rate=0.1, max_depth=3, accuracy=0.88\n",
      "n_trees=200, learning_rate=0.1, max_depth=5, accuracy=0.88\n",
      "n_trees=200, learning_rate=0.1, max_depth=7, accuracy=0.87\n",
      "n_trees=200, learning_rate=1, max_depth=3, accuracy=0.86\n",
      "n_trees=200, learning_rate=1, max_depth=5, accuracy=0.85\n",
      "n_trees=200, learning_rate=1, max_depth=7, accuracy=0.85\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Experiment with different hyperparameters\n",
    "for n_trees in [50, 100, 200]:\n",
    "    for learning_rate in [0.01, 0.1, 1]:\n",
    "        for max_depth in [3, 5, 7]:\n",
    "            # Train a GradientBoostingClassifier\n",
    "            gb = GradientBoostingClassifier(n_estimators=n_trees, learning_rate=learning_rate, max_depth=max_depth)\n",
    "            gb.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions on the test set\n",
    "            y_pred = gb.predict(X_test)\n",
    "\n",
    "            # Evaluate the accuracy of the model\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f'n_trees={n_trees}, learning_rate={learning_rate}, max_depth={max_depth}, accuracy={accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier accuracy: 0.85\n",
      "AdaBoost Classifier accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train an AdaBoostClassifier\n",
    "ab = AdaBoostClassifier()\n",
    "ab.fit(X_train, y_train)\n",
    "\n",
    "# Train the XGBClassiifer\n",
    "#xgb = XGBClassifier()\n",
    "#xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compare the performance of the two models\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "\n",
    "#y_pred_xgb = xgb.predict(X_test)\n",
    "#accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'Gradient Boosting Classifier accuracy: {accuracy_gb:.2f}')\n",
    "print(f'AdaBoost Classifier accuracy: {accuracy_ab:.2f}')\n",
    "#print(f'XGBoost Classifier accuracy: {accuracy_xgb:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom loss function accuracy: 0.87\n",
      "Default loss function accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4\n",
    "\n",
    "# Train a GradientBoostingClassifier with custom loss function\n",
    "gb_custom = GradientBoostingClassifier(loss='exponential')\n",
    "gb_custom.fit(X_train, y_train)\n",
    "\n",
    "# Train a GradientBoostingClassifier with default loss function\n",
    "gb_default = GradientBoostingClassifier()\n",
    "gb_default.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compare the performance of the two models\n",
    "y_pred_custom = gb_custom.predict(X_test)\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "\n",
    "y_pred_default = gb_default.predict(X_test)\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "\n",
    "print(f'Custom loss function accuracy: {accuracy_custom:.2f}')\n",
    "print(f'Default loss function accuracy: {accuracy_default:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vegardlarsen/opt/anaconda3/envs/GRA4160/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/vegardlarsen/opt/anaconda3/envs/GRA4160/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with top-5 most important features: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Perform feature selection using the GradientBoostingClassifier\n",
    "fs = SelectFromModel(gb, prefit=True, max_features=5)\n",
    "X_train_fs = fs.transform(X_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "\n",
    "# Train a GradientBoostingClassifier with the selected features\n",
    "gb_fs = GradientBoostingClassifier()\n",
    "gb_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "# Make predictions on the test set and evaluate the performance of the model\n",
    "y_pred_fs = gb_fs.predict(X_test_fs)\n",
    "accuracy_fs = accuracy_score(y_test, y_pred_fs)\n",
    "print(f'Accuracy with top-5 most important features: {accuracy_fs:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of SelectFromModel(estimator=GradientBoostingClassifier(learning_rate=1,\n",
       "                                                     max_depth=7,\n",
       "                                                     n_estimators=200),\n",
       "                max_features=5, prefit=True)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
