{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Model selection, evaluation, and assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **k-Nearest Neighbors and Cross-Validation**\n",
    "   - **Task**: Use cross-validation to evaluate the performance of a $k$-nearest neighbors (kNN) model trained on the Iris dataset. Vary the number of neighbors (for example, from $k = 1 $ to $k = 15$) and compare the resulting cross-validation scores.\n",
    "     - **Questions**:\n",
    "       1. Which value of $k$ gives the best performance?\n",
    "       2. How does the value of $k$ affect the model’s bias and variance?\n",
    "       3. When you examine the cross-validation scores, what do they tell you about the stability of the model for different $k$ values?\n",
    "\n",
    "2. **Comparing Multiple Models with Accuracy and Confusion Matrix**\n",
    "   - **Task**: Train different classification models on the Iris dataset (e.g., kNN, Decision Tree, Logistic Regression, etc.) and compare their performance using:\n",
    "     - Accuracy\n",
    "     - Confusion matrix\n",
    "   - **Questions**:\n",
    "     1. Which model achieves the highest accuracy?\n",
    "     2. By looking at the confusion matrices for each model, can you identify which classes (species) are most often misclassified? \n",
    "     3. How does the confusion matrix give you deeper insight into the types of errors each model makes (e.g., mixing up similar classes)?\n",
    "\n",
    "3. **k-Fold Cross-Validation with Multiple Models**\n",
    "   - **Task**: Perform $k$-fold cross-validation on the Iris dataset and compare the performance of several models (e.g., kNN, Decision Trees, Logistic Regression, SVM).\n",
    "   - **Questions**:\n",
    "     1. Which model has the highest cross-validation accuracy on average?\n",
    "     2. How does the cross-validation process help mitigate issues like overfitting and give a more reliable estimate of model performance?\n",
    "     3. If two models have similar average performance, what additional criteria could you use to select one model over the other (e.g., interpretability, training time, etc.)?\n",
    "\n",
    "4. **Hyperparameter Tuning with Decision Trees**\n",
    "   - **Task**: Train a Decision Tree on the Iris dataset. Use `GridSearchCV` to find the best hyperparameters for `max_depth`, `min_samples_leaf`, and `min_samples_split`.\n",
    "   - **Questions**:\n",
    "     1. What is the model accuracy (on a held-out test set or via cross-validation) with the optimized hyperparameters?\n",
    "     2. How do changes in hyperparameters (like `max_depth`) reflect the trade-off between underfitting and overfitting?\n",
    "     3. Beyond accuracy, what other model outputs or diagnostics (e.g., feature importance) can help you interpret the model’s behavior and performance?\n",
    "\n",
    "5. **ROC Curves and Precision-Recall Curves**\n",
    "   - **Task**: Review the following links from scikit-learn documentation:\n",
    "     - [Scikit-learn: ROC curves](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)\n",
    "     - [Scikit-learn: Precision-Recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)\n",
    "   - **Questions**:\n",
    "     1. In a multiclass setting (like the Iris dataset), how can you adapt or interpret ROC and Precision-Recall curves? \n",
    "     2. What do the shapes of these curves tell you about the performance trade-offs at different decision thresholds?\n",
    "     3. When might a Precision-Recall curve be more informative than a ROC curve, and vice versa? \n",
    "     4. How would you interpret the area under the curve (AUC) in the context of this (relatively balanced) dataset?\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Interpretation and Discussion Points**:\n",
    "- **Class Overlap**: Iris classes can sometimes overlap in feature space. Are there any insights in the misclassified examples that suggest the features used (e.g., petal length vs. sepal width) are not always sufficient to separate classes?\n",
    "- **Practical Significance**: If the difference in performance between two models is very small, do you see any practical significance or can you justify choosing a simpler model for the sake of interpretability?\n",
    "- **Error Analysis**: Look at misclassified samples for each model. Is there a consistent pattern in these misclassifications (e.g., Setosa being misclassified as Versicolor)? What might this indicate about the feature distribution or class similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Range of k values to test\n",
    "k_values = range(1, 15)\n",
    "cv_scores = []  # List to store average cross-validation scores\n",
    "\n",
    "# Perform 5-fold cross-validation for each k\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Using 5-fold cross validation\n",
    "    scores = cross_val_score(knn, X, y, cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"k={k}: Cross-Validation Accuracy = {scores.mean():.3f}\")\n",
    "\n",
    "# Plot the cross-validation accuracy vs. k\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(k_values, cv_scores, marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('5-Fold CV Accuracy')\n",
    "plt.title('kNN: Varying Number of Neighbors')\n",
    "plt.xticks(k_values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'kNN (k=5))': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train, predict and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    results[name] = {'accuracy': acc, 'confusion_matrix': cm}\n",
    "    print(f\"{name} Accuracy: {acc:.3f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    \n",
    "    # Display the confusion matrix using a heatmap\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define additional model: Support Vector Machine (SVM)\n",
    "models_cv = {\n",
    "    'kNN': KNeighborsClassifier(n_neighbors=10),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "# Perform 5-fold cross-validation for each model\n",
    "for name, model in models_cv.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    cv_results[name] = scores.mean()\n",
    "    print(f\"{name} 5-Fold CV Accuracy: {scores.mean():.3f}\")\n",
    "\n",
    "# Optional: Display a bar plot of the results\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(cv_results.keys(), cv_results.values(), color='skyblue')\n",
    "plt.ylabel('5-Fold CV Accuracy')\n",
    "plt.title('Comparison of Multiple Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [None, 2, 3, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator to compute feature importance\n",
    "best_dtree = grid_search.best_estimator_\n",
    "print(\"Feature Importances:\", best_dtree.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
