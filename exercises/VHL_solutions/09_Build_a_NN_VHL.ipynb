{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginner's Guide to Deep Learning in PyTorch: Multi-Layer Networks, Mini-Batches, MNIST, L2 Regularization, and Dropout\n",
    "\n",
    "**Objective:**  \n",
    "In this tutorial, we'll work on the exercise from last week. We will build and train neural networks step by step using **PyTorch**. \n",
    "We will start with simple networks and gradually add complexity and improvements:\n",
    "1. Build a deeper neural network with multiple hidden layers (vs. a shallow one) and try different activation functions (Tanh, Sigmoid, ReLU).\n",
    "2. Implement **mini-batch gradient descent** instead of full-batch training and discuss efficiency vs. convergence.\n",
    "3. Train our network on the **MNIST dataset** (handwritten digit images) with proper data preprocessing for 10-class classification.\n",
    "4. Add **L2 regularization (weight decay)** to the loss to reduce overfitting and discuss its impact.\n",
    "5. Manually implement **Dropout** regularization in the network to improve generalization, ensuring it behaves correctly during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a Deeper Network with Multiple Hidden Layers\n",
    "\n",
    "Neural networks are composed of layers of interconnected neurons. \n",
    "A **shallow network** typically has only one hidden layer, whereas a **deep network** has multiple hidden layers. \n",
    "Deeper networks can capture more complex patterns, but they may be harder to train. \n",
    "\n",
    "### Why Multiple Hidden Layers?\n",
    "- A network with **one hidden layer** (a single-layer network) can approximate many functions but might require a very large number of neurons to do so.\n",
    "- **Multiple hidden layers** can learn hierarchical representations: each layer builds more abstract features from the previous one (for example, in image recognition, early layers learn edges, later layers learn object parts).\n",
    "- Deeper networks often can represent functions more efficiently (with fewer neurons) than a very wide single-layer network, given the same number of total neurons.\n",
    "\n",
    "However, deeper networks might be more prone to issues like vanishing gradients (especially with certain activation functions), and they require more data and computation to train effectively.\n",
    "\n",
    "### Activation Functions: ReLU, Tanh, Sigmoid\n",
    "**Activation functions** introduce non-linearity to networks, enabling them to learn complex patterns. Different hidden layers can use different activations. Common activation functions include:\n",
    "- **Sigmoid**: Outputs a value between 0 and 1. Historically used in early networks, but can saturate (gradients become very small for values near 0 or 1), which can slow training. Good for binary outputs or probabilistic interpretation, but less common in deep hidden layers now.\n",
    "- **Tanh**: Outputs between -1 and 1. It's zero-centered (unlike Sigmoid) which can be advantageous. But it also saturates at the extremes, causing vanishing gradients for deep networks.\n",
    "- **ReLU (Rectified Linear Unit)**: Outputs 0 for negative inputs and a linear output for positive inputs. It doesn't saturate for positive values, which helps mitigate the vanishing gradient problem, and it tends to converge faster in practice. A downside is that neurons can \"die\" if they only output 0 (if inputs are always negative), but this is usually manageable.\n",
    "- *(There are others like Leaky ReLU, ELU, etc., but we'll focus on these basic ones.)*\n",
    "\n",
    "**Comparison:** In practice, ReLU is often a good default for hidden layers because of its training efficiency. \n",
    "Sigmoid and Tanh can still be used, especially in shallower networks or specific applications, but they may require careful tuning (like smaller learning rates) due to gradient saturation.\n",
    "\n",
    "### Defining a Neural Network in PyTorch\n",
    "We'll start by defining two simple neural network architectures to compare:\n",
    "- A **single-layer (shallow) network**: just an input layer directly to an output layer (no hidden layer). This is essentially a logistic regression model if used for classification.\n",
    "- A **multi-layer (deep) network**: one with two hidden layers. You can extend this idea to even more layers as needed.\n",
    "\n",
    "We'll use fully connected linear layers (`nn.Linear`) for this multi-layer perceptron (MLP). \n",
    "For now, let's use ReLU activation in hidden layers as a default (we will mention how to switch to Tanh or Sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "SingleLayerNet(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "TwoHiddenLayerNet(\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration (use GPU if available for faster training, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define a simple single-layer network (no hidden layers, just input -> output)\n",
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SingleLayerNet, self).__init__() # call the constructor of the parent class (nn.Module)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        # Note: No hidden layers, so no activation needed here (we'll apply softmax via loss for classification)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape [batch_size, input_size]\n",
    "        out = self.linear(x)  # linear output\n",
    "        return out\n",
    "\n",
    "# Define a deeper network with two hidden layers\n",
    "class TwoHiddenLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, activation_fn=nn.ReLU):\n",
    "        super(TwoHiddenLayerNet, self).__init__()\n",
    "        # Define layers\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
    "        # Activation function for hidden layers (we can pass nn.ReLU, nn.Tanh, nn.Sigmoid, etc.)\n",
    "        self.activation = activation_fn()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First hidden layer + activation\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)   # apply activation (ReLU/Tanh/Sigmoid as specified)\n",
    "        # Second hidden layer + activation\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        # Output layer (no activation here if we'll use CrossEntropyLoss which applies Softmax internally)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "\n",
    "# Example: Initialize networks for an input of size 784 (e.g., flattened 28x28 image) and 10 output classes.\n",
    "input_size = 784   # for MNIST images flattened\n",
    "output_size = 10   # for 10 classes (digits 0-9)\n",
    "hidden_size1 = 128  # number of neurons in first hidden layer\n",
    "hidden_size2 = 64   # number of neurons in second hidden layer\n",
    "\n",
    "model_shallow = SingleLayerNet(input_size, output_size).to(device)\n",
    "# Available activation functions: nn.ReLU, nn.Tanh, nn.Sigmoid\n",
    "model_deep = TwoHiddenLayerNet(input_size, hidden_size1, hidden_size2, output_size, activation_fn=nn.ReLU).to(device)\n",
    "\n",
    "print(model_shallow)\n",
    "print(model_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above:\n",
    "- `SingleLayerNet` has just one `nn.Linear` layer. This will take the input and directly produce outputs. If we use it for classification with 10 classes, it will produce 10 scores (logits) for each class.\n",
    "- `TwoHiddenLayerNet` has two hidden linear layers (`hidden1` and `hidden2`) and an `output_layer`. We apply an activation function after each hidden layer. By default, we use `ReLU` (by passing `nn.ReLU` to the `activation_fn` argument), but we could replace it with `nn.Tanh` or `nn.Sigmoid` when instantiating the model to experiment with those activations.\n",
    "- We do not apply an activation on the output layer because we'll use `CrossEntropyLoss` for training, which expects raw scores for each class (and internally applies Softmax when computing the loss).\n",
    "\n",
    "After defining the models, we moved them to the chosen `device` (CPU or GPU). We then printed the model architectures to see the layers.\n",
    "\n",
    "### Comparing Shallow vs Deep Network Performance\n",
    "To compare, we'd train both a shallow and a deep network on the same task and see the results:\n",
    "- The **shallow network** (no hidden layer) is essentially a linear classifier. It may not fit complex patterns in data if the data is not linearly separable.\n",
    "- The **deep network** (with hidden layers) can fit more complex functions due to the additional layers and non-linear activations.\n",
    "\n",
    "**Expectations:** On a complex dataset like MNIST (10-class digit images), a deep network will usually achieve higher training accuracy and lower loss than a shallow one, because the deep network can model non-linear relationships better. The shallow network might converge faster initially (fewer parameters to adjust) but will likely plateau at a lower accuracy. The deep network, with more parameters and layers, might take a bit longer to train per epoch but can reach a higher accuracy given enough training. We'll see this in practice in section 3 when we train on MNIST.\n",
    "\n",
    "We will hold off actual training until we introduce the data in section 3. For now, we have our models defined and ready to use. Next, we will discuss using **mini-batch gradient descent** to train these networks efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Implementing Mini-Batch Gradient Descent\n",
    "\n",
    "When training neural networks, we use **gradient descent** to optimize the model's parameters (weights). There are a few variations of how we can compute the gradients over the training data:\n",
    "- **Full-batch (Batch) Gradient Descent:** Use *all training examples* to compute the loss and gradient, then update the weights. This means one very large batch (the entire dataset) per update.\n",
    "- **Stochastic Gradient Descent (SGD):** Use a *single training example* to compute the loss and gradient, update the weights immediately for each training example. This can be very noisy but updates happen very frequently.\n",
    "- **Mini-Batch Gradient Descent:** Use a **small batch of training examples** (e.g., 16, 32, 64 samples) to compute the loss and gradient, then update weights. This is the most commonly used approach as it balances efficiency and stability.\n",
    "\n",
    "### Why Mini-Batches?\n",
    "- Using the **entire dataset** for each update (full-batch) can be very slow if the dataset is large, and it doesn't provide more frequent feedback to the model about the direction to move.\n",
    "- Using a **single example** for each update (SGD) is fast per update, but the direction of the gradient is very noisy and can jump around, sometimes causing instability or requiring more updates to converge.\n",
    "- **Mini-batches** strike a good balance:\n",
    "  - They allow vectorized operations on multiple samples at once, which is computationally efficient (especially on GPUs, which excel at parallel operations).\n",
    "  - The gradient computed on a batch is a good approximation of the gradient on the full dataset, but with some noise that can help escape local minima and prevent certain kinds of overfitting.\n",
    "  - By adjusting the batch size, we can tune the trade-off: larger batches = more stable, accurate gradient directions (but more memory and possibly slower per update), smaller batches = more noise but faster updates and less memory.\n",
    "\n",
    "**Trade-offs between batch sizes:**\n",
    "- *Computational Efficiency:* Larger batches make better use of hardware (especially GPUs) through parallelism, up to a point. Very small batches might not fully utilize the GPU.\n",
    "- *Convergence:* Noisy gradients (small batch) can help generalize but too much noise might hinder learning of precise patterns. Very large batches might converge to sharp minima and potentially generalize worse, and you get fewer weight updates for the same number of samples seen.\n",
    "- *Memory:* Larger batch sizes require more memory to store the data and intermediate activations. You may be limited by GPU memory, for example.\n",
    "\n",
    "In practice, common batch sizes are 16, 32, 64, 128, etc. Often 32 or 64 is a good start for many problems, but this can be tuned.\n",
    "\n",
    "### Implementing Mini-Batch Training Loop\n",
    "PyTorch makes mini-batch training easy with the `DataLoader` class, which automatically divides the dataset into batches for us. We'll see how to use `DataLoader` in the MNIST section. But to illustrate the concept, here's what a typical training loop with mini-batches looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Assume we have a DataLoader for training data: train_loader\n",
    "# Also assume we have a model, loss function (criterion), and optimizer defined.\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 32  # for example\n",
    "for epoch in range(num_epochs):\n",
    "    model_deep.train()  # set model to training mode (important for dropout, batch norm, etc.)\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        # Move batch to the device (CPU/GPU)\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        # If data is an image (N, 1, 28, 28 for MNIST), flatten it to (N, 784) for our MLP\n",
    "        data = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Forward pass: compute model output\n",
    "        outputs = model_deep(data)\n",
    "        loss = criterion(outputs, labels)  # compute loss for this batch\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        optimizer.zero_grad()    # zero out gradients from previous step\n",
    "        loss.backward()          # backpropagation: compute gradients of loss w.r.t. parameters\n",
    "        optimizer.step()         # update parameters using the optimizer (gradient descent step)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet:\n",
    "- We loop over each `epoch` (one pass through the dataset).\n",
    "- For each epoch, we loop over the `train_loader`, which yields mini-batches of `(data, labels)` pairs.\n",
    "- We move the data and labels to the chosen `device` (so that computation happens on GPU if available).\n",
    "- If dealing with images, we flatten each batch of images with `data.view(data.size(0), -1)`. For MNIST, each image is 28x28 = 784 features.\n",
    "- We perform the forward pass on the batch to get outputs, then compute the loss against the true labels for that batch.\n",
    "- We zero out previous gradients (`optimizer.zero_grad()`), do backprop (`loss.backward()`), and take an optimizer step (`optimizer.step()`) to update weights.\n",
    "- We accumulate the loss to compute an average loss for the epoch (just for logging).\n",
    "- We set `model_deep.train()` at the start of training to ensure the model is in training mode (this matters for layers like Dropout or BatchNorm, which behave differently in training vs evaluation).\n",
    "\n",
    "Using `DataLoader` automatically shuffles the data (if specified) and yields exactly `batch_size` samples per iteration (except possibly the last one if the dataset size isn't divisible by the batch size). This greatly simplifies our training loop.\n",
    "\n",
    "We'll implement this in practice for MNIST in the next section. For now, remember:\n",
    "- Changing `batch_size` in the DataLoader is how you experiment with different mini-batch sizes (e.g., 16, 32, 64). It's usually as simple as passing a different `batch_size` argument when creating the DataLoader.\n",
    "- If `batch_size = len(dataset)`, you're effectively doing full-batch gradient descent.\n",
    "- If `batch_size = 1`, you're doing (close to) stochastic gradient descent.\n",
    "\n",
    "Now that we understand mini-batches, let's apply what we've learned by training our network on a real dataset: **MNIST**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training on the MNIST Dataset\n",
    "\n",
    "The **MNIST dataset** is a classic benchmark in machine learning. It consists of 60,000 training images and 10,000 test images of handwritten digits (0 through 9), each image being 28x28 pixels in grayscale. Our task is to classify each image into the correct digit.\n",
    "\n",
    "### Data Loading and Preprocessing\n",
    "PyTorch provides convenient tools to load common datasets like MNIST through `torchvision.datasets`. We will:\n",
    "- Download/load the MNIST dataset.\n",
    "- Transform the images into tensor format and normalize the pixel values.\n",
    "- Flatten each 28x28 image into a 784-dimensional vector, since our neural network expects a 1D feature vector input.\n",
    "- Prepare DataLoader objects for the training set and test set with a chosen batch size.\n",
    "\n",
    "Normalization: Neural networks train faster when input features are scaled. Pixel values in MNIST are 0 to 255. We will normalize them to a 0-1 range by converting images to tensors of type float and dividing by 255 (the `ToTensor()` transform in PyTorch does this automatically: it converts images to tensors and scales pixels to [0,1]). We could further normalize to have mean 0 and variance 1 (like subtract 0.1307 and divide by 0.3081, which are the mean and std of MNIST), but a simpler 0-1 scaling is sufficient for this demonstration. \n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transforms: convert images to tensor and normalize to [0,1]. \n",
    "# Also, we won't flatten here; we'll do that in the training loop for clarity.\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the MNIST training and test datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Define DataLoaders for training and testing\n",
    "batch_size = 32  # we can experiment with 16, 32, 64, etc.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above:\n",
    "- We set `transform = transforms.ToTensor()` which means each image will be converted to a PyTorch tensor and scaled to [0, 1].\n",
    "- `datasets.MNIST` is used to download (if not already) and load the dataset. We specify `train=True` for the training set and `train=False` for the test set.\n",
    "- `DataLoader` is used to create an iterable over the dataset. We set `shuffle=True` for the training loader to shuffle the data each epoch (improving training randomness) and `shuffle=False` for test (not necessary to shuffle test data).\n",
    "- `batch_size` is set to 32 as an example. You can change this and observe differences in training speed or performance.\n",
    "\n",
    "### Adjusting the Network for 10 Output Classes\n",
    "Our network architecture needs to have an output size equal to the number of classes (10 for digits 0-9). In the earlier section, we already defined `output_size = 10`. If you were previously using a network for a different task (say binary classification with 1 output or 2 outputs), you would change it to 10 outputs for MNIST.\n",
    "\n",
    "We will use the `TwoHiddenLayerNet` defined earlier as our model for MNIST, since a deeper network should perform better than a single-layer one on this task. We already instantiated `model_deep` with `output_size=10`. Let's re-initialize it to make sure we start with a fresh model (since we haven't actually trained it yet) and choose an appropriate loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize a fresh model for training on MNIST\n",
    "model = TwoHiddenLayerNet(input_size, hidden_size1, hidden_size2, output_size, activation_fn=nn.ReLU).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # for multi-class classification, CrossEntropyLoss is appropriate (it applies Softmax internally)\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# PyTorch's torch.optim.SGD isn't strictly stochastic gradient descent by definition.\n",
    "# Whether it's \"batch,\" \"mini-batch,\" or \"stochastic\" depends on how much data is provided per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the choices:\n",
    "- We use `nn.CrossEntropyLoss` for multi-class classification. This loss function expects raw logits from the model and true class labels. It will compute softmax internally and then compute the negative log-likelihood. So our model's output should be of shape `[batch_size, 10]` (score for each class) and labels of shape `[batch_size]` with class indices 0-9.\n",
    "- We chose Stochastic Gradient Descent (SGD) as the optimizer with a learning rate of 0.1. This is a fairly high learning rate for MNIST; we might reduce it or use a more advanced optimizer like Adam for faster convergence, but let's start simple. (We can adjust if needed.)\n",
    "- `optimizer = torch.optim.SGD(model.parameters(), lr=0.1)` will update our model's parameters using the gradients computed. We did not set `momentum` or `weight_decay` here; momentum can accelerate SGD and weight_decay is for L2 regularization (which we'll add separately later).\n",
    "\n",
    "### Training the Model on MNIST\n",
    "Now we'll train the model for a few epochs and monitor the performance. For each epoch, we'll compute the average training loss and also compute accuracy on the training set (and later on the test set) to see how well the model is doing.\n",
    "\n",
    "We'll also compare how our model is doing against what a shallow model would achieve, to highlight the difference. First, let's train the `model` (which is our two-hidden-layer network with ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 0.3651, Training Accuracy: 89.02%\n",
      "Epoch [2/5], Training Loss: 0.1302, Training Accuracy: 96.08%\n",
      "Epoch [3/5], Training Loss: 0.0890, Training Accuracy: 97.27%\n",
      "Epoch [4/5], Training Loss: 0.0685, Training Accuracy: 97.85%\n",
      "Epoch [5/5], Training Loss: 0.0540, Training Accuracy: 98.30%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # make sure to set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Move data to device (CPU or GPU)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Flatten the images into vectors of size 784\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)             # shape: [batch_size, 10]\n",
    "        loss = criterion(outputs, labels)   # compute loss for this batch\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()   # clear previous gradients\n",
    "        loss.backward()         # compute gradients\n",
    "        optimizer.step()        # update weights\n",
    "        \n",
    "        # Accumulate training statistics\n",
    "        running_loss += loss.item() * images.size(0)  # loss.item() is average loss per sample in the batch\n",
    "        _, predicted = torch.max(outputs, 1)          # predicted class is the index of max logit\n",
    "        correct += (predicted == labels).sum().item() # count how many predictions were correct\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    # Compute average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each epoch:\n",
    "- We set `model.train()` to ensure dropout (if any) is active and other layers behave in training mode.\n",
    "- We iterate over `train_loader` to get mini-batches of `images` and `labels`.\n",
    "- We flatten `images` from shape `(batch_size, 1, 28, 28)` to `(batch_size, 784)` to feed into our linear layers.\n",
    "- We compute `outputs = model(images)` and then `loss = criterion(outputs, labels)`.\n",
    "- We perform backpropagation and an optimizer step to update the model's parameters.\n",
    "- We also track the `running_loss` (accumulating the loss of each batch, weighted by batch size to later compute average loss) and count correct predictions to compute accuracy.\n",
    "- After the inner loop, we calculate `epoch_loss` and `epoch_acc` for the whole training dataset and print them.\n",
    "\n",
    "After training for a few epochs, we expect the training loss to decrease and training accuracy to increase. A well-configured two-layer network should reach a high accuracy on MNIST (potentially above 95% on training within 5 epochs if learning rate is okay and hidden sizes are sufficient).\n",
    "\n",
    "Let's evaluate the model on the **test dataset** to see how well it generalizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0834, Test Accuracy: 97.40%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "model.eval()  # set model to evaluation mode (important for dropout, batchnorm, etc.)\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():  # disable gradient computation for efficiency\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)  # flatten\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "avg_test_loss = test_loss / test_total\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we:\n",
    "- Set `model.eval()` to notify layers like Dropout (if present) to behave in evaluation mode (i.e., not drop neurons).\n",
    "- Use `torch.no_grad()` to skip gradient calculations since we're only doing forward passes (this makes it faster and uses less memory).\n",
    "- Loop through the test data and compute the accumulated loss and correct predictions similar to training, but without updating the model.\n",
    "- Calculate `avg_test_loss` and `test_accuracy`.\n",
    "\n",
    "**Interpreting Results:** We should see a test accuracy that is hopefully close to the training accuracy if the model generalizes well. If the training accuracy is significantly higher than test accuracy, it might indicate overfitting (the model memorized training data patterns that don't generalize to new data). We will address overfitting using regularization in the next sections.\n",
    "\n",
    "Now, as an additional comparison, let's see how a **shallow network (SingleLayerNet)** would perform on the same task. We can train the `SingleLayerNet` for a few epochs and compare its accuracy to the `TwoHiddenLayerNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Shallow Net Training Accuracy: 88.69%\n",
      "Epoch [2/5], Shallow Net Training Accuracy: 91.25%\n",
      "Epoch [3/5], Shallow Net Training Accuracy: 91.63%\n",
      "Epoch [4/5], Shallow Net Training Accuracy: 91.92%\n",
      "Epoch [5/5], Shallow Net Training Accuracy: 92.21%\n",
      "Shallow Net Test Accuracy: 92.26%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a single-layer model (logistic regression) for comparison\n",
    "shallow_model = SingleLayerNet(input_size, output_size).to(device)\n",
    "optimizer_shallow = torch.optim.SGD(shallow_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the shallow model for a few epochs\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    shallow_model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = shallow_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer_shallow.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_shallow.step()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Shallow Net Training Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# Evaluate shallow model on test data\n",
    "shallow_model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = shallow_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "test_acc = 100 * test_correct / test_total\n",
    "print(f\"Shallow Net Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't double print losses for brevity, focusing on accuracy:\n",
    "- This trains a logistic regression model (no hidden layers). We expect it to do reasonably well on MNIST (likely achieving ~88-93% test accuracy because linear models can capture a lot of the variance in digit images, but not as much as a multi-layer network).\n",
    "- By comparing the results of `shallow_model` vs `model` (deep network), you should observe:\n",
    "  - The deep network reaches a higher accuracy than the shallow model.\n",
    "  - The shallow model might train a bit faster initially (fewer parameters, simpler model), but its performance plateaus earlier and lower.\n",
    "  - This confirms that multiple hidden layers (with non-linear activations) give the network more representational power to solve the task.\n",
    "\n",
    "So far, we've trained a multi-layer network on MNIST and improved performance over a single-layer network. Next, we'll look at ways to further improve generalization and training through regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding L2 Regularization (Weight Decay)\n",
    "\n",
    "Our deep network likely has a lot of parameters (weights). For example, with input_size=784, hidden sizes 128 and 64, and output 10, the number of parameters is:\n",
    "- Hidden1 layer: 784 x 128 weights + 128 biases\n",
    "- Hidden2 layer: 128 x 64 weights + 64 biases\n",
    "- Output layer: 64 x 10 weights + 10 biases  \n",
    "That’s a total of many thousands of parameters. With a large number of parameters, a network can fit the training data very closely, sometimes even memorizing it. This can lead to **overfitting**, where the model performs much worse on new, unseen data (test set) than on the training set.\n",
    "\n",
    "**L2 Regularization**, also known as **weight decay**, is one way to combat overfitting:\n",
    "- The idea is to add an extra term to the loss function that penalizes large weights. Specifically, for each weight $w$, we add $\\frac{\\lambda}{2} w^2$ (or just $\\lambda w^2$ depending on convention) to the loss, where $\\lambda$ is a small positive number (the **regularization strength** or coefficient).\n",
    "- This encourages the network to keep the weights small. Smaller weights generally produce smoother and less complex mappings from inputs to outputs, which often generalize better.\n",
    "- Effect: The model will try to make a trade-off between fitting the data well (minimizing the original loss) and keeping weights small (minimizing the regularization term). This can prevent any single weight from growing too large to fit some noise or outlier perfectly.\n",
    "\n",
    "In practice, adding L2 regularization tends to:\n",
    "- **Reduce overfitting:** The gap between training and test performance often shrinks. Test accuracy may improve if the model was overfitting.\n",
    "- **Slightly increase training loss:** Because we added an extra penalty, the optimizer might not be able to drive the training loss as low as before (which is fine if the test loss improves).\n",
    "- **Potentially slow training convergence:** The optimizer has to also minimize the regularization term, which can make it a bit slower to reach minimum on the original loss. Often, this effect is minor if $\\lambda$ is small.\n",
    "\n",
    "### Implementing L2 Regularization in PyTorch\n",
    "There are two common ways to add L2 regularization in PyTorch:\n",
    "1. **Manual addition to the loss function:** After computing the normal loss, compute the sum of squares of all weights and add $\\lambda$ times that to the loss.\n",
    "2. **Using `weight_decay` parameter in the optimizer:** Many PyTorch optimizers (SGD, Adam, etc.) have a `weight_decay` argument which, if set to a non-zero value, automatically adds L2 penalty to the weights during the update step. This is convenient and functionally equivalent.\n",
    "\n",
    "For educational purposes, we'll show the manual method as it makes the concept clear. Let's take our training loop and add L2 regularization to it. We'll use a new model instance to compare training with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss (no reg term): 0.3730\n",
      "Epoch [2/5], Training Loss (no reg term): 0.1295\n",
      "Epoch [3/5], Training Loss (no reg term): 0.0902\n",
      "Epoch [4/5], Training Loss (no reg term): 0.0702\n",
      "Epoch [5/5], Training Loss (no reg term): 0.0559\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize a new model for a fair comparison (same architecture)\n",
    "model_reg = TwoHiddenLayerNet(input_size, hidden_size1, hidden_size2, output_size, activation_fn=nn.ReLU).to(device)\n",
    "optimizer_reg = torch.optim.SGD(model_reg.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set L2 regularization coefficient (lambda)\n",
    "lambda_reg = 1e-4  # you can experiment with values like 1e-3, 1e-4, 1e-5, etc.\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model_reg.train()\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        outputs = model_reg(images)\n",
    "        base_loss = criterion(outputs, labels)  # standard data loss\n",
    "        # Manual L2 loss: sum of squared weights (we won't include biases to be more standard, but that's a minor detail)\n",
    "        l2_loss = 0.0\n",
    "        for param in model_reg.parameters():\n",
    "            # Only add weight parameters (skip biases). In PyTorch, biases are usually one-dimensional.\n",
    "            # We'll include everything for simplicity, it's a small effect whether biases are included or not.\n",
    "            l2_loss += torch.sum(param.pow(2))\n",
    "        # Add the L2 penalty to the base loss\n",
    "        loss = base_loss + lambda_reg * l2_loss\n",
    "        \n",
    "        optimizer_reg.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_reg.step()\n",
    "        total_loss += base_loss.item() * images.size(0)  # note: logging base_loss (data loss) for transparency\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss (no reg term): {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did:\n",
    "- Set `lambda_reg = 1e-4` as the regularization strength. This is a common scale for weight decay; if we set it too high (e.g., 0.1), the model might underfit because weights are forced to be extremely small.\n",
    "- In each training iteration, we calculated `base_loss` as usual, then computed `l2_loss` by iterating over `model_reg.parameters()` and summing up `param.pow(2)`. This gives $\\sum w^2$ for all weights.\n",
    "- We then formed the final `loss = base_loss + lambda_reg * l2_loss`.\n",
    "- Notice we added the regularization *after* computing base_loss, and we used `base_loss.item()` for logging the part of loss without regularization, just to see how the data fitting is going. The actual loss used for backward includes the L2 term.\n",
    "- We could skip biases (which are 1D parameters) by checking `param.ndim > 1` (2D weights) to only penalize those, but including biases typically doesn't hurt if lambda is small (biases are far fewer parameters).\n",
    "\n",
    "We would train and then evaluate `model_reg` on the test set similar to before to see the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with L2 regularization: 97.65%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the L2-regularized model\n",
    "model_reg.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = model_reg(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "test_acc_reg = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy with L2 regularization: {test_acc_reg:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected impact of L2 regularization:** \n",
    "- The training loss printed might be slightly higher than without regularization (because part of it, not displayed in `base_loss.item()`, is coming from L2 penalty).\n",
    "- The final training accuracy might be a bit lower than the non-regularized model if the original model was able to nearly perfectly fit the training data by making weights large.\n",
    "- The test accuracy might be higher than the non-regularized model *if* the original was overfitting. If the original model wasn't heavily overfitting, the difference may be small. But if there was a noticeable gap (say, training acc 99% vs test acc 94%), adding weight decay could improve test accuracy by a couple of points while slightly reducing training accuracy, narrowing the gap.\n",
    "\n",
    "**Tuning $\\lambda$:** If $\\lambda$ is too high, the model will underfit (too much pressure to keep weights near zero, so it can't even fit the training data well). If $\\lambda$ is too low, it might not make any noticeable difference. A good practice is to try a few values (e.g., 1e-3, 1e-4, 1e-5) to see which yields the best validation performance.\n",
    "\n",
    "Now that we've addressed overfitting via weight decay, let's introduce another powerful regularization technique: **Dropout**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manually Implementing Dropout\n",
    "\n",
    "**Dropout** is a regularization technique that reduces overfitting by *randomly dropping units (neurons) during training*. \n",
    "It's very effective for large neural networks. \n",
    "Here's how it works conceptually:\n",
    "- During each training forward pass, each neuron (in certain layers, usually the hidden layers) has a probability *p* of being \"dropped out,\" meaning its output is set to zero for that pass.\n",
    "- This forces the network to not rely too heavily on any single neuron, because that neuron might be gone in the next training step. It's like training an ensemble of many smaller networks that each omit different neurons, and they all share weights.\n",
    "- At training time, when we drop a neuron with probability *p*, we *scale up* the remaining active neurons' outputs by *1/(1-p)*. This is called **inverted dropout** and it ensures that the *expected* sum of inputs to the next layer is the same as it would be without dropout. (Another way to see it: at test time, we use all neurons but would scale their outputs by (1-p) since none are dropped. Inverted dropout does the scaling during training instead, so we don't have to scale at test time.)\n",
    "- During inference (evaluation), we **turn off dropout** (no neurons are dropped). We want the full network's capacity when making predictions. The randomness is only for training to introduce robustness.\n",
    "\n",
    "**Impact of Dropout:**\n",
    "- It adds noise to the training process (the network sees a slightly different architecture each time), which can slow down training convergence somewhat.\n",
    "- It greatly helps in preventing overfitting, especially in large networks, since neurons can't co-adapt to each other as much.\n",
    "- With dropout, you often might need to train for more epochs or with a slightly higher learning rate to compensate for the noise, but the end result is a model that generalizes better.\n",
    "\n",
    "### Implementing Dropout Without PyTorch's Built-in Layers\n",
    "PyTorch provides `nn.Dropout` modules that one can insert into a network, but here we'll implement it manually to see what's happening under the hood:\n",
    "- We will manually zero-out random neurons' outputs in the forward pass during training.\n",
    "- We'll ensure to multiply the remaining outputs by `1/(1-p)` to maintain the scale.\n",
    "- We'll use `self.training` flag inside the `forward` method to check if the model is in training mode or eval mode. (`model.train()` sets `self.training=True`, `model.eval()` sets it to False).\n",
    "\n",
    "Let's define a new network class similar to our `TwoHiddenLayerNet` but with dropout in the hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoHiddenLayerNetWithDropout(\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TwoHiddenLayerNetWithDropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.5):\n",
    "        super(TwoHiddenLayerNetWithDropout, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
    "        self.activation = nn.ReLU()       # using ReLU for hidden layers\n",
    "        self.dropout_prob = dropout_prob  # dropout probability (p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Hidden layer 1\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        if self.training:  # only apply dropout during training\n",
    "            # Create a dropout mask with the same shape as x\n",
    "            # Mask has 0s with probability dropout_prob, 1s with probability (1 - dropout_prob)\n",
    "            mask = (torch.rand_like(x) > self.dropout_prob).float()\n",
    "            # Apply mask: zero out some activations\n",
    "            x = x * mask\n",
    "            # Scale up the remaining activations to account for dropout (inverted dropout technique)\n",
    "            x = x / (1.0 - self.dropout_prob)\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        if self.training:\n",
    "            mask = (torch.rand_like(x) > self.dropout_prob).float()\n",
    "            x = x * mask\n",
    "            x = x / (1.0 - self.dropout_prob)\n",
    "        \n",
    "        # Output layer (no dropout here, typically we only dropout in hidden layers)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "\n",
    "# Initialize the network with dropout\n",
    "dropout_prob = 0.5  # 50% dropout, a common choice\n",
    "model_dropout = TwoHiddenLayerNetWithDropout(input_size, hidden_size1, hidden_size2, output_size, dropout_prob).to(device)\n",
    "print(model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class:\n",
    "- We added a `dropout_prob` attribute to store the dropout probability *p*.\n",
    "- After computing the activation of each hidden layer, we apply dropout:\n",
    "  - We create a `mask` the same shape as the layer's output using `torch.rand_like(x) > p`. `torch.rand_like(x)` generates a tensor of random values in [0,1] with the same shape as `x`. Comparing it to `p` gives a boolean mask that's True (1) with probability (1-p) and False (0) with probability p. We convert that to float (so we have 1s and 0s).\n",
    "  - We multiply `x` by this `mask`, zeroing out a fraction `p` of the elements of `x`.\n",
    "  - We then divide `x` by (1-p) to scale up the remaining active neurons' outputs.\n",
    "- We only do this when `self.training` is True, meaning the model is in training mode. In evaluation mode (`model.eval()` called), the `if self.training` blocks will be skipped, so no dropout will be applied and outputs won't be scaled (effectively, it's as if we are using the whole network with weights scaled properly already).\n",
    "- We choose not to apply dropout on the output layer in this implementation. Dropout is usually applied to hidden layers. Dropping out units in the output layer (just before softmax) is less common and can hurt performance because the output layer directly corresponds to predictions. So we typically keep the output layer intact.\n",
    "\n",
    "Now, let's train this network with dropout on MNIST and see how it does compared to before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 0.6326, Training Accuracy: 80.54%\n",
      "Epoch [2/5], Training Loss: 0.3421, Training Accuracy: 90.24%\n",
      "Epoch [3/5], Training Loss: 0.2903, Training Accuracy: 91.84%\n",
      "Epoch [4/5], Training Loss: 0.2606, Training Accuracy: 92.52%\n",
      "Epoch [5/5], Training Loss: 0.2444, Training Accuracy: 93.03%\n"
     ]
    }
   ],
   "source": [
    "# Train the model with dropout\n",
    "model_dropout.train()\n",
    "optimizer_do = torch.optim.SGD(model_dropout.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model_dropout.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        outputs = model_dropout(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer_do.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_do.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then evaluate on the test set as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Dropout: 96.66%\n"
     ]
    }
   ],
   "source": [
    "model_dropout.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = model_dropout(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "test_acc_do = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy with Dropout: {test_acc_do:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect with Dropout:** \n",
    "- The training accuracy reported might be a bit lower than the model without dropout, because dropout is making the task harder on the training data (it's like training a bunch of smaller networks each epoch). For example, you might see the training accuracy rising slower or plateau a bit lower than 100%.\n",
    "- The test accuracy, however, could be higher than the non-dropout model if that model was overfitting. If the original model already generalized well, dropout might not boost test accuracy much, but if it was overfitting, dropout often gives a noticeable improvement.\n",
    "- Overall, you usually use dropout when you have a large network and a risk of overfitting. In our case, a two-hidden-layer network on MNIST might not strictly need dropout to reach good performance, but this is for learning purposes. On more complex tasks or deeper networks, dropout is very beneficial.\n",
    "\n",
    "**Important:** Remember to call `model_dropout.eval()` when evaluating; otherwise, dropout will remain active and you'll get inconsistent, random results on the test set (since it will drop random neurons even when you're trying to evaluate performance).\n",
    "\n",
    "### Combining L2 and Dropout\n",
    "You can use **both** L2 regularization and dropout together. They address overfitting in different ways: L2 softly penalizes complexity (large weights) and dropout adds noise and forces redundancy in the network. In practice, using both often yields a better result than either alone, especially in very deep networks.\n",
    "\n",
    "If using both:\n",
    "- You would keep the `lambda_reg` term in the loss or `weight_decay` in optimizer, and also include the dropout layers in the model.\n",
    "- Ensure to tune hyperparameters (learning rate might need adjustment since both regularizations can slow learning slightly).\n",
    "\n",
    "### Recap and Conclusions\n",
    "- We built a deep neural network with multiple hidden layers and saw that it can outperform a single-layer (shallow) network on a complex task.\n",
    "- We used different activation functions (ReLU as default, with mentions of how to use Sigmoid/Tanh) and discussed their impact on training.\n",
    "- We implemented mini-batch gradient descent to make training efficient and stable, and discussed how batch size affects training.\n",
    "- We trained on the MNIST dataset, handling data loading, preprocessing (flattening and normalization), and adjusted the network for multi-class output.\n",
    "- We added L2 regularization (weight decay) to the loss function to combat overfitting, and observed how it can improve generalization.\n",
    "- We manually implemented dropout in our network, dropping neurons during training to further improve the model's robustness and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
