{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested Solution: Customer Segmentation with Bank Marketing Data Set\n",
    "\n",
    "This notebook presents a suggested solution for the customer segmentation exercise using the Bank Marketing Data Set. In the following sections, we:\n",
    "\n",
    "- **Load and preprocess the data** by handling missing values, scaling numerical features, and encoding categorical features.\n",
    "- **Visualize the data** using plots to explore distributions and relationships.\n",
    "- **Perform clustering** using the $K$-Means algorithm, including the use of the elbow method and silhouette scores to choose the optimal number of clusters.\n",
    "- **Visualize clusters** with Principal Component Analysis (PCA) to reduce the data to 2D for easier interpretation.\n",
    "- **Interpret the clusters** by summarizing key features and discussing insights.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load the Bank Marketing Data Set (assumed to be in the file `bank-additional-full.csv`), handle missing values, normalize numerical features, and encode categorical features. Note that for clustering (an unsupervised method), the target variable (`y`) is dropped, although we keep it aside for later interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the dataset (ensure the file is in your working directory)\n",
    "# The file uses semicolon (;) as the separator\n",
    "df = pd.read_csv('../../data/bank_marketing_data_set/bank-additional/bank-additional-full.csv', sep=';')\n",
    "print('Dataset shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "The dataset uses the string `'unknown'` to represent missing values. Here, we replace `'unknown'` with `NaN` and then drop any rows containing missing values. In a production system, you might opt to impute missing values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'unknown' with NaN and drop rows with missing values\n",
    "df.replace('unknown', np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print('Dataset shape after dropping missing values:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Separation and Preprocessing\n",
    "\n",
    "Since our goal is to perform unsupervised clustering, we drop the target variable (`y`) from our feature set. We then identify numerical and categorical features, scale the numerical features using `StandardScaler`, and one-hot encode the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable from features (retain it for later interpretation)\n",
    "target = df['y']\n",
    "df_features = df.drop('y', axis=1)\n",
    "\n",
    "# Define the list of numeric columns based on dataset description\n",
    "numeric_cols = [\n",
    "    'age', 'duration', 'campaign', 'pdays', 'previous', \n",
    "    'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'\n",
    "]\n",
    "\n",
    "# All other columns are considered categorical\n",
    "categorical_cols = [col for col in df_features.columns if col not in numeric_cols]\n",
    "\n",
    "print('Numeric columns:', numeric_cols)\n",
    "print('Categorical columns:', categorical_cols)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "df_features[numeric_cols] = scaler.fit_transform(df_features[numeric_cols])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df_features_encoded = pd.get_dummies(df_features, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Final preprocessed dataset for clustering\n",
    "X = df_features_encoded\n",
    "print('Preprocessed feature set shape:', X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization\n",
    "\n",
    "Before clustering, it is helpful to visualize the data. Here, we plot the distribution of the numerical features and inspect the correlations among them. Such visualizations can reveal potential outliers and guide our understanding of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for numeric features\n",
    "X_numeric = pd.DataFrame(X[numeric_cols], columns=numeric_cols)\n",
    "X_numeric.hist(bins=30, figsize=(15, 10));\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot a correlation heatmap for numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(X_numeric.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering with $K$-Means\n",
    "\n",
    "We now apply the $K$-Means clustering algorithm to the preprocessed data. In order to determine a good number of clusters, we use the elbow method (plotting the Within-Cluster Sum of Squares, WCSS) and compute the silhouette score for each candidate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use the elbow method and silhouette scores to determine the optimal number of clusters\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)  # testing k from 2 to 10\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    labels = kmeans.labels_\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    print(f\"Clusters: {k}, WCSS: {kmeans.inertia_:.2f}, Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# Plot the elbow curve and silhouette scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(list(K_range), wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(list(K_range), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Based on the plots and scores, choose an optimal number of clusters (for example, k=3)\n",
    "optimal_k = 3\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans_optimal.fit_predict(X)\n",
    "print('Optimal number of clusters chosen:', optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster Visualization using PCA\n",
    "\n",
    "Since the dataset is high-dimensional, we use Principal Component Analysis (PCA) to reduce the data to 2 dimensions for visualization purposes. The following scatter plot shows the clusters in the reduced space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality using PCA for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the clusters using the first two principal components\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cluster_labels, palette='Set1', s=50)\n",
    "plt.title('Clusters Visualization using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Interpretation and Insights\n",
    "\n",
    "To interpret the clusters, we attach the cluster labels back to the original dataset and compute summary statistics for each cluster. We also examine the distribution of the target variable (`y`) within each cluster to see if certain segments are more likely to subscribe to a term deposit.\n",
    "\n",
    "Feel free to extend this analysis by exploring additional features or visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original dataframe for further interpretation\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Display summary statistics for numeric features in each cluster\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nSummary statistics for Cluster {cluster}:\")\n",
    "    display(df[df['cluster'] == cluster][numeric_cols].describe())\n",
    "\n",
    "# Examine the distribution of the target variable 'y' in each cluster\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nTarget variable distribution in Cluster {cluster}:\")\n",
    "    print(df[df['cluster'] == cluster]['y'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Reflection\n",
    "\n",
    "- The **elbow method** and **silhouette scores** helped us choose an appropriate number of clusters (in this example, we selected 3).\n",
    "- The **PCA visualization** shows that while clusters are reasonably separated in a 2D projection, some overlap still exists. This is expected when reducing high-dimensional data to 2 dimensions.\n",
    "- The **cluster summaries** and the target variable distributions provide insights into how different customer segments behave. For example, one cluster might have a higher proportion of customers who subscribe to term deposits.\n",
    "- In a real-world scenario, you could further refine this analysis by experimenting with other clustering techniques, adjusting preprocessing steps, or incorporating domain knowledge to interpret the clusters.\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Customer segmentation can drive targeted marketing strategies and help improve campaign efficiency. However, it is important to remember that unsupervised clustering may require iterative tuning and expert interpretation to yield actionable insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
