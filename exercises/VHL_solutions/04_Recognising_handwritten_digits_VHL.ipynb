{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Recognising handwritten digits\n",
    "\n",
    "### The digits dataset:\n",
    "Use the Scikit-learn digits dataset (`from sklearn.datasets import load_digits`).\n",
    "The data consist of images from handwritten digits and contains 250 samples from 44 writers.\n",
    "\n",
    "This data is stored in the `.data` member, which is an `n_samples`, `n_features` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Load the digits dataset and print the shape of the data and the target.\n",
    "2. Display the ten first images and the training set and the corresponding target values.\n",
    "3. Split the data set into a training and test set and train a (multinomial) logistic regression classifier on the dataset and evaluate its accuracy.\n",
    "4. Write some code that draw one random image from the test set, displays this image, and print the predicted value of the digit.\n",
    "5. Train some alternative classifiers and compare their accuracy to the logistic regression classifier. Here are some suggested models:\n",
    "    a. Decision tree\n",
    "    b. K-nearest neighbors\n",
    "    c. Naive Bayes\n",
    "    d. Support vector machine (not covered in this course)\n",
    "6. Try to improve the accuracy of some classifiers by tuning the hyperparameters such as `max_depth` for the decision tree or `n_neighbors` for the KNN classifier.\n",
    "7. (Optional) If you want to experiment with a larger dataset, try the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This dataset contains 70,000 images of handwritten digits, and is a good starting point for more advanced image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(np.shape(digits.data)) # We see that the data has 1797 rows and 64 columns that corresponds to the futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"truth\" for each row is given by the `.target` member. This is what we would like to learn. Given the features we would like to be able to predict this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(digits.target) # The target is a number from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the original image as `(8, 8)` pixels. Here is the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to predict, given an image, which digit it represents.\n",
    "We are given samples of each of the 10 possible classes (the digits zero through nine) on which we fit an estimator to be able to predict the classes to which unseen samples belong.\n",
    "\n",
    "The digits dataset consists of `8x8` pixel images of digits.\n",
    "The images attribute of the dataset stores `8x8` arrays of grayscale values for each image.\n",
    "We will use these arrays to visualize the first 4 images. The target attribute of the dataset stores the digit each image represents and this is included in the title of the 10 plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(15, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 3\n",
    "# When the outcome has multiple classes, logistic regression can still be used,\n",
    "# but it needs to be extended to handle multiple classes.\n",
    "# This is known as multinomial logistic regression.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "clf_lr = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=10000)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "acc_lr = clf_lr.score(X_test, y_test)\n",
    "print(\"Accuracy:\", acc_lr.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 4\n",
    "# Load an image and reshape it to the correct shape\n",
    "\n",
    "random_image = np.random.choice(range(len(X_test)))\n",
    "print(\"Random image is number:\", random_image, \"and the corresponding target is:\", y_test[random_image], \"and the corresponding image is:\")\n",
    "\n",
    "img = X_test[random_image, :].reshape(1, -1)\n",
    "img_to_disp = X_test[random_image, :].reshape(8, 8)\n",
    "\n",
    "plt.imshow(img_to_disp, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction for the image\n",
    "prediction = clf_lr.predict(img)\n",
    "print(\"The prediction for the image is:\", prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 5a: Decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier's accuracy on the test data\n",
    "acc_dt = clf_dt.score(X_test, y_test)\n",
    "print(\"Accuracy:\", acc_dt.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 5b: K-nearest neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train a K-nearest neighbors classifier\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier's accuracy on the test data\n",
    "acc_knn = clf_knn.score(X_test, y_test)\n",
    "print(\"Accuracy:\", acc_knn.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 5c: Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier's accuracy on the test data\n",
    "acc_nb = clf_nb.score(X_test, y_test)\n",
    "print(\"Accuracy:\", acc_nb.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 5d: Support vector machine\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a Support Vector Machine classifier\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier's accuracy on the test data\n",
    "acc_svm = clf_svm.score(X_test, y_test)\n",
    "print(\"Accuracy:\", acc_svm.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 6: Hyperparameter tuning\n",
    "\n",
    "# Decision tree classifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 6, 10]\n",
    "}\n",
    "\n",
    "# max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "# min_samples_split: The minimum number of samples required to split an internal node.\n",
    "# Setting min_samples_split to a higher value can make the tree more compact and reduce overfitting.\n",
    "# On the other hand, setting it to a lower value can lead to more complex trees that may overfit the data.\n",
    "\n",
    "# Initialize the classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "for max_depth in param_grid['max_depth']:\n",
    "    for min_samples_split in param_grid['min_samples_split']:\n",
    "        dtc.set_params(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        \n",
    "        dtc.fit(X_train, y_train)\n",
    "        print(\"max_depth:\", max_depth, \"min_samples_split:\", min_samples_split, \" Accuracy:\", dtc.score(X_test, y_test).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# K-nearest neighbors classifier\n",
    "\n",
    "# Define the para# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11]\n",
    "}\n",
    "\n",
    "# n_neighbors: Number of neighbors to used by the k-neighbors algorithm.\n",
    "\n",
    "# Initialize the classifier\n",
    "knn = KNeighborsClassifier()\n",
    "for n_neighbors in param_grid['n_neighbors']:\n",
    "    knn.set_params(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"n_neighbors:\", n_neighbors, \" Accuracy:\", knn.score(X_test, y_test).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
