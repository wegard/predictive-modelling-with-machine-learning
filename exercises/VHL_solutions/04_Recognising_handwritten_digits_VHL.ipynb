{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Recognising handwritten digits\n",
    "\n",
    "### The digits dataset\n",
    "We will use the [Scikit-learn](https://scikit-learn.org/) `digits` dataset (`from sklearn.datasets import load_digits`).\n",
    "This dataset consists of 8×8 images of handwritten digits (0–9). There are 1797 total samples, each belonging to one of 10 classes (digits 0–9). Each image can be thought of as 64 features (the pixel intensities) when flattened.\n",
    "\n",
    "We aim to build classification models that can accurately predict which digit an image represents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. **Load** the digits dataset and **print** the shape of the data and the target.\n",
    "2. **Display** the first ten images in the training set along with their **corresponding target** values.\n",
    "3. **Split** the dataset into a training and test set and **train** a logistic regression classifier (using multinomial extension) on the dataset. **Evaluate** its accuracy.\n",
    "4. Write code that **draws one random image** from the test set, **displays** this image, and **prints the predicted value** of the digit.\n",
    "5. **Train alternative classifiers** and compare their accuracy to the logistic regression classifier. Suggested models:\n",
    "    - Decision Tree\n",
    "    - K-Nearest Neighbors\n",
    "    - Naive Bayes\n",
    "    - Support Vector Machine (not covered in detail in this course)\n",
    "6. **Tune hyperparameters** (e.g., `max_depth` for Decision Tree, `n_neighbors` for KNN, etc.) to see if you can **improve** the accuracy of the classifiers.\n",
    "7. (Optional) If you'd like to try a **larger dataset**, explore the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) which contains 70,000 images of handwritten digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of digits.data: (1797, 64)\n",
      "Shape of digits.target: (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Solution 1\n",
    "# Step 1: Load the digits dataset and print its shape.\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(\"Shape of digits.data:\", np.shape(digits.data))\n",
    "print(\"Shape of digits.target:\", np.shape(digits.target))\n",
    "\n",
    "# The data has 1797 samples and 64 features (8×8 pixels), and 1797 corresponding target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. Why does each image become 64 features when flattened?\n",
    "2. What does the `digits.target` array represent in terms of the classification task?\n",
    "3. How might the shape of the data change if you were to use a higher-resolution image for each digit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.target` array gives the ground-truth digit (0–9) for each sample. Below we can inspect its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target array:\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target array:\")\n",
    "print(digits.target)  # This will show the correct digit class for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access each original image in an `(8×8)` pixel format. Here is the first image in the dataset (in array form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image in digits.images:\n",
      "[[ 0.  0.  0.  4. 15. 12.  0.  0.]\n",
      " [ 0.  0.  3. 16. 15. 14.  0.  0.]\n",
      " [ 0.  0.  8. 13.  8. 16.  0.  0.]\n",
      " [ 0.  0.  1.  6. 15. 11.  0.  0.]\n",
      " [ 0.  1.  8. 13. 15.  1.  0.  0.]\n",
      " [ 0.  9. 16. 16.  5.  0.  0.  0.]\n",
      " [ 0.  3. 13. 16. 16. 11.  5.  0.]\n",
      " [ 0.  0.  0.  3. 11. 16.  9.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First image in digits.images:\")\n",
    "print(digits.images[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification task is to predict which digit (0–9) the image represents based on these pixel intensities.\n",
    "Below, we visualize the first **10 images** of the dataset alongside their target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(15, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Label: %i\" % label)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use **multinomial logistic regression** to classify the digits. When the outcome variable has multiple classes, standard logistic regression is extended to handle more than two outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic Regression Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Solution 3\n",
    "# Train a (multinomial) logistic regression classifier on the dataset and evaluate its accuracy.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Flatten the images into vectors of size 64 (8x8)\n",
    "X = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the data into training (80%) and test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model (the default is multinomial logistic regression)\n",
    "clf_lr = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "# Train the model\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "acc_lr = clf_lr.score(X_test, y_test)\n",
    "print(\"Multinomial Logistic Regression Accuracy:\", acc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. What do you think happens if we don’t **flatten** the images (i.e., if we don’t reshape `digits.images`)? How would the classifier handle the data?\n",
    "2. Try changing the parameter `max_iter`. Does it affect convergence or accuracy?\n",
    "3. Logistic Regression has a `C` parameter (inverse regularization strength). What happens if you vary `C` significantly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pick a **random image** from the **test set**, display it, and make a prediction using our trained logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image index: 233\n",
      "True label for this image: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution 4\n",
    "\n",
    "import random\n",
    "\n",
    "# Choose a random index from the test set\n",
    "random_idx = random.choice(range(len(X_test)))\n",
    "\n",
    "print(\"Random image index:\", random_idx)\n",
    "print(\"True label for this image:\", y_test[random_idx])\n",
    "\n",
    "# Reshape for display (8x8)\n",
    "img_to_disp = X_test[random_idx, :].reshape(8, 8)\n",
    "plt.imshow(img_to_disp, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.title(\"True Label: %i\" % y_test[random_idx])\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction for the same image\n",
    "prediction = clf_lr.predict(X_test[random_idx, :].reshape(1, -1))\n",
    "print(\"Predicted Label:\", prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. How often (qualitatively) do you see correct vs. incorrect predictions if you keep running the random selection?\n",
    "2. Try displaying the **raw feature vector** (64 values) of a random image. How do you interpret these values?\n",
    "3. What does `clf_lr.predict_proba(...)` return, and how could you interpret it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s train **alternative classifiers** and compare their accuracies with the logistic regression baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8416666666666667\n"
     ]
    }
   ],
   "source": [
    "# Solution 5a: Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train a Decision Tree Classifier\n",
    "clf_dt = DecisionTreeClassifier(random_state=42)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "acc_dt = clf_dt.score(X_test, y_test)\n",
    "print(\"Decision Tree Accuracy:\", acc_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. How does this Decision Tree accuracy compare with Logistic Regression?\n",
    "2. Inspect the parameters of the decision tree using `clf_dt.get_params()`. Which parameters might be most critical to adjust?\n",
    "3. How might a deeper tree (larger `max_depth`) affect bias vs. variance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "# Solution 5b: K-Nearest Neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize and train a KNN classifier\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "acc_knn = clf_knn.score(X_test, y_test)\n",
    "print(\"KNN Accuracy:\", acc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. The default value for `n_neighbors` is 5. Experiment with other values (e.g., 3, 7, 10). Do you see any accuracy changes?\n",
    "2. KNN can be slow for large datasets because it stores all training data. What might be a remedy?\n",
    "3. How does KNN’s decision boundary concept differ from Decision Trees?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Solution 5c: Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "acc_nb = clf_nb.score(X_test, y_test)\n",
    "print(\"Naive Bayes Accuracy:\", acc_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. Unlike other models, Naive Bayes assumes feature independence. Where might this assumption break down for image data?\n",
    "2. What are the main differences between `MultinomialNB` and `GaussianNB`?\n",
    "3. Could you apply `MultinomialNB` directly to raw pixel intensities on a more complex dataset (e.g. colored images)? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "# Solution 5d: Support Vector Machine (SVM)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(random_state=42)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "acc_svm = clf_svm.score(X_test, y_test)\n",
    "print(\"SVM Accuracy:\", acc_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. The default kernel for `SVC` is the RBF kernel. Try another kernel (e.g., `linear` or `poly`). Does accuracy change?\n",
    "2. SVMs can be sensitive to scaling. How might you preprocess the data for improved performance?\n",
    "3. Could SVMs scale well to the larger MNIST dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Below, we manually loop over some candidate hyperparameters for **Decision Tree** and **KNN**, printing out the accuracy. This is a simple approach. One can also use Scikit-learn’s `GridSearchCV` or `RandomizedSearchCV` for a more systematic tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Hyperparameter Tuning:\n",
      "\n",
      "max_depth=10, min_samples_split=2 => Accuracy: 0.8500\n",
      "max_depth=10, min_samples_split=6 => Accuracy: 0.8583\n",
      "max_depth=10, min_samples_split=10 => Accuracy: 0.8333\n",
      "max_depth=20, min_samples_split=2 => Accuracy: 0.8417\n",
      "max_depth=20, min_samples_split=6 => Accuracy: 0.8528\n",
      "max_depth=20, min_samples_split=10 => Accuracy: 0.8250\n",
      "max_depth=None, min_samples_split=2 => Accuracy: 0.8417\n",
      "max_depth=None, min_samples_split=6 => Accuracy: 0.8528\n",
      "max_depth=None, min_samples_split=10 => Accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "# Solution 6: Hyperparameter tuning (Decision Tree)\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 6, 10]\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "print(\"Decision Tree Hyperparameter Tuning:\\n\")\n",
    "for max_depth in param_grid_dt['max_depth']:\n",
    "    for min_samples_split in param_grid_dt['min_samples_split']:\n",
    "        dtc.set_params(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        dtc.fit(X_train, y_train)\n",
    "        score = dtc.score(X_test, y_test)\n",
    "        print(f\"max_depth={max_depth}, min_samples_split={min_samples_split} => Accuracy: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. Which combination of parameters gives the highest accuracy? Are there multiple parameter sets with similar performance?\n",
    "2. If you were to use `GridSearchCV`, how would it streamline this process?\n",
    "3. What other hyperparameters might matter for decision trees?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Hyperparameter Tuning:\n",
      "\n",
      "n_neighbors=3 => Accuracy: 0.9833\n",
      "n_neighbors=5 => Accuracy: 0.9861\n",
      "n_neighbors=7 => Accuracy: 0.9889\n",
      "n_neighbors=9 => Accuracy: 0.9806\n",
      "n_neighbors=11 => Accuracy: 0.9833\n",
      "n_neighbors=60 => Accuracy: 0.9472\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors Hyperparameter Tuning\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11,60]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print(\"\\nKNN Hyperparameter Tuning:\\n\")\n",
    "for n_neighbors in param_grid_knn['n_neighbors']:\n",
    "    knn.set_params(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    score = knn.score(X_test, y_test)\n",
    "    print(f\"n_neighbors={n_neighbors} => Accuracy: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Extensions\n",
    "1. How does changing the number of neighbors affect the bias-variance tradeoff?\n",
    "2. If you had a **much larger** training set, would KNN potentially perform better or worse?\n",
    "3. Can you combine KNN with dimensionality reduction (e.g., PCA) for potentially faster predictions?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
