{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: Neural networks with Keras II\n",
    "\n",
    "## GRA 4160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the below examples and experiment with the code. Try to understand what is happening in each example. Try to change the code and see what happens.\n",
    "\n",
    "1. Example of using different weight initialisations\n",
    "\n",
    "2. Example of using different optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Example of using different weight initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((60000, 28 * 28))\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "\n",
    "x_test = x_test.reshape((10000, 28 * 28))\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define a function to create the model with the specified weight initialization\n",
    "def create_model(initializer):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", kernel_initializer=initializer, input_shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(64, activation=\"relu\", kernel_initializer=initializer))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\", kernel_initializer=initializer))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides several weight initializers in the tf.keras.initializers module.\n",
    "\n",
    "- Zeros: Initializes the weights with all zeros.\n",
    "- Ones: Initializes the weights with all ones.\n",
    "- Constant: Initializes the weights with a constant value.\n",
    "- RandomNormal: Initializes the weights with a normal distribution.\n",
    "- RandomUniform: Initializes the weights with a uniform distribution.\n",
    "- TruncatedNormal: Initializes the weights with a truncated normal distribution, where any value beyond two standard deviations from the mean is discarded and resampled.\n",
    "- VarianceScaling: Initializes the weights by scaling their variance according to the input and output dimensions.\n",
    "- GlorotNormal (Xavier normal): Initializes the weights with a normal distribution with mean 0 and variance 2 / (input_units + output_units).\n",
    "- GlorotUniform (Xavier uniform): Initializes the weights with a uniform distribution within the range [-limit, limit], where limit = sqrt(6 / (input_units + output_units)).\n",
    "- LecunNormal: Initializes the weights with a normal distribution with mean 0 and variance 1 / input_units.\n",
    "- LecunUniform: Initializes the weights with a uniform distribution within the range [-limit, limit], where limit = sqrt(3 / input_units).\n",
    "- HeNormal: Initializes the weights with a normal distribution with mean 0 and variance 2 / input_units.\n",
    "- HeUniform: Initializes the weights with a uniform distribution within the range [-limit, limit], where limit = sqrt(6 / input_units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create models with different weight initializations\n",
    "initializers = {\n",
    "    \"Random Normal\": tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None),\n",
    "    \"Glorot Uniform\": tf.keras.initializers.GlorotUniform(seed=None),\n",
    "    \"He Normal\": tf.keras.initializers.HeNormal(seed=None),\n",
    "    \"LeCun Normal\": tf.keras.initializers.LecunNormal(seed=None),\n",
    "}\n",
    "\n",
    "for name, initializer in initializers.items():\n",
    "    model = create_model(initializer)\n",
    "    print(f\"Training model with {name} initialization\")\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy with {name} initialization: {round(test_acc, 4)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example of using different optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic Gradient Descent (SGD): A basic optimization algorithm that updates the model's weights iteratively using a fixed learning rate. It supports optional momentum and Nesterov momentum for improved convergence.\n",
    "- Adaptive Moment Estimation (Adam): A popular optimization algorithm that adapts the learning rate for each weight individually by computing the first and second moments of the gradients. It usually provides fast convergence and requires less tuning of the learning rate.\n",
    "- RMSprop: An adaptive learning rate optimization algorithm that divides the learning rate for each weight by a running average of the magnitudes of recent gradients for that weight. It is well-suited for problems with non-stationary objectives, such as online learning and noisy gradients.\n",
    "- Adagrad: An adaptive learning rate optimization algorithm that scales the learning rate based on the sum of squares of the past gradients. It is particularly useful for sparse data and often achieves good results in natural language processing and recommendation systems.\n",
    "- Adadelta: An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate by maintaining a moving average of the gradients and updating parameters based on the ratio of the accumulated gradients.\n",
    "- Adamax: A variant of Adam based on the infinity norm, which can be more stable and provide better results for some models.\n",
    "- Nadam: A combination of Adam and Nesterov momentum, which incorporates Nesterov's accelerated gradient into the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_simple_nn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (SGD):\n",
    "model_sgd = create_simple_nn()\n",
    "model_sgd.compile(optimizer='sgd',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history_sgd = model_sgd.fit(train_images, train_labels, epochs=10, batch_size=32,\n",
    "                            validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Adam:\n",
    "model_adam = create_simple_nn()\n",
    "model_adam.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_adam = model_adam.fit(train_images, train_labels, epochs=10, batch_size=32,\n",
    "                              validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# RMSprop:\n",
    "model_rmsprop = create_simple_nn()\n",
    "model_rmsprop.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_rmsprop = model_rmsprop.fit(train_images, train_labels, epochs=10, batch_size=32,\n",
    "                                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(optimizer_name, history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'{optimizer_name} - Accuracy')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{optimizer_name} - Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_history('SGD', history_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_history('Adam', history_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_history('RMSprop', history_rmsprop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
