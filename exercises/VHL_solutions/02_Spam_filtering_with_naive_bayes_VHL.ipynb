{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02: Spam filtering with naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The naive Bayes classifier\n",
    "\n",
    "The naive Bayes classifier is a probabilistic machine learning model that is used for classification tasks. It is based on the idea that features in a dataset are independent of each other, which is called the \"naive\" assumption. Despite this assumption not always being true, the naive Bayes classifier has shown to be quite effective in many real-world applications.\n",
    "\n",
    "The algorithm works by training on a labeled dataset, where the input data is split into classes based on the target variable. For each class, the algorithm calculates the probability of each feature being associated with that class. During the prediction phase, the algorithm uses these probabilities to predict the class for a new, unlabeled example by finding the class with the highest probability.\n",
    "\n",
    "One of the strengths of the naive Bayes classifier is that it is simple and easy to implement, yet it can perform well on a variety of tasks. It is also relatively fast to train, which makes it a good choice for large datasets. However, it is important to note that the assumption of feature independence can sometimes lead to inaccurate predictions, particularly when the features are highly correlated. Despite this, the naive Bayes classifier can still be a useful tool in many situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset with $n$ features and a target variable with $k$ classes, the goal is to estimate the class probabilities $P(C_i)$ for each class $C_i$ and the feature probabilities $P(x_j|C_i)$ for each feature $x_j$ given each class.\n",
    "\n",
    "To make a prediction for a new, unlabeled example with feature values $x$, we can use Bayes' theorem to calculate the probability $P(C_i|x)$ for each class $C_i$:\n",
    "\n",
    "$$ P(C_i|x) = \\frac{P(x|C_i)P(C_i)}{P(x)} $$\n",
    "\n",
    "$P(C_i|x)$ is called the likelihood function, and we will often calculate the logarithm of this function often referred to as the log-likelihood. Then, we can predict the class with the highest probability (or maximum likelihood):\n",
    "\n",
    "$$\\arg\\max_i P(C_i|x)$$\n",
    "\n",
    "The probability $P(x)$ is often difficult to calculate, so it is usually dropped from the equation. This results in the simplified prediction rule:\n",
    "\n",
    "$$\\arg\\max_i (P(x|C_i) \\cdot P(C_i))$$\n",
    "\n",
    "In the case of the naive Bayes classifier, we assume that the features are independent, so we can estimate $P(x|C_i)$ as the product of the individual feature probabilities:\n",
    "\n",
    "$$P(x|C_i) = P(x_1|C_i) \\cdot P(x_2|C_i) \\cdot ... \\cdot P(x_n|C_i)$$\n",
    "\n",
    "The class probabilities $P(C_i)$ and feature probabilities $P(x_j|C_i)$ can be estimated using maximum likelihood estimation, which involves counting the number of occurrences of each class and feature in the training data and dividing by the total number of examples.\n",
    "\n",
    "For example, to estimate the probability $P(C_i)$, we would count the number of examples in class $C_i$ and divide by the total number of examples:\n",
    "\n",
    "$$P(C_i) = \\frac{\\text{count}(C_i)}{\\text{count}(total)}$$\n",
    "\n",
    "To estimate the probability $P(x_j|C_i)$, we would count the number of occurrences of feature $x_j$ in examples belonging to class $C_i$ and divide by the total number of examples in class $C_i$:\n",
    "\n",
    "$$P(x_j|C_i) = \\frac{\\text{count}(x_j, C_i)}{\\text{count}(C_i)} $$\n",
    "\n",
    "These probabilities can then be plugged into the prediction rule to make predictions for new examples.\n",
    "\n",
    "The prediction rule is:\n",
    "\n",
    "$$\\arg\\max_i (P(x|C_i) \\cdot P(C_i))$$\n",
    "\n",
    "This means that, given a new example with feature values $x$, we want to find the class $C_i$ that has the highest probability of occurring given the feature values $x$. To do this, we multiply the probability of the features $x$ given the class $C_i$ by the probability of the class $C_i$ occurring, and choose the class with the highest resulting probability.\n",
    "\n",
    "The probability $P(x|C_i)$ is calculated as the product of the individual feature probabilities $P(x_j|C_i)$, which are estimated using maximum likelihood estimation as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filtering\n",
    "\n",
    "A classic example of a classification problem that can be solved using the Naive Bayes algorithm is spam filtering.\n",
    "\n",
    "Here the input data consists of the text from either emails, sms or other types of messages. The goal is to classify the message as either \"spam\" or \"not spam\" based on the words that appear in the message and other features such as the sender and the subject line. We will only look at the content of the message.\n",
    "\n",
    "To solve this problem using the Naive Bayes algorithm, we start by extracting features from the messages, such as the presence or absence of certain words or phrases. We would then create a dataset with these features and the corresponding labels (\"spam\" or \"not spam\") for each message.\n",
    "\n",
    "Next, we train a naive Bayes classifier on this dataset, using the features to make predictions about the labels. The classifier would use the relative frequency of each word or phrase in the \"spam\" and \"not spam\" messages to estimate the probability that a new message is spam or not spam. We could then use this trained classifier to predict the class label for new, unseen email messages.\n",
    "\n",
    "<p>\n",
    "\n",
    "## Data\n",
    "\n",
    "The SMS Spam Collection is a set of labeled SMS messages that have been collected for mobile phone spam research. The dataset contains 5,572 SMS messages in English, tagged as either \"spam\" or \"ham\" (not spam). The messages have been collected from various sources, mostly from a publicly available corpus of SMS messages.\n",
    "\n",
    "Each line in the data file corresponds to one message, and each line contains the label (ham or spam) and the message text, separated by a tab character.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "1. Load the `smsspamcolection` dataset and inspect its content (ham means \"no spam\"). The dataset is also available on itslearning. Change the label (spam or ham) to a numeric value, e.g., `[0,1]`.\n",
    "2. Split the data into a training and a test set (you can use `from sklearn.model_selection import train_test_split`)\n",
    "3. Create a feature matrix (one feature is one word), in this case this the count matrix. You can use `from sklearn.feature_extraction.text import CountVectorizer` to generate this matrix\n",
    "4. Compute the class probabilities $P(C_1) = P(C_i='\\text{Spam}')$ and $P(C_2) = P(C_i='\\text{No spam}')$. What is the probability of an SMS being spam?\n",
    "5. Compute the conditional probabilities $P(x_i|C_1)$ and $P(x_i|C_2)$.\n",
    "6. Print out the five most used word in the messages classified as spam and not-spam.\n",
    "7. Calculate $log(P(C_1|x)) \\propto log(P(C_1)) + \\sum_{i=1}^n log(P(x_i|C_1))$ and $log(P(C_2|x)) \\propto log(P(C_2)) + \\sum_{i=1}^n log(P(x_i|C_2))$\n",
    "8. Without using Scikit-learn write a classifier and classify the messages in the test set using the prediction rule and evaluate how well your predictions are compared to the true labels.\n",
    "9. Use a builtin model for Naive Bayes in  Scikit-learn, e.g., `MultinomialNB` to train the classifier on the training set and evaluate how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      label                                            message\n0         1  Go until jurong point, crazy.. Available only ...\n1         1                      Ok lar... Joking wif u oni...\n2         0  Free entry in 2 a wkly comp to win FA Cup fina...\n3         1  U dun say so early hor... U c already then say...\n4         1  Nah I don't think he goes to usf, he lives aro...\n...     ...                                                ...\n5567      0  This is the 2nd time we have tried 2 contact u...\n5568      1               Will Ã¼ b going to esplanade fr home?\n5569      1  Pity, * was in mood for that. So...any other s...\n5570      1  The guy did some bitching but I acted like i'd...\n5571      1                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>0</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>1</td>\n      <td>Will Ã¼ b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>1</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>1</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>1</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows Ã 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution 1\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/smsspamcollection/SMSSpamCollection',\n",
    "                 sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# We have two classes:\n",
    "# $C_1 = 0$ (spam)\n",
    "# $C_2 = 1$ (not spam)\n",
    "df['label'] = df['label'].map({'spam': 0, 'ham': 1})\n",
    "\n",
    "# Print the first 5 rows\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'],\n",
    "                                                    test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 3\n",
    "\n",
    "# Import the CountVectorizer class from the sklearn.feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of the CountVectorizer class\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform it into a numerical array\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "# Transform the test data into a numerical array using the already-fitted vectorizer\n",
    "X_test = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3733, 6984)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the count matrix\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The count matrix is very sparse \n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of an email being spam: 0.134\n"
     ]
    }
   ],
   "source": [
    "# Solution 4\n",
    "\n",
    "# Create a dictionary to store the class probabilities\n",
    "class_probs = {}\n",
    "\n",
    "# Calculate the class probabilities\n",
    "# P(C_1) and P(C_2)\n",
    "class_probs[0] = (y_train == 0).mean()\n",
    "class_probs[1] = (y_train == 1).mean()\n",
    "\n",
    "\n",
    "print(f'The probability of an email being spam: {class_probs[0].round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 5\n",
    "\n",
    "# The conditional probabilities for each class is given by P(x_i|C_1) and P(x_i|C_2)\n",
    "# We get these by calculating the number of times each word appears in each class\n",
    "# and dividing by the total number of words in each class\n",
    "\n",
    "# The total number of words in each class\n",
    "total_words_in_class_0 = X_train[y_train == 0].sum()\n",
    "total_words_in_class_1 = X_train[y_train == 1].sum()\n",
    "\n",
    "# Calculate the number of times each word appears in each class and add 0.1 to each count to avoid taking the log of 0 later\n",
    "count_x_0 = X_train[y_train == 0].sum(axis=0) + 0.1\n",
    "count_x_1 = X_train[y_train == 1].sum(axis=0) + 0.1\n",
    "\n",
    "cond_probs_0 = count_x_0 / total_words_in_class_0\n",
    "cond_probs_1 = count_x_1 / total_words_in_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6984,)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(cond_probs_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 6\n",
    "\n",
    "# The most used words in the \"spam\" sms\n",
    "top5_spam_words = pd.DataFrame(cond_probs_0).sort_values(by=0, ascending=False).head(10)\n",
    "\n",
    "# The most used words in the \"no-spam\" sms\n",
    "top5_ham_words = pd.DataFrame(cond_probs_1).sort_values(by=0, ascending=False).head(10)\n",
    "\n",
    "# Create a utility function for printing out the top words\n",
    "def idx_to_word(idx):\n",
    "    print(list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "call\n",
      "you\n",
      "your\n",
      "for\n",
      "the\n",
      "free\n",
      "or\n",
      "now\n",
      "txt\n"
     ]
    }
   ],
   "source": [
    "for i in top5_spam_words.index:\n",
    "    idx_to_word(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "to\n",
      "the\n",
      "and\n",
      "in\n",
      "me\n",
      "is\n",
      "my\n",
      "it\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "for i in top5_ham_words.index:\n",
    "    idx_to_word(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution 7\n",
    "\n",
    "# We can now calculate the probability of a message being spam\n",
    "# P(C_1|x) = \\frac{P(C_1) \\prod_{i=1}^n P(x_i|C_1)}{P(x)}\n",
    "\n",
    "# We can ignore the denominator since it is the same for both classes\n",
    "# P(C_1|x) \\propto P(C_1) \\prod_{i=1}^n P(x_i|C_1)\n",
    "\n",
    "# We can calculate the log of the probability to avoid underflow\n",
    "# log(P(C_1|x)) \\propto log(P(C_1)) + \\sum_{i=1}^n log(P(x_i|C_1))\n",
    "\n",
    "log_probs = {}\n",
    "log_probs[0] = np.log(class_probs[0]) + np.log(cond_probs_0).dot(X_test.T)\n",
    "\n",
    "# And we do the same for P(C_2|x)\n",
    "log_probs[1] = np.log(class_probs[1]) + np.log(cond_probs_1).dot(X_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869\n"
     ]
    }
   ],
   "source": [
    "# Solution 8\n",
    "\n",
    "# We can now use the log probabilities to make our predictions\n",
    "\n",
    "# Use the argmax function to find the class with the highest probability\n",
    "y_pred = np.argmax([log_probs[0], log_probs[1]], axis=0)\n",
    "\n",
    "# Check the accuracy of our model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869\n"
     ]
    }
   ],
   "source": [
    "# Solution 9\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the accuracy of the model\n",
    "y_pred_sklearn = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_sklearn).round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
