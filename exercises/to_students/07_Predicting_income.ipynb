{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Predicting income\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble technique that trains a sequence of models, each one correcting the errors of the previous model.\n",
    "Boosting focuses on learning from the mistakes of the previous models and adjusts the weights of the training samples to emphasize the harder samples in the next round of training.\n",
    "In boosting, each model is trained on a weighted version of the training data, with more weight assigned to the samples that were misclassified by the previous models.\n",
    "The final prediction is the weighted sum of the predictions of the individual models. One of the most popular boosting algorithms is the Gradient Boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The `GradientBoostingClassifier` in Scikit-Learn\n",
    "\n",
    "The gradient boosting algorithm is a popular ensemble learning method that combines multiple weak learners to form a strong learner.\n",
    "\n",
    "The `GradientBoostingClassifier` builds an ensemble of decision trees sequentially, where each subsequent tree aims to correct the errors made by the previous tree.\n",
    "Specifically, it minimizes a loss function by using gradient descent to update the parameters of the model, such as the weights of the features and the thresholds for the decision nodes.\n",
    "\n",
    "The `GradientBoostingClassifier` has several hyperparameters that can be tuned to control the complexity of the model and prevent overfitting, including:\n",
    "\n",
    "- **n_estimators**: The number of decision trees in the ensemble\n",
    "- **learning_rate**: The step size of the gradient descent algorithm\n",
    "- **max_depth**: The maximum depth of each decision tree in the ensemble\n",
    "- **min_samples_split**: The minimum number of samples required to split a decision node.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "The GradientBoostingClassifier also supports several loss functions, such as the deviance loss (used for binary classification) and the exponential loss (used for AdaBoost).\n",
    "Additionally, it provides several methods for making predictions, such as predict for obtaining the class labels and predict_proba for obtaining the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                 header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                                     'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                                     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "\n",
    "# Replace '?' values with NaN\n",
    "df = df.replace('?', pd.NaT)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                        'relationship', 'race', 'sex', 'native-country']\n",
    "encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    df[feature] = encoder.fit_transform(df[feature])\n",
    "\n",
    "# Encode target variable\n",
    "df['income'] = df['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = df.drop(columns=['income'])\n",
    "y = df['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "1. Train a Gradient Boosting Classifier using the \"Adult\" dataset and evaluate its performance using the area under the ROC curve (AUC).\n",
    "2. Experiment wih different hyperparameters of the Gradient Boosting Classifier, such as the number of trees, learning rate, and maximum depth of each tree, and observe how they affect the model's performance.\n",
    "3. Train an AdaBoost Classifier using the \"Adult\" dataset and compare its performance to the Gradient Boosting Classifier. You can also try with XGBoost Classifier (need to be installed first).\n",
    "4. Perform feature selection using the Gradient Boosting Classifier and evaluate the performance of the model using only the top-5 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
