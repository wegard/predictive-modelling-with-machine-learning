{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting income\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble technique that trains a sequence of models, each one correcting the errors of the previous model.\n",
    "Boosting focuses on learning from the mistakes of the previous models and adjusts the weights of the training samples to emphasize the harder samples in the next round of training.\n",
    "In boosting, each model is trained on a weighted version of the training data, with more weight assigned to the samples that were misclassified by the previous models.\n",
    "The final prediction is the weighted sum of the predictions of the individual models. One of the most popular boosting algorithms is the Gradient Boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `GradientBoostingClassifier` in Scikit-Learn\n",
    "\n",
    "The gradient boosting algorithm is a popular ensemble learning method that combines multiple weak learners to form a strong learner.\n",
    "\n",
    "The `GradientBoostingClassifier` builds an ensemble of decision trees sequentially, where each subsequent tree aims to correct the errors made by the previous tree.\n",
    "Specifically, it minimizes a loss function by using gradient descent to update the parameters of the model, such as the weights of the features and the thresholds for the decision nodes.\n",
    "\n",
    "The `GradientBoostingClassifier` has several hyperparameters that can be tuned to control the complexity of the model and prevent overfitting, including:\n",
    "\n",
    "- **n_estimators**: The number of decision trees in the ensemble\n",
    "- **learning_rate**: The step size of the gradient descent algorithm\n",
    "- **max_depth**: The maximum depth of each decision tree in the ensemble\n",
    "- **min_samples_split**: The minimum number of samples required to split a decision node.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "The GradientBoostingClassifier also supports several loss functions, such as the deviance loss (used for binary classification) and the exponential loss (used for AdaBoost).\n",
    "Additionally, it provides several methods for making predictions, such as predict for obtaining the class labels and predict_proba for obtaining the class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "\n",
    "The Adult dataset, also commonly referred to as the \"Census Income\" dataset, is a popular resource for machine learning, especially for tasks involving classification and pattern recognition. This dataset was extracted from the 1994 Census database by Barry Becker and contains demographic information about adults from various backgrounds.\n",
    "\n",
    "Key features of the dataset include:\n",
    "\n",
    "- age: The age of the individual.\n",
    "- workclass: The type of employment of the individual (e.g., Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, etc.).\n",
    "- fnlwgt: Final weight. The number of people the census believes the entry represents.\n",
    "- education: The highest level of education achieved by the individual (e.g., Bachelors, Some-college, 11th, HS-grad, Prof-school, etc.).\n",
    "- education-num: The highest level of education in numerical form.\n",
    "- marital-status: Marital status of the individual.\n",
    "- occupation: The individual's occupation (e.g., Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, etc.).\n",
    "- relationship: Describes the individual's role in the family (e.g., Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried).\n",
    "- race: Race of the individual (e.g., White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black).\n",
    "- sex: The sex of the individual (Male, Female).\n",
    "- capital-gain: Income from investment sources, apart from wages/salary.\n",
    "- capital-loss: Losses from investment sources.\n",
    "- hours-per-week: Number of hours worked per week.\n",
    "- native-country: Country of origin for the individual.\n",
    "- income: Whether the individual earns more than $50K/year.\n",
    "\n",
    "The dataset is often used for predictive modeling and binary classification tasks, such as predicting whether an individual's income exceeds a certain threshold based on their demographic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                 header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                                     'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                                     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "\n",
    "# Replace '?' values with NaN\n",
    "df = df.replace('?', pd.NaT)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                        'relationship', 'race', 'sex', 'native-country']\n",
    "encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    df[feature] = encoder.fit_transform(df[feature])\n",
    "\n",
    "# Encode target variable\n",
    "df['income'] = df['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = df.drop(columns=['income'])\n",
    "y = df['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Train a Gradient Boosting Classifier using the \"Adult\" dataset and evaluate its performance using the area under the ROC curve (AUC).\n",
    "2. Experiment with different hyperparameters of the Gradient Boosting Classifier, such as the number of trees, learning rate, and maximum depth of each tree, and observe how they affect the model's performance.\n",
    "3. Train an AdaBoost Classifier using the \"Adult\" dataset and compare its performance to the Gradient Boosting Classifier. You can also try with XGBoost Classifier (need to be installed first).\n",
    "4. Perform feature selection using the Gradient Boosting Classifier and evaluate the performance of the model using only the top-5 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gra4160",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
