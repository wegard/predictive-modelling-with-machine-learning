{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Model selection, evaluation, and assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **k-Nearest Neighbors and Cross-Validation**\n",
    "   - **Task**: Use cross-validation to evaluate the performance of a $k$-nearest neighbors (kNN) model trained on the Iris dataset. Vary the number of neighbors (for example, from $k = 1$ to $k = 15$) and compare the resulting cross-validation scores.\n",
    "     - **Questions**:\n",
    "       1. Which value of $k$ gives the best performance?\n",
    "       2. How does the value of $k$ affect the model’s bias and variance?\n",
    "       3. When you examine the cross-validation scores, what do they tell you about the stability of the model for different $k$ values?\n",
    "\n",
    "2. **Comparing Multiple Models with Accuracy and Confusion Matrix**\n",
    "   - **Task**: Train different classification models on the Iris dataset (e.g., kNN, Decision Tree, Logistic Regression, etc.) and compare their performance using:\n",
    "     - Accuracy\n",
    "     - Confusion matrix\n",
    "   - **Questions**:\n",
    "     1. Which model achieves the highest accuracy?\n",
    "     2. By looking at the confusion matrices for each model, can you identify which classes (species) are most often misclassified? \n",
    "     3. How does the confusion matrix give you deeper insight into the types of errors each model makes (e.g., mixing up similar classes)?\n",
    "\n",
    "3. **k-Fold Cross-Validation with Multiple Models**\n",
    "   - **Task**: Perform $k$-fold cross-validation on the Iris dataset and compare the performance of several models (e.g., kNN, Decision Trees, Logistic Regression, SVM).\n",
    "   - **Questions**:\n",
    "     1. Which model has the highest cross-validation accuracy on average?\n",
    "     2. How does the cross-validation process help mitigate issues like overfitting and give a more reliable estimate of model performance?\n",
    "     3. If two models have similar average performance, what additional criteria could you use to select one model over the other (e.g., interpretability, training time, etc.)?\n",
    "\n",
    "4. **Hyperparameter Tuning with Decision Trees**\n",
    "   - **Task**: Train a Decision Tree on the Iris dataset. Use `GridSearchCV` to find the best hyperparameters for `max_depth`, `min_samples_leaf`, and `min_samples_split`.\n",
    "   - **Questions**:\n",
    "     1. What is the model accuracy (on a held-out test set or via cross-validation) with the optimized hyperparameters?\n",
    "     2. How do changes in hyperparameters (like `max_depth`) reflect the trade-off between underfitting and overfitting?\n",
    "     3. Beyond accuracy, what other model outputs or diagnostics (e.g., feature importance) can help you interpret the model’s behavior and performance?\n",
    "\n",
    "5. **ROC Curves and Precision-Recall Curves**\n",
    "   - **Task**: Review the following links from scikit-learn documentation:\n",
    "     - [Scikit-learn: ROC curves](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)\n",
    "     - [Scikit-learn: Precision-Recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)\n",
    "   - **Questions**:\n",
    "     1. In a multiclass setting (like the Iris dataset), how can you adapt or interpret ROC and Precision-Recall curves? \n",
    "     2. What do the shapes of these curves tell you about the performance trade-offs at different decision thresholds?\n",
    "     3. When might a Precision-Recall curve be more informative than a ROC curve, and vice versa? \n",
    "     4. How would you interpret the area under the curve (AUC) in the context of this (relatively balanced) dataset?\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Interpretation and Discussion Points**:\n",
    "- **Class Overlap**: Iris classes can sometimes overlap in feature space. Are there any insights in the misclassified examples that suggest the features used (e.g., petal length vs. sepal width) are not always sufficient to separate classes?\n",
    "- **Practical Significance**: If the difference in performance between two models is very small, do you see any practical significance or can you justify choosing a simpler model for the sake of interpretability?\n",
    "- **Error Analysis**: Look at misclassified samples for each model. Is there a consistent pattern in these misclassifications (e.g., Setosa being misclassified as Versicolor)? What might this indicate about the feature distribution or class similarities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
