{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Exercise: Extending the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Build a Deeper Network with Multiple Hidden Layers**  \n",
    "   - **Task:** Extend the network architecture to include two or three hidden layers instead of one.  \n",
    "   - **Details:**  \n",
    "     - Replace the single hidden layer with multiple layers, each with its own weight and bias tensors.  \n",
    "     - Experiment with different activation functions (e.g., Tanh or Sigmoid) between layers.  \n",
    "     - Compare the performance and training behavior with the original single-layer network.  \n",
    "   - **Challenge:** Understand how deeper architectures affect gradient flow and learning stability.\n",
    "\n",
    "2. **Implement Mini-Batch Gradient Descent**  \n",
    "   - **Task:** Modify the training loop to use mini-batches instead of processing the entire training set at once.  \n",
    "   - **Details:**  \n",
    "     - Split the training dataset into small batches (for example, 16 or 32 samples per batch).  \n",
    "     - In each epoch, iterate through each mini-batch, compute the loss, perform the backward pass, and update the parameters.  \n",
    "     - Observe how mini-batch training affects convergence speed and accuracy compared to full-batch gradient descent.  \n",
    "   - **Challenge:** Explore the trade-offs between computational efficiency and convergence noise.\n",
    "\n",
    "3. **Train on a More Challenging Dataset (e.g., MNIST)**  \n",
    "   - **Task:** Replace the Iris dataset with a more complex dataset such as MNIST (handwritten digit classification with 10 classes).  \n",
    "   - **Details:**  \n",
    "     - Load and preprocess the MNIST dataset (e.g., normalize the pixel values).  \n",
    "     - Adjust the network’s input layer to accept 28×28 images (flattened to 784 features) and update the output layer to have 10 classes.  \n",
    "     - Modify the training loop and hyperparameters as needed for the increased complexity.  \n",
    "   - **Challenge:** Understand how network design and training adjustments are required when scaling up to a more challenging and higher-dimensional dataset.\n",
    "\n",
    "4. **Add L2 Regularization (Weight Decay)**  \n",
    "   - **Task:** Introduce L2 regularization manually in the loss function to penalize large weights and potentially reduce overfitting.  \n",
    "   - **Details:**  \n",
    "     - After computing the cross-entropy loss, add a regularization term (e.g., lambda * (sum of squared weights)) to the loss.  \n",
    "     - Experiment with different values of the regularization coefficient (lambda) and observe its effect on training and generalization.  \n",
    "     - Ensure that biases are optionally excluded from regularization.  \n",
    "   - **Challenge:** Learn how regularization can improve model generalization and prevent overfitting in neural networks.\n",
    "\n",
    "5. **Implement Dropout Manually**  \n",
    "   - **Task:** Add dropout to the network by manually zeroing out a fraction of the neurons during training.  \n",
    "   - **Details:**  \n",
    "     - In the forward pass, after computing the activation from the first layer (or one of the hidden layers), create a dropout mask (a tensor of zeros and ones) based on a dropout probability (e.g., 0.5).  \n",
    "     - Multiply the activations element-wise by this mask to “drop” certain neurons during training.  \n",
    "     - Ensure that during evaluation (inference), the dropout is turned off (or the activations are scaled appropriately).  \n",
    "   - **Challenge:** Implementing dropout from scratch reinforces understanding of how randomness can improve model robustness by preventing co-adaptation of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
