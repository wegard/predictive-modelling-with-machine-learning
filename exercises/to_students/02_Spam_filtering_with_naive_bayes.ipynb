{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02: Spam filtering with naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The naive Bayes classifier\n",
    "\n",
    "The naive Bayes classifier is a probabilistic machine learning model that is used for classification tasks. It is based on the idea that features in a dataset are independent of each other, which is a \"naive\" assumption. Despite this assumption not always being true, the naive Bayes classifier has shown to be quite effective in many real-world applications.\n",
    "\n",
    "The algorithm works by training on a labeled dataset, where the input data is split into classes based on the target variable. For each class, the algorithm calculates the probability of each feature being associated with that class.\n",
    "\n",
    "During the prediction phase, the algorithm uses these probabilities to predict the class for a new, unlabeled example by finding the class with the highest probability.\n",
    "\n",
    "One of the strengths of the naive Bayes classifier is that it is simple and easy to implement, yet it can perform well on a variety of prediction tasks. It is also relatively fast to train, which makes it a good choice for large datasets. However, it is important to note that the assumption of feature independence can sometimes lead to inaccurate predictions, particularly when the features are highly correlated.\n",
    "\n",
    "\n",
    "### 1. Bayes’ Theorem and the Posterior\n",
    "\n",
    "Given a dataset with $n$ features and a target variable (class) that can take $k$ different values $C_1, C_2, \\ldots, C_k$, our goal is to predict the class of a new example with feature values $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$.\n",
    "\n",
    "According to **Bayes’ theorem**, the posterior probability of class $C_i$ given the features $\\mathbf{x}$ is:\n",
    "\n",
    "$$\n",
    "P(C_i \\mid \\mathbf{x}) \\;=\\; \\frac{P(\\mathbf{x} \\mid C_i)\\,P(C_i)}{P(\\mathbf{x})}.\n",
    "$$\n",
    "\n",
    "- $P(C_i)$ is the **prior** probability of class $C_i$.  \n",
    "- $P(\\mathbf{x} \\mid C_i)$ is the **likelihood** of observing $\\mathbf{x}$ given class $C_i$.  \n",
    "- $P(\\mathbf{x})$ is the **evidence**, which is the same for every class when we take the $\\arg\\max$.\n",
    "\n",
    "Because $P(\\mathbf{x})$ does not vary across classes, for classification we typically use only:\n",
    "\n",
    "$$\n",
    "\\arg\\max_i P(C_i \\mid \\mathbf{x}) \\;=\\; \\arg\\max_i \\bigl[P(\\mathbf{x} \\mid C_i)\\,P(C_i)\\bigr].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The “Naive” Conditional Independence Assumption\n",
    "\n",
    "In **naive Bayes**, we assume the features are conditionally independent given the class. Therefore,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x} \\mid C_i) \\;=\\; \\prod_{j=1}^{n} P(x_j \\mid C_i).\n",
    "$$\n",
    "\n",
    "This “naive” assumption simplifies the computation dramatically: we estimate each $P(x_j \\mid C_i)$ separately.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Maximum Likelihood Estimation of Probabilities\n",
    "\n",
    "We typically estimate $P(C_i)$ and $P(x_j \\mid C_i)$ from the training data via **maximum likelihood** counts:\n",
    "\n",
    "1. **Class probabilities**:\n",
    "   $$\n",
    "   P(C_i) \\;=\\; \\frac{\\text{count}(C_i)}{\\text{count}(\\text{total})},\n",
    "   $$\n",
    "   where $\\text{count}(C_i)$ is the number of training examples belonging to class $C_i$, and $\\text{count}(\\text{total})$ is the total number of training examples.\n",
    "\n",
    "2. **Feature probabilities**:\n",
    "   $$\n",
    "   P(x_j \\mid C_i) \\;=\\; \\frac{\\text{count}(x_j, C_i)}{\\text{count}(C_i)},\n",
    "   $$\n",
    "   where $\\text{count}(x_j, C_i)$ is the number of training examples that have feature $x_j$ *and* belong to class $C_i$.\n",
    "\n",
    "> **Note**: Often, we use Laplace smoothing (also called additive smoothing) to avoid zero probabilities by adding a small constant to the counts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Making Predictions\n",
    "\n",
    "To predict the class of a **new**, unlabeled example $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$:\n",
    "\n",
    "1. Compute the **likelihood** $P(\\mathbf{x} \\mid C_i)$ using the product of individual feature probabilities:\n",
    "   $$\n",
    "   P(\\mathbf{x} \\mid C_i) \\;=\\; \\prod_{j=1}^n P(x_j \\mid C_i).\n",
    "   $$\n",
    "\n",
    "2. Multiply by the **prior** $P(C_i)$:\n",
    "   $$\n",
    "   P(\\mathbf{x} \\mid C_i)\\, P(C_i).\n",
    "   $$\n",
    "\n",
    "3. Choose the class $C_i$ that **maximizes** this quantity:\n",
    "   $$\n",
    "   \\hat{C} \\;=\\; \\arg\\max_i \\bigl[P(\\mathbf{x} \\mid C_i) \\, P(C_i)\\bigr].\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Log-Likelihood (Optional but Recommended)\n",
    "\n",
    "Because the product of many probabilities can be **very** small, it is common to use the **log-likelihood** instead:\n",
    "\n",
    "$$\n",
    "\\log P(\\mathbf{x} \\mid C_i) \\;=\\; \\sum_{j=1}^n \\log P(x_j \\mid C_i).\n",
    "$$\n",
    "\n",
    "When choosing the maximizing class, the logs do not change the $\\arg\\max$:\n",
    "\n",
    "$$\n",
    "\\arg\\max_i \\Bigl[\\log P(\\mathbf{x} \\mid C_i) \\;+\\; \\log P(C_i)\\Bigr]\n",
    "\\;\\;=\\;\\;\n",
    "\\arg\\max_i \\Bigl[P(\\mathbf{x} \\mid C_i)\\; P(C_i)\\Bigr].\n",
    "$$\n",
    "\n",
    "Using logs helps with numerical stability and can prevent underflow issues when probabilities are extremely small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filtering\n",
    "\n",
    "A classic example of a classification problem that can be solved using the Naive Bayes algorithm is spam filtering.\n",
    "\n",
    "Here the input data consists of the text from either emails, sms or other types of messages. The goal is to classify the message as either \"spam\" or \"not spam\" based on the words that appear in the message and other features such as the sender and the subject line. We will only look at the content of the message.\n",
    "\n",
    "To solve this problem using the Naive Bayes algorithm, we start by extracting features from the messages, such as the presence or absence of certain words or phrases. We would then create a dataset with these features and the corresponding labels (\"spam\" or \"not spam\") for each message.\n",
    "\n",
    "Next, we train a naive Bayes classifier on this dataset, using the features to make predictions about the labels. The classifier would use the relative frequency of each word or phrase in the \"spam\" and \"not spam\" messages to estimate the probability that a new message is spam or not spam. We could then use this trained classifier to predict the class label for new, unseen email messages.\n",
    "\n",
    "<p>\n",
    "\n",
    "## Data\n",
    "\n",
    "The SMS Spam Collection is a set of labeled SMS messages that have been collected for mobile phone spam research. The dataset contains 5,572 SMS messages in English, tagged as either \"spam\" or \"ham\" (not spam). The messages have been collected from various sources, mostly from a publicly available corpus of SMS messages.\n",
    "\n",
    "Each line in the data file corresponds to one message, and each line contains the label (ham or spam) and the message text, separated by a tab character.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "1. Load the `smsspamcollection` dataset and inspect its content (ham means \"no spam\"). The dataset is also available on itslearning. Change the label (spam or ham) to a numeric value, e.g., `[0,1]`. Hint: I used `pd.read_csv` to load the data with tab as separator.\n",
    "\n",
    "2. Split the data into a training and a test set (you can use `from sklearn.model_selection import train_test_split`)\n",
    "\n",
    "3. Create a feature matrix (one feature is one word), in this case, this is the count matrix. You can use `from sklearn.feature_extraction.text import CountVectorizer` to generate this matrix\n",
    "\n",
    "4. Compute the class probabilities \n",
    "$$P(C_1) = P(C_i=\\text{Spam})$$ \n",
    "and \n",
    "$$P(C_2) = P(C_i=\\text{No spam}).$$ \n",
    "\n",
    "What is the probability of an SMS being spam?\n",
    "\n",
    "5. Compute the conditional probabilities $P(x_i|C_1)$ and $P(x_i|C_2)$. The $x_i$'s represents the features (words in this case). What is the probability of a word being in a spam message?\n",
    "\n",
    "6. Print out the five most frequently used words in messages classified as spam and not-spam.\n",
    "\n",
    "7. Calculate $log(P(C_1|x)) \\propto log(P(C_1)) + \\sum_{i=1}^n log(P(x_i|C_1))$ and $log(P(C_2|x)) \\propto log(P(C_2)) + \\sum_{i=1}^n log(P(x_i|C_2))$\n",
    "\n",
    "8. Without using Scikit-learn write a classifier and classify the messages in the test set using the prediction rule and evaluate how well your predictions are compared to the true labels.\n",
    "\n",
    "9. Use a built-in model for Naive Bayes in Scikit-learn, e.g., `MultinomialNB` to train the classifier on the training set and evaluate how well it performs on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
