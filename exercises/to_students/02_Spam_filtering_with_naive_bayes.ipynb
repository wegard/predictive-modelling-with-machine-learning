{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02: Spam filtering with naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The naive Bayes classifier\n",
    "\n",
    "The naive Bayes classifier is a probabilistic machine learning model that is used for classification tasks. It is based on the idea that features in a dataset are independent of each other, which is a \"naive\" assumption. Despite this assumption not always being true, the naive Bayes classifier has shown to be quite effective in many real-world applications.\n",
    "\n",
    "The algorithm works by training on a labeled dataset, where the input data is split into classes based on the target variable. For each class, the algorithm calculates the probability of each feature being associated with that class.\n",
    "\n",
    "During the prediction phase, the algorithm uses these probabilities to predict the class for a new, unlabeled example by finding the class with the highest probability.\n",
    "\n",
    "One of the strengths of the naive Bayes classifier is that it is simple and easy to implement, yet it can perform well on a variety of prediction tasks. It is also relatively fast to train, which makes it a good choice for large datasets. However, it is important to note that the assumption of feature independence can sometimes lead to inaccurate predictions, particularly when the features are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The naive Bayes classifier works as follows:\n",
    "\n",
    "Given a dataset with $n$ features and a target variable with $k$ classes, the goal is to estimate the class probabilities $P(C_i)$ for each class $C_i$ and the feature probabilities $P(x_j|C_i)$ for each feature $x_j$ given each class.\n",
    "\n",
    "To make a prediction for a new, unlabeled example with feature values $x$, we can use Bayes' theorem to calculate the posterior probability $P(C_i|x)$ for each class $C_i$:\n",
    "\n",
    "$$ P(C_i|x) = \\frac{P(x|C_i) P(C_i)}{P(x)} $$\n",
    "\n",
    "$P(x|C_i)$ is called the likelihood function, and we will often calculate the logarithm of this function, often referred to as the **log-likelihood function**. Then, we can predict the class with the highest probability (or maximum likelihood):\n",
    "\n",
    "$$\\arg\\max_i P(x|C_i)$$\n",
    "\n",
    "The probability $P(x)$ is often difficult to calculate, so it is usually dropped from the equation. This results in the simplified prediction rule:\n",
    "\n",
    "$$\\arg\\max_i (P(x|C_i)P(C_i))$$\n",
    "\n",
    "In the case of the naive Bayes classifier, we assume that the features are independent, so we can estimate $P(x|C_i)$ as the product of the individual feature probabilities:\n",
    "\n",
    "$$P(x|C_i) = P(x_1|C_i) \\cdot P(x_2|C_i) \\cdot ... \\cdot P(x_n|C_i)$$\n",
    "\n",
    "The class probabilities $P(C_i)$ and feature probabilities $P(x_j|C_i)$ can be estimated using maximum likelihood estimation, which involves counting the number of occurrences of each class and feature in the training data and dividing by the total number of examples.\n",
    "\n",
    "For example, to estimate the probability $P(C_i)$, we would count the number of examples in class $C_i$ and divide by the total number of examples:\n",
    "\n",
    "$$P(C_i) = \\frac{\\text{count}(C_i)}{\\text{count}(total)}$$\n",
    "\n",
    "To estimate the probability $P(x_j|C_i)$, we would count the number of occurrences of feature $x_j$ in examples belonging to class $C_i$ and divide by the total number of examples in class $C_i$:\n",
    "\n",
    "$$P(x_j|C_i) = \\frac{\\text{count}(x_j, C_i)}{\\text{count}(C_i)} $$\n",
    "\n",
    "These probabilities can then be plugged into the prediction rule to make predictions for new examples.\n",
    "\n",
    "The prediction rule is:\n",
    "\n",
    "$$\\arg\\max_i (P(x|C_i) P(C_i))$$\n",
    "\n",
    "This means that, given a new example with feature values $x$, we want to find the class $C_i$ that has the highest probability of occurring given the feature values $x$. To do this, we multiply the probability of the features $x$ given the class $C_i$ by the probability of the class $C_i$ occurring, and choose the class with the highest resulting probability.\n",
    "\n",
    "The probability $P(x|C_i)$ is calculated as the product of the individual feature probabilities $P(x_j|C_i)$, which are estimated using maximum likelihood estimation as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filtering\n",
    "\n",
    "A classic example of a classification problem that can be solved using the Naive Bayes algorithm is spam filtering.\n",
    "\n",
    "Here the input data consists of the text from either emails, sms or other types of messages. The goal is to classify the message as either \"spam\" or \"not spam\" based on the words that appear in the message and other features such as the sender and the subject line. We will only look at the content of the message.\n",
    "\n",
    "To solve this problem using the Naive Bayes algorithm, we start by extracting features from the messages, such as the presence or absence of certain words or phrases. We would then create a dataset with these features and the corresponding labels (\"spam\" or \"not spam\") for each message.\n",
    "\n",
    "Next, we train a naive Bayes classifier on this dataset, using the features to make predictions about the labels. The classifier would use the relative frequency of each word or phrase in the \"spam\" and \"not spam\" messages to estimate the probability that a new message is spam or not spam. We could then use this trained classifier to predict the class label for new, unseen email messages.\n",
    "\n",
    "<p>\n",
    "\n",
    "## Data\n",
    "\n",
    "The SMS Spam Collection is a set of labeled SMS messages that have been collected for mobile phone spam research. The dataset contains 5,572 SMS messages in English, tagged as either \"spam\" or \"ham\" (not spam). The messages have been collected from various sources, mostly from a publicly available corpus of SMS messages.\n",
    "\n",
    "Each line in the data file corresponds to one message, and each line contains the label (ham or spam) and the message text, separated by a tab character.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "1. Load the `smsspamcollection` dataset and inspect its content (ham means \"no spam\"). The dataset is also available on itslearning. Change the label (spam or ham) to a numeric value, e.g., `[0,1]`. Hint: I used `pd.read_csv` to load the data with tab as separator.\n",
    "\n",
    "2. Split the data into a training and a test set (you can use `from sklearn.model_selection import train_test_split`)\n",
    "\n",
    "3. Create a feature matrix (one feature is one word), in this case, this is the count matrix. You can use `from sklearn.feature_extraction.text import CountVectorizer` to generate this matrix\n",
    "\n",
    "4. Compute the class probabilities $P(C_1) = P(C_i=\\text{Spam})$ and $P(C_2) = P(C_i=\\text{No spam})$. What is the probability of an SMS being spam?\n",
    "\n",
    "5. Compute the conditional probabilities $P(x_i|C_1)$ and $P(x_i|C_2)$. The $x_i$s represents the features (words in this case). What is the probability of a word being in a spam message?\n",
    "\n",
    "6. Print out the five most frequently used words in messages classified as spam and not-spam.\n",
    "\n",
    "7. Calculate $log(P(C_1|x)) \\propto log(P(C_1)) + \\sum_{i=1}^n log(P(x_i|C_1))$ and $log(P(C_2|x)) \\propto log(P(C_2)) + \\sum_{i=1}^n log(P(x_i|C_2))$\n",
    "\n",
    "8. Without using Scikit-learn write a classifier and classify the messages in the test set using the prediction rule and evaluate how well your predictions are compared to the true labels.\n",
    "\n",
    "9. Use a built-in model for Naive Bayes in Scikit-learn, e.g., `MultinomialNB` to train the classifier on the training set and evaluate how well it performs on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
